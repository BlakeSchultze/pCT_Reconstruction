//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Fri Mar 14 18:52:22 2014 (1394848342)
// Cuda compilation tools, release 6.0, V6.0.1
//

.version 4.0
.target sm_20
.address_size 64

.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.global .align 8 .b8 _ZTVSt12codecvt_base[56];
.global .align 8 .b8 _ZTVSt10ctype_base[32];
.global .align 8 .b8 _ZTVSt5ctypeIcE[96];
.global .align 8 .b8 _ZTVSt8ios_base[32];
.global .align 8 .b8 _ZTVSt9basic_iosIcSt11char_traitsIcEE[32];
.global .align 8 .b8 _ZTVSo__St14basic_ofstreamIcSt11char_traitsIcEE[40];
.global .align 8 .b8 _ZTVSt9basic_iosIcSt11char_traitsIcEE__So__St14basic_ofstreamIcS1_E[40];
.global .align 8 .b8 _ZTVSi__St14basic_ifstreamIcSt11char_traitsIcEE[40];
.global .align 8 .b8 _ZTVSt9basic_iosIcSt11char_traitsIcEE__Si__St14basic_ifstreamIcS1_E[40];
.global .align 8 .b8 _ZTVSt15basic_streambufIcSt11char_traitsIcEE[144];
.global .align 8 .b8 _ZTVSt7num_putIcSt19ostreambuf_iteratorIcSt11char_traitsIcEEE[96];
.global .align 8 .b8 _ZTVSt7num_getIcSt19istreambuf_iteratorIcSt11char_traitsIcEEE[120];
.global .align 8 .b8 _ZTTSo[8];
.global .align 8 .b8 _ZTVSt7codecvtIcciE[88];
.global .align 8 .b8 _ZTTSi[8];
.global .align 8 .b8 _ZTVN10__cxxabiv117__class_type_infoE[8];
.global .align 8 .b8 _ZTVN10__cxxabiv120__si_class_type_infoE[8];
.global .align 8 .b8 _ZTVSt9bad_alloc[40];
.global .align 8 .b8 _ZTVSt13runtime_error[40];
.global .align 8 .b8 _ZTVNSt6locale5facetE[32];
.global .align 8 .b8 _ZTVSt12system_error[40];
.global .align 8 .b8 _ZTVNSt8ios_base7failureE[40];
.global .align 8 .b8 _ZTVSt14basic_ofstreamIcSt11char_traitsIcEE[80] = {168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 88, 255, 255, 255, 255, 255, 255, 255, 88, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZTVSt14basic_ifstreamIcSt11char_traitsIcEE[80] = {176, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 80, 255, 255, 255, 255, 255, 255, 255, 80, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZTVSt13basic_filebufIcSt11char_traitsIcEE[144];
.global .align 8 .u64 _ZTTSt14basic_ofstreamIcSt11char_traitsIcEE[4] = {generic(_ZTVSt14basic_ofstreamIcSt11char_traitsIcEE)+24, generic(_ZTVSo__St14basic_ofstreamIcSt11char_traitsIcEE)+24, generic(_ZTVSt9basic_iosIcSt11char_traitsIcEE__So__St14basic_ofstreamIcS1_E)+24, generic(_ZTVSt14basic_ofstreamIcSt11char_traitsIcEE)+64};
.global .align 8 .u64 _ZTTSt14basic_ifstreamIcSt11char_traitsIcEE[4] = {generic(_ZTVSt14basic_ifstreamIcSt11char_traitsIcEE)+24, generic(_ZTVSi__St14basic_ifstreamIcSt11char_traitsIcEE)+24, generic(_ZTVSt9basic_iosIcSt11char_traitsIcEE__Si__St14basic_ifstreamIcS1_E)+24, generic(_ZTVSt14basic_ifstreamIcSt11char_traitsIcEE)+64};
.global .align 1 .b8 $str[30] = {98, 105, 110, 95, 110, 117, 109, 91, 105, 93, 32, 61, 32, 37, 100, 32, 78, 85, 77, 95, 66, 73, 78, 83, 32, 61, 32, 37, 100, 0};
.global .align 1 .b8 $str1[67] = {120, 95, 109, 111, 118, 101, 95, 100, 105, 114, 101, 99, 116, 105, 111, 110, 32, 61, 32, 37, 100, 32, 121, 95, 109, 111, 118, 101, 95, 100, 105, 114, 101, 99, 116, 105, 111, 110, 32, 61, 32, 37, 100, 32, 122, 95, 109, 111, 118, 101, 95, 100, 105, 114, 101, 99, 116, 105, 111, 110, 32, 61, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str2[60] = {86, 79, 88, 69, 76, 95, 87, 73, 68, 84, 72, 32, 61, 32, 37, 51, 102, 32, 86, 79, 88, 69, 76, 95, 72, 69, 73, 71, 72, 84, 32, 61, 32, 37, 51, 102, 32, 83, 76, 73, 67, 69, 95, 84, 72, 73, 67, 75, 78, 69, 83, 83, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str3[49] = {118, 111, 120, 101, 108, 95, 120, 95, 105, 110, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 121, 95, 105, 110, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 122, 95, 105, 110, 32, 61, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str4[52] = {118, 111, 120, 101, 108, 95, 120, 95, 111, 117, 116, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 121, 95, 111, 117, 116, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 122, 95, 111, 117, 116, 32, 61, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str5[30] = {118, 111, 120, 101, 108, 95, 105, 110, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 111, 117, 116, 32, 61, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str6[52] = {120, 95, 101, 110, 116, 114, 121, 91, 105, 93, 32, 61, 32, 37, 51, 102, 32, 121, 95, 101, 110, 116, 114, 121, 91, 105, 93, 32, 61, 32, 37, 51, 102, 32, 122, 95, 101, 110, 116, 114, 121, 91, 105, 93, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str7[49] = {120, 95, 101, 120, 105, 116, 91, 105, 93, 32, 61, 32, 37, 56, 101, 32, 121, 95, 101, 120, 105, 116, 91, 105, 93, 32, 61, 32, 37, 51, 102, 32, 122, 95, 101, 120, 105, 116, 91, 105, 93, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str8[24] = {120, 95, 114, 101, 112, 108, 97, 99, 101, 32, 62, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str9[24] = {120, 95, 114, 101, 112, 108, 97, 99, 101, 32, 60, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str10[24] = {120, 95, 114, 101, 112, 108, 97, 99, 101, 32, 61, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str11[24] = {120, 95, 101, 120, 105, 116, 91, 105, 93, 32, 62, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str12[24] = {120, 95, 101, 120, 105, 116, 91, 105, 93, 32, 60, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str13[24] = {120, 95, 101, 120, 105, 116, 91, 105, 93, 32, 61, 32, 45, 55, 46, 50, 56, 63, 58, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str14[21] = {120, 95, 101, 120, 105, 116, 91, 105, 93, 32, 118, 111, 120, 101, 108, 32, 37, 56, 102, 10, 0};
.global .align 1 .b8 $str15[46] = {100, 101, 108, 116, 97, 95, 121, 120, 32, 61, 32, 37, 51, 102, 32, 100, 101, 108, 116, 97, 95, 122, 120, 32, 61, 32, 37, 51, 102, 32, 100, 101, 108, 116, 97, 95, 122, 121, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str16[43] = {120, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 32, 121, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 32, 122, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str18[50] = {122, 95, 116, 111, 95, 103, 111, 32, 60, 61, 32, 120, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 38, 38, 32, 122, 95, 116, 111, 95, 103, 111, 32, 60, 61, 32, 121, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 10, 0};
.global .align 1 .b8 $str19[40] = {120, 95, 109, 111, 118, 101, 32, 61, 32, 37, 51, 102, 32, 121, 95, 109, 111, 118, 101, 32, 61, 32, 37, 51, 102, 32, 122, 95, 109, 111, 118, 101, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str20[43] = {120, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 56, 102, 32, 121, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 32, 122, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str21[30] = {32, 120, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 60, 61, 32, 121, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 10, 0};
.global .align 1 .b8 $str22[29] = {32, 121, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 60, 32, 120, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 10, 0};
.global .align 1 .b8 $str23[51] = {118, 111, 120, 101, 108, 95, 120, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 121, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 95, 122, 32, 61, 32, 37, 100, 32, 118, 111, 120, 101, 108, 32, 61, 32, 37, 100, 10, 0};
.global .align 1 .b8 $str25[25] = {32, 120, 95, 116, 111, 95, 103, 111, 32, 60, 61, 32, 121, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 10, 0};
.global .align 1 .b8 $str26[28] = {120, 95, 109, 111, 118, 101, 32, 61, 32, 37, 51, 102, 32, 121, 95, 109, 111, 118, 101, 32, 61, 32, 37, 51, 102, 32, 10, 0};
.global .align 1 .b8 $str27[29] = {120, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 32, 121, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 51, 102, 10, 0};
.global .align 1 .b8 $str28[28] = {32, 121, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 32, 60, 32, 120, 95, 101, 120, 116, 101, 110, 115, 105, 111, 110, 10, 0};
.global .align 1 .b8 $str29[3] = {45, 49, 0};
.global .align 1 .b8 $str30[2] = {49, 0};
.global .align 1 .b8 $str32[59] = {32, 100, 101, 108, 116, 97, 95, 121, 120, 32, 61, 32, 37, 56, 102, 32, 121, 95, 109, 111, 118, 101, 32, 61, 32, 37, 56, 102, 32, 121, 95, 116, 111, 95, 103, 111, 32, 61, 32, 37, 56, 102, 32, 121, 95, 116, 111, 95, 103, 111, 50, 32, 61, 32, 37, 56, 102, 10, 0};
.global .align 1 .b8 $str33[22] = {37, 51, 102, 32, 37, 51, 102, 32, 37, 51, 102, 32, 37, 100, 32, 37, 100, 32, 37, 100, 10, 0};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .func  (.param .b64 func_retval0) _Z8_Pow_intIdET_S0_i(
	.param .b64 _Z8_Pow_intIdET_S0_i_param_0,
	.param .b32 _Z8_Pow_intIdET_S0_i_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .s32 	%r<12>;
	.reg .f64 	%fd<14>;


	ld.param.f64 	%fd12, [_Z8_Pow_intIdET_S0_i_param_0];
	ld.param.u32 	%r4, [_Z8_Pow_intIdET_S0_i_param_1];
	shr.s32 	%r5, %r4, 31;
	add.s32 	%r6, %r4, %r5;
	xor.b32  	%r7, %r6, %r5;
	and.b32  	%r8, %r6, 1;
	setp.eq.b32	%p1, %r8, 1;
	and.b32  	%r9, %r5, 1;
	setp.eq.b32	%p2, %r9, 1;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	mul.f64 	%fd10, %fd12, 0d3FF0000000000000;
	selp.f64	%fd13, 0d3FF0000000000000, %fd10, %p4;
	shr.u32 	%r11, %r7, 1;
	setp.eq.s32	%p5, %r11, 0;
	@%p5 bra 	BB0_2;

BB0_1:
	and.b32  	%r10, %r11, 1;
	setp.eq.b32	%p6, %r10, 1;
	not.pred 	%p7, %p6;
	mul.f64 	%fd12, %fd12, %fd12;
	mul.f64 	%fd11, %fd13, %fd12;
	selp.f64	%fd13, %fd13, %fd11, %p7;
	shr.u32 	%r11, %r11, 1;
	setp.ne.s32	%p8, %r11, 0;
	@%p8 bra 	BB0_1;

BB0_2:
	setp.gt.s32	%p9, %r4, -1;
	@%p9 bra 	BB0_4;

	rcp.rn.f64 	%fd13, %fd13;

BB0_4:
	st.param.f64	[func_retval0+0], %fd13;
	ret;
}

.visible .func  (.param .b32 func_retval0) _Z8_Pow_intIfET_S0_i(
	.param .b32 _Z8_Pow_intIfET_S0_i_param_0,
	.param .b32 _Z8_Pow_intIfET_S0_i_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .s32 	%r<12>;
	.reg .f32 	%f<14>;


	ld.param.f32 	%f12, [_Z8_Pow_intIfET_S0_i_param_0];
	ld.param.u32 	%r4, [_Z8_Pow_intIfET_S0_i_param_1];
	shr.s32 	%r5, %r4, 31;
	add.s32 	%r6, %r4, %r5;
	xor.b32  	%r7, %r6, %r5;
	and.b32  	%r8, %r6, 1;
	setp.eq.b32	%p1, %r8, 1;
	and.b32  	%r9, %r5, 1;
	setp.eq.b32	%p2, %r9, 1;
	xor.pred  	%p3, %p1, %p2;
	not.pred 	%p4, %p3;
	mul.f32 	%f10, %f12, 0f3F800000;
	selp.f32	%f13, 0f3F800000, %f10, %p4;
	shr.u32 	%r11, %r7, 1;
	setp.eq.s32	%p5, %r11, 0;
	@%p5 bra 	BB1_2;

BB1_1:
	and.b32  	%r10, %r11, 1;
	setp.eq.b32	%p6, %r10, 1;
	not.pred 	%p7, %p6;
	mul.f32 	%f12, %f12, %f12;
	mul.f32 	%f11, %f13, %f12;
	selp.f32	%f13, %f13, %f11, %p7;
	shr.u32 	%r11, %r11, 1;
	setp.ne.s32	%p8, %r11, 0;
	@%p8 bra 	BB1_1;

BB1_2:
	setp.gt.s32	%p9, %r4, -1;
	@%p9 bra 	BB1_4;

	rcp.rn.f32 	%f13, %f13;

BB1_4:
	st.param.f32	[func_retval0+0], %f13;
	ret;
}

.visible .func  (.param .b32 func_retval0) _Z20calculate_interceptsdddRdS_(
	.param .b64 _Z20calculate_interceptsdddRdS__param_0,
	.param .b64 _Z20calculate_interceptsdddRdS__param_1,
	.param .b64 _Z20calculate_interceptsdddRdS__param_2,
	.param .b64 _Z20calculate_interceptsdddRdS__param_3,
	.param .b64 _Z20calculate_interceptsdddRdS__param_4
)
{
	.local .align 4 .b8 	__local_depot2[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .s32 	%r<7>;
	.reg .f32 	%f<5>;
	.reg .s64 	%rd<6>;
	.reg .f64 	%fd<117>;


	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f64 	%fd17, [_Z20calculate_interceptsdddRdS__param_0];
	ld.param.f64 	%fd18, [_Z20calculate_interceptsdddRdS__param_1];
	ld.param.f64 	%fd19, [_Z20calculate_interceptsdddRdS__param_2];
	ld.param.u64 	%rd2, [_Z20calculate_interceptsdddRdS__param_3];
	ld.param.u64 	%rd3, [_Z20calculate_interceptsdddRdS__param_4];
	add.u64 	%rd4, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd4;
	setp.lt.f64	%p4, %fd19, 0d4002D97C7F3321D2;
	setp.gt.f64	%p5, %fd19, 0d3FE921FB54442D18;
	and.pred  	%p6, %p5, %p4;
	@!%p6 bra 	BB2_2;
	bra.uni 	BB2_1;

BB2_1:
	mov.pred 	%p16, -1;
	bra.uni 	BB2_3;

BB2_2:
	setp.gt.f64	%p7, %fd19, 0d400F6A7A2955385E;
	setp.lt.f64	%p8, %fd19, 0d4015FDBBE9BBA775;
	and.pred  	%p16, %p7, %p8;

BB2_3:
	neg.f64 	%fd20, %fd18;
	selp.f64	%fd1, %fd20, %fd17, %p16;
	selp.f64	%fd2, %fd17, %fd18, %p16;
	add.f64 	%fd21, %fd19, 0d3FF921FB54442D18;
	selp.f64	%fd114, %fd21, %fd19, %p16;
	abs.f64 	%fd22, %fd114;
	setp.neu.f64	%p10, %fd22, 0d7FF0000000000000;
	@%p10 bra 	BB2_5;

	mov.f64 	%fd23, 0d0000000000000000;
	mul.rn.f64 	%fd114, %fd114, %fd23;

BB2_5:
	mul.f64 	%fd24, %fd114, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r6, %fd24;
	st.local.u32 	[%rd1], %r6;
	cvt.rn.f64.s32	%fd25, %r6;
	neg.f64 	%fd26, %fd25;
	mov.f64 	%fd27, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd114;
	mov.f64 	%fd29, 0d3C91A62633145C00;
	fma.rn.f64 	%fd30, %fd26, %fd29, %fd28;
	mov.f64 	%fd31, 0d397B839A252049C0;
	fma.rn.f64 	%fd115, %fd26, %fd31, %fd30;
	abs.f64 	%fd32, %fd114;
	setp.leu.f64	%p11, %fd32, 0d41E0000000000000;
	@%p11 bra 	BB2_7;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd114;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd4;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd115, [retval0+0];
	}
	// Callseq End 0
	ld.local.u32 	%r6, [%rd1];

BB2_7:
	mul.f64 	%fd33, %fd115, %fd115;
	mov.f64 	%fd34, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd35, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd36, %fd35, %fd33, %fd34;
	mov.f64 	%fd37, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd38, %fd36, %fd33, %fd37;
	mov.f64 	%fd39, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd40, %fd38, %fd33, %fd39;
	mov.f64 	%fd41, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd42, %fd40, %fd33, %fd41;
	mov.f64 	%fd43, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd44, %fd42, %fd33, %fd43;
	mov.f64 	%fd45, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd46, %fd44, %fd33, %fd45;
	mov.f64 	%fd47, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd48, %fd46, %fd33, %fd47;
	mov.f64 	%fd49, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd50, %fd48, %fd33, %fd49;
	mov.f64 	%fd51, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd52, %fd50, %fd33, %fd51;
	mov.f64 	%fd53, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd54, %fd52, %fd33, %fd53;
	mov.f64 	%fd55, 0d3F9664F485D25660;
	fma.rn.f64 	%fd56, %fd54, %fd33, %fd55;
	mov.f64 	%fd57, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd58, %fd56, %fd33, %fd57;
	mov.f64 	%fd59, 0d3FC11111111105D2;
	fma.rn.f64 	%fd60, %fd58, %fd33, %fd59;
	mov.f64 	%fd61, 0d3FD555555555555E;
	fma.rn.f64 	%fd62, %fd60, %fd33, %fd61;
	mul.f64 	%fd9, %fd62, %fd33;
	fma.rn.f64 	%fd116, %fd9, %fd115, %fd115;
	and.b32  	%r4, %r6, 1;
	setp.eq.b32	%p12, %r4, 1;
	@!%p12 bra 	BB2_9;
	bra.uni 	BB2_8;

BB2_8:
	sub.f64 	%fd65, %fd116, %fd115;
	neg.f64 	%fd66, %fd65;
	fma.rn.f64 	%fd67, %fd9, %fd115, %fd66;
	// inline asm
	cvt.rn.f32.f64     %f1,%fd116;
	// inline asm
	// inline asm
	rcp.approx.ftz.f32 %f2,%f1;
	// inline asm
	// inline asm
	cvt.f64.f32        %fd64,%f2;
	// inline asm
	neg.f64 	%fd68, %fd116;
	mov.f64 	%fd69, 0d3FF0000000000000;
	fma.rn.f64 	%fd70, %fd68, %fd64, %fd69;
	fma.rn.f64 	%fd71, %fd70, %fd70, %fd70;
	fma.rn.f64 	%fd72, %fd71, %fd64, %fd64;
	neg.f64 	%fd73, %fd72;
	fma.rn.f64 	%fd74, %fd116, %fd73, %fd69;
	fma.rn.f64 	%fd75, %fd73, %fd67, %fd74;
	fma.rn.f64 	%fd116, %fd75, %fd73, %fd73;

BB2_9:
	mul.f64 	%fd76, %fd116, %fd116;
	mul.f64 	%fd77, %fd116, %fd1;
	sub.f64 	%fd13, %fd2, %fd77;
	mul.f64 	%fd78, %fd13, %fd13;
	add.f64 	%fd79, %fd116, %fd116;
	mul.f64 	%fd14, %fd79, %fd13;
	fma.rn.f64 	%fd15, %fd76, 0d3FF0000000000000, 0d3FF0000000000000;
	mul.f64 	%fd80, %fd14, %fd14;
	fma.rn.f64 	%fd81, %fd78, 0d3FF0000000000000, 0dC050000000000000;
	mul.f64 	%fd82, %fd15, 0dC010000000000000;
	mul.f64 	%fd83, %fd82, %fd81;
	fma.rn.f64 	%fd16, %fd80, 0d3FF0000000000000, %fd83;
	setp.gt.f64	%p3, %fd16, 0d0000000000000000;
	setp.leu.f64	%p13, %fd16, 0d0000000000000000;
	@%p13 bra 	BB2_11;

	sqrt.rn.f64 	%fd84, %fd16;
	sub.f64 	%fd85, %fd84, %fd14;
	add.f64 	%fd86, %fd15, %fd15;
	div.rn.f64 	%fd87, %fd85, %fd86;
	sub.f64 	%fd88, %fd87, %fd1;
	mul.f64 	%fd89, %fd88, %fd88;
	add.f64 	%fd90, %fd84, %fd14;
	div.rn.f64 	%fd91, %fd90, %fd86;
	fma.rn.f64 	%fd92, %fd116, %fd87, %fd13;
	mul.f64 	%fd93, %fd116, %fd91;
	sub.f64 	%fd94, %fd93, %fd13;
	sub.f64 	%fd95, %fd92, %fd2;
	mul.f64 	%fd96, %fd95, %fd95;
	mul.f64 	%fd97, %fd96, 0d3FF0000000000000;
	add.f64 	%fd98, %fd91, %fd1;
	mul.f64 	%fd99, %fd98, %fd98;
	fma.rn.f64 	%fd100, %fd89, 0d3FF0000000000000, %fd97;
	add.f64 	%fd101, %fd94, %fd2;
	mul.f64 	%fd102, %fd101, %fd101;
	mul.f64 	%fd103, %fd102, 0d3FF0000000000000;
	fma.rn.f64 	%fd104, %fd99, 0d3FF0000000000000, %fd103;
	setp.le.f64	%p14, %fd100, %fd104;
	selp.f64	%fd105, %fd87, 0d0000000000000000, %p14;
	setp.gt.f64	%p15, %fd100, %fd104;
	selp.f64	%fd106, %fd91, 0d0000000000000000, %p15;
	sub.f64 	%fd107, %fd105, %fd106;
	st.f64 	[%rd2], %fd107;
	selp.f64	%fd108, %fd92, 0d0000000000000000, %p14;
	selp.f64	%fd109, %fd94, 0d0000000000000000, %p15;
	sub.f64 	%fd110, %fd108, %fd109;
	st.f64 	[%rd3], %fd110;

BB2_11:
	@!%p16 bra 	BB2_13;
	bra.uni 	BB2_12;

BB2_12:
	ld.f64 	%fd111, [%rd2];
	ld.f64 	%fd112, [%rd3];
	st.f64 	[%rd2], %fd112;
	neg.f64 	%fd113, %fd111;
	st.f64 	[%rd3], %fd113;

BB2_13:
	selp.u32	%r5, 1, 0, %p3;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func _Z14voxel_walk_GPURPbffffff(
	.param .b64 _Z14voxel_walk_GPURPbffffff_param_0,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_1,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_2,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_3,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_4,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_5,
	.param .b32 _Z14voxel_walk_GPURPbffffff_param_6
)
{
	.reg .pred 	%p<64>;
	.reg .s16 	%rs<4>;
	.reg .s32 	%r<115>;
	.reg .f32 	%f<80>;
	.reg .s64 	%rd<11>;
	.reg .f64 	%fd<65>;


	ld.param.u64 	%rd1, [_Z14voxel_walk_GPURPbffffff_param_0];
	ld.param.f32 	%f40, [_Z14voxel_walk_GPURPbffffff_param_1];
	ld.param.f32 	%f41, [_Z14voxel_walk_GPURPbffffff_param_2];
	ld.param.f32 	%f42, [_Z14voxel_walk_GPURPbffffff_param_3];
	ld.param.f32 	%f43, [_Z14voxel_walk_GPURPbffffff_param_4];
	ld.param.f32 	%f44, [_Z14voxel_walk_GPURPbffffff_param_5];
	ld.param.f32 	%f45, [_Z14voxel_walk_GPURPbffffff_param_6];
	setp.le.f32	%p2, %f40, %f43;
	selp.u32	%r36, 1, 0, %p2;
	setp.gt.f32	%p3, %f40, %f43;
	selp.u32	%r37, 1, 0, %p3;
	sub.s32 	%r1, %r36, %r37;
	setp.le.f32	%p4, %f41, %f44;
	selp.u32	%r38, 1, 0, %p4;
	setp.gt.f32	%p5, %f41, %f44;
	selp.u32	%r39, 1, 0, %p5;
	sub.s32 	%r2, %r38, %r39;
	setp.le.f32	%p6, %f42, %f45;
	selp.u32	%r40, 1, 0, %p6;
	setp.gt.f32	%p7, %f42, %f45;
	selp.u32	%r41, 1, 0, %p7;
	sub.s32 	%r3, %r40, %r41;
	cvt.f64.f32	%fd18, %f40;
	add.f64 	%fd19, %fd18, 0d4020000000000000;
	div.rn.f64 	%fd1, %fd19, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p8, %fd2, 0d7FF0000000000000;
	@%p8 bra 	BB3_4;

	setp.eq.f64	%p9, %fd2, 0d7FF0000000000000;
	@%p9 bra 	BB3_3;

	add.f64 	%fd60, %fd1, %fd1;
	mov.f64 	%fd61, %fd60;
	bra.uni 	BB3_5;

BB3_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd1;
	}
	and.b32  	%r43, %r42, -2147483648;
	mov.f64 	%fd20, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd20;
	}
	or.b32  	%r45, %r44, %r43;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r46, %temp}, %fd20;
	}
	mov.b64 	%fd60, {%r46, %r45};
	mov.f64 	%fd61, %fd1;
	bra.uni 	BB3_5;

BB3_4:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd21, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd21;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd1;
	}
	and.b32  	%r50, %r49, -2147483648;
	or.b32  	%r51, %r48, %r50;
	mov.b64 	%fd60, {%r47, %r51};
	mov.f64 	%fd61, %fd5;

BB3_5:
	cvt.rzi.s32.f64	%r111, %fd61;
	mul.f64 	%fd22, %fd60, 0d3FB47AE147AE147B;
	mov.f64 	%fd23, 0d3FB47AE147AE147B;
	sub.f64 	%fd24, %fd23, %fd22;
	setp.gt.s32	%p10, %r1, 0;
	selp.f64	%fd25, %fd24, 0d0000000000000000, %p10;
	setp.lt.s32	%p11, %r1, 1;
	selp.f64	%fd26, %fd22, 0d0000000000000000, %p11;
	add.f64 	%fd27, %fd25, %fd26;
	cvt.rn.f32.f64	%f79, %fd27;
	neg.s32 	%r5, %r2;
	cvt.f64.f32	%fd28, %f41;
	mov.f64 	%fd29, 0d4020000000000000;
	sub.f64 	%fd30, %fd29, %fd28;
	div.rn.f64 	%fd9, %fd30, 0d3FB47AE147AE147B;
	abs.f64 	%fd10, %fd9;
	setp.lt.f64	%p12, %fd10, 0d7FF0000000000000;
	@%p12 bra 	BB3_9;

	setp.eq.f64	%p13, %fd10, 0d7FF0000000000000;
	@%p13 bra 	BB3_8;

	add.f64 	%fd63, %fd9, %fd9;
	mov.f64 	%fd64, %fd63;
	bra.uni 	BB3_10;

BB3_8:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd9;
	}
	and.b32  	%r53, %r52, -2147483648;
	mov.f64 	%fd31, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd31;
	}
	or.b32  	%r55, %r54, %r53;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd31;
	}
	mov.b64 	%fd63, {%r56, %r55};
	mov.f64 	%fd64, %fd9;
	bra.uni 	BB3_10;

BB3_9:
	cvt.rzi.f64.f64	%fd13, %fd9;
	sub.f64 	%fd32, %fd9, %fd13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r57, %temp}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd32;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd9;
	}
	and.b32  	%r60, %r59, -2147483648;
	or.b32  	%r61, %r58, %r60;
	mov.b64 	%fd63, {%r57, %r61};
	mov.f64 	%fd64, %fd13;

BB3_10:
	cvt.rzi.s32.f64	%r112, %fd64;
	shr.u32 	%r62, %r2, 31;
	cvt.rn.f64.s32	%fd33, %r62;
	mul.f64 	%fd34, %fd63, 0d3FB47AE147AE147B;
	sub.f64 	%fd36, %fd23, %fd34;
	setp.lt.s32	%p14, %r5, 1;
	selp.f64	%fd37, %fd34, 0d0000000000000000, %p14;
	fma.rn.f64 	%fd38, %fd33, %fd36, %fd37;
	cvt.rn.f32.f64	%f78, %fd38;
	neg.s32 	%r7, %r3;
	cvt.f64.f32	%fd39, %f42;
	mov.f64 	%fd40, 0d4008000000000000;
	sub.f64 	%fd41, %fd40, %fd39;
	mul.f64 	%fd42, %fd41, 0d4010000000000000;
	cvt.rn.f32.f64	%f3, %fd42;
	abs.f32 	%f4, %f3;
	setp.lt.f32	%p15, %f4, 0f7F800000;
	@%p15 bra 	BB3_14;

	setp.eq.f32	%p16, %f4, 0f7F800000;
	@%p16 bra 	BB3_13;

	add.f32 	%f75, %f3, %f3;
	mov.f32 	%f76, %f75;
	bra.uni 	BB3_15;

BB3_13:
	mov.b32 	 %r63, %f3;
	and.b32  	%r64, %r63, -2147483648;
	mov.b32 	 %f75, %r64;
	mov.f32 	%f76, %f3;
	bra.uni 	BB3_15;

BB3_14:
	cvt.rzi.f32.f32	%f7, %f3;
	sub.f32 	%f46, %f3, %f7;
	mov.b32 	 %r65, %f46;
	mov.b32 	 %r66, %f3;
	and.b32  	%r67, %r66, -2147483648;
	or.b32  	%r68, %r65, %r67;
	mov.b32 	 %f75, %r68;
	mov.f32 	%f76, %f7;

BB3_15:
	shr.u32 	%r69, %r3, 31;
	cvt.rn.f64.s32	%fd43, %r69;
	mul.f32 	%f47, %f75, 0f3E800000;
	cvt.f64.f32	%fd44, %f47;
	mov.f64 	%fd45, 0d3FD0000000000000;
	sub.f64 	%fd46, %fd45, %fd44;
	setp.lt.s32	%p17, %r7, 1;
	selp.f64	%fd47, %fd44, 0d0000000000000000, %p17;
	fma.rn.f64 	%fd17, %fd43, %fd46, %fd47;
	cvt.rzi.s32.f32	%r113, %f76;
	mul.lo.s32 	%r9, %r113, 40000;
	mad.lo.s32 	%r70, %r112, 200, %r111;
	add.s32 	%r71, %r70, %r9;
	sub.f32 	%f48, %f43, %f40;
	sub.f32 	%f49, %f44, %f41;
	div.rn.f32 	%f50, %f49, %f48;
	abs.f32 	%f11, %f50;
	sub.f32 	%f51, %f45, %f42;
	div.rn.f32 	%f52, %f51, %f48;
	abs.f32 	%f12, %f52;
	div.rn.f32 	%f53, %f51, %f49;
	abs.f32 	%f13, %f53;
	cvt.f64.f32	%fd48, %f43;
	add.f64 	%fd49, %fd48, 0d4020000000000000;
	div.rn.f64 	%fd51, %fd49, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r72, %fd51;
	cvt.f64.f32	%fd52, %f44;
	sub.f64 	%fd53, %fd29, %fd52;
	div.rn.f64 	%fd54, %fd53, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r73, %fd54;
	cvt.f64.f32	%fd55, %f45;
	sub.f64 	%fd57, %fd40, %fd55;
	mul.f64 	%fd58, %fd57, 0d4010000000000000;
	cvt.rzi.s32.f64	%r74, %fd58;
	mad.lo.s32 	%r75, %r73, 200, %r72;
	mad.lo.s32 	%r10, %r74, 40000, %r75;
	setp.ne.s32	%p18, %r71, %r10;
	setp.lt.s32	%p19, %r111, 200;
	and.pred  	%p20, %p18, %p19;
	setp.lt.s32	%p21, %r112, 200;
	and.pred  	%p22, %p20, %p21;
	setp.lt.s32	%p23, %r113, 24;
	and.pred  	%p1, %p23, %p22;
	@!%p1 bra 	BB3_17;
	bra.uni 	BB3_16;

BB3_16:
	ld.u64 	%rd2, [%rd1];
	cvt.s64.s32	%rd3, %r71;
	add.s64 	%rd4, %rd2, %rd3;
	mov.u16 	%rs1, 0;
	st.u8 	[%rd4], %rs1;

BB3_17:
	setp.neu.f32	%p24, %f42, %f45;
	@%p24 bra 	BB3_25;

	@!%p1 bra 	BB3_33;
	bra.uni 	BB3_19;

BB3_19:
	mul.f32 	%f14, %f11, 0f3DA3D70A;

BB3_20:
	div.rn.f32 	%f17, %f78, %f11;
	setp.gtu.f32	%p25, %f79, %f17;
	@%p25 bra 	BB3_22;

	sub.f32 	%f55, %f78, %f14;
	add.s32 	%r111, %r111, %r1;
	setp.gtu.f32	%p28, %f55, 0f00000000;
	selp.b32	%r14, 0, %r2, %p28;
	selp.f32	%f78, %f55, 0f3DA3D70A, %p28;
	mov.f32 	%f79, 0f3DA3D70A;
	mov.u32 	%r110, %r14;
	bra.uni 	BB3_23;

BB3_22:
	sub.f32 	%f79, %f79, %f17;
	mov.f32 	%f78, 0f3DA3D70A;
	mov.u32 	%r110, %r2;

BB3_23:
	mov.u32 	%r16, %r110;
	sub.s32 	%r112, %r112, %r16;
	add.s32 	%r86, %r111, %r9;
	mad.lo.s32 	%r19, %r112, 200, %r86;
	setp.eq.s32	%p33, %r19, %r10;
	setp.gt.s32	%p34, %r111, 199;
	or.pred  	%p35, %p33, %p34;
	setp.gt.s32	%p36, %r112, 199;
	or.pred  	%p37, %p35, %p36;
	@%p37 bra 	BB3_33;

	ld.u64 	%rd5, [%rd1];
	cvt.s64.s32	%rd6, %r19;
	add.s64 	%rd7, %rd5, %rd6;
	mov.u16 	%rs2, 0;
	st.u8 	[%rd7], %rs2;
	bra.uni 	BB3_20;

BB3_25:
	@!%p1 bra 	BB3_33;
	bra.uni 	BB3_26;

BB3_26:
	cvt.rn.f32.f64	%f77, %fd17;
	mul.f32 	%f23, %f11, 0f3DA3D70A;

BB3_27:
	mul.f32 	%f27, %f13, %f78;
	setp.le.f32	%p38, %f77, %f27;
	mul.f32 	%f28, %f12, %f79;
	setp.le.f32	%p39, %f77, %f28;
	and.pred  	%p40, %p39, %p38;
	@%p40 bra 	BB3_31;

	setp.gtu.f32	%p41, %f28, %f27;
	@%p41 bra 	BB3_30;

	sub.f32 	%f58, %f78, %f23;
	add.s32 	%r111, %r111, %r1;
	setp.gtu.f32	%p44, %f58, 0f00000000;
	sub.s32 	%r92, %r38, %r39;
	selp.b32	%r114, 0, %r92, %p44;
	selp.f32	%f78, %f58, 0f3DA3D70A, %p44;
	mov.f32 	%f79, 0f3DA3D70A;
	fma.rn.f32 	%f77, %f12, 0fBDA3D70A, %f77;
	bra.uni 	BB3_32;

BB3_30:
	div.rn.f32 	%f60, %f78, %f11;
	sub.f32 	%f79, %f79, %f60;
	sub.s32 	%r114, %r38, %r39;
	mov.f32 	%f78, 0f3DA3D70A;
	fma.rn.f32 	%f77, %f13, 0fBDA3D70A, %f77;
	bra.uni 	BB3_32;

BB3_31:
	div.rn.f32 	%f62, %f77, %f12;
	sub.f32 	%f63, %f79, %f62;
	div.rn.f32 	%f64, %f77, %f13;
	sub.f32 	%f65, %f78, %f64;
	sub.s32 	%r113, %r113, %r3;
	setp.gtu.f32	%p51, %f63, 0f00000000;
	selp.b32	%r101, 0, %r1, %p51;
	add.s32 	%r111, %r101, %r111;
	selp.f32	%f79, %f63, 0f3DA3D70A, %p51;
	setp.gtu.f32	%p54, %f65, 0f00000000;
	sub.s32 	%r104, %r38, %r39;
	selp.b32	%r114, 0, %r104, %p54;
	selp.f32	%f78, %f65, 0f3DA3D70A, %p54;
	mov.f32 	%f77, 0f3E800000;

BB3_32:
	sub.s32 	%r112, %r112, %r114;
	mad.lo.s32 	%r105, %r113, 40000, %r111;
	mad.lo.s32 	%r35, %r112, 200, %r105;
	setp.ne.s32	%p57, %r35, %r10;
	setp.lt.s32	%p58, %r111, 200;
	and.pred  	%p59, %p57, %p58;
	setp.lt.s32	%p60, %r112, 200;
	and.pred  	%p61, %p59, %p60;
	setp.lt.s32	%p62, %r113, 24;
	and.pred  	%p63, %p62, %p61;
	@%p63 bra 	BB3_34;

BB3_33:
	ret;

BB3_34:
	ld.u64 	%rd8, [%rd1];
	cvt.s64.s32	%rd9, %r35;
	add.s64 	%rd10, %rd8, %rd9;
	mov.u16 	%rs3, 0;
	st.u8 	[%rd10], %rs3;
	bra.uni 	BB3_27;
}

.visible .func  (.param .b64 func_retval0) _Z15x_remaining_GPUdiRi(
	.param .b64 _Z15x_remaining_GPUdiRi_param_0,
	.param .b32 _Z15x_remaining_GPUdiRi_param_1,
	.param .b64 _Z15x_remaining_GPUdiRi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .s32 	%r<13>;
	.reg .s64 	%rd<2>;
	.reg .f64 	%fd<22>;


	ld.param.f64 	%fd9, [_Z15x_remaining_GPUdiRi_param_0];
	ld.param.u32 	%r1, [_Z15x_remaining_GPUdiRi_param_1];
	ld.param.u64 	%rd1, [_Z15x_remaining_GPUdiRi_param_2];
	add.f64 	%fd10, %fd9, 0d4020000000000000;
	div.rn.f64 	%fd1, %fd10, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p1, %fd2, 0d7FF0000000000000;
	@%p1 bra 	BB4_4;

	setp.eq.f64	%p2, %fd2, 0d7FF0000000000000;
	@%p2 bra 	BB4_3;

	add.f64 	%fd20, %fd1, %fd1;
	mov.f64 	%fd21, %fd20;
	bra.uni 	BB4_5;

BB4_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, -2147483648;
	mov.f64 	%fd11, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd11;
	}
	or.b32  	%r5, %r4, %r3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r6, %temp}, %fd11;
	}
	mov.b64 	%fd20, {%r6, %r5};
	mov.f64 	%fd21, %fd1;
	bra.uni 	BB4_5;

BB4_4:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd12, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd1;
	}
	and.b32  	%r10, %r9, -2147483648;
	or.b32  	%r11, %r8, %r10;
	mov.b64 	%fd20, {%r7, %r11};
	mov.f64 	%fd21, %fd5;

BB4_5:
	cvt.rzi.s32.f64	%r12, %fd21;
	st.u32 	[%rd1], %r12;
	mul.f64 	%fd13, %fd20, 0d3FB47AE147AE147B;
	mov.f64 	%fd14, 0d3FB47AE147AE147B;
	sub.f64 	%fd15, %fd14, %fd13;
	setp.gt.s32	%p3, %r1, 0;
	selp.f64	%fd16, %fd15, 0d0000000000000000, %p3;
	setp.lt.s32	%p4, %r1, 1;
	selp.f64	%fd17, %fd13, 0d0000000000000000, %p4;
	add.f64 	%fd18, %fd16, %fd17;
	st.param.f64	[func_retval0+0], %fd18;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z15y_remaining_GPUdiRi(
	.param .b64 _Z15y_remaining_GPUdiRi_param_0,
	.param .b32 _Z15y_remaining_GPUdiRi_param_1,
	.param .b64 _Z15y_remaining_GPUdiRi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .s32 	%r<13>;
	.reg .s64 	%rd<2>;
	.reg .f64 	%fd<23>;


	ld.param.f64 	%fd9, [_Z15y_remaining_GPUdiRi_param_0];
	ld.param.u32 	%r1, [_Z15y_remaining_GPUdiRi_param_1];
	ld.param.u64 	%rd1, [_Z15y_remaining_GPUdiRi_param_2];
	mov.f64 	%fd10, 0d4020000000000000;
	sub.f64 	%fd11, %fd10, %fd9;
	div.rn.f64 	%fd1, %fd11, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p1, %fd2, 0d7FF0000000000000;
	@%p1 bra 	BB5_4;

	setp.eq.f64	%p2, %fd2, 0d7FF0000000000000;
	@%p2 bra 	BB5_3;

	add.f64 	%fd21, %fd1, %fd1;
	mov.f64 	%fd22, %fd21;
	bra.uni 	BB5_5;

BB5_3:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd1;
	}
	and.b32  	%r3, %r2, -2147483648;
	mov.f64 	%fd12, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd12;
	}
	or.b32  	%r5, %r4, %r3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r6, %temp}, %fd12;
	}
	mov.b64 	%fd21, {%r6, %r5};
	mov.f64 	%fd22, %fd1;
	bra.uni 	BB5_5;

BB5_4:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd13, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd13;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd1;
	}
	and.b32  	%r10, %r9, -2147483648;
	or.b32  	%r11, %r8, %r10;
	mov.b64 	%fd21, {%r7, %r11};
	mov.f64 	%fd22, %fd5;

BB5_5:
	cvt.rzi.s32.f64	%r12, %fd22;
	st.u32 	[%rd1], %r12;
	mul.f64 	%fd14, %fd21, 0d3FB47AE147AE147B;
	mov.f64 	%fd15, 0d3FB47AE147AE147B;
	sub.f64 	%fd16, %fd15, %fd14;
	setp.gt.s32	%p3, %r1, 0;
	selp.f64	%fd17, %fd16, 0d0000000000000000, %p3;
	setp.lt.s32	%p4, %r1, 1;
	selp.f64	%fd18, %fd14, 0d0000000000000000, %p4;
	add.f64 	%fd19, %fd17, %fd18;
	st.param.f64	[func_retval0+0], %fd19;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z15z_remaining_GPUdiRi(
	.param .b64 _Z15z_remaining_GPUdiRi_param_0,
	.param .b32 _Z15z_remaining_GPUdiRi_param_1,
	.param .b64 _Z15z_remaining_GPUdiRi_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .s32 	%r<9>;
	.reg .f32 	%f<14>;
	.reg .s64 	%rd<2>;
	.reg .f64 	%fd<11>;


	ld.param.f64 	%fd1, [_Z15z_remaining_GPUdiRi_param_0];
	ld.param.u32 	%r1, [_Z15z_remaining_GPUdiRi_param_1];
	ld.param.u64 	%rd1, [_Z15z_remaining_GPUdiRi_param_2];
	mov.f64 	%fd2, 0d4008000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	mul.f64 	%fd4, %fd3, 0d4010000000000000;
	cvt.rn.f32.f64	%f1, %fd4;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p1, %f2, 0f7F800000;
	@%p1 bra 	BB6_4;

	setp.eq.f32	%p2, %f2, 0f7F800000;
	@%p2 bra 	BB6_3;

	add.f32 	%f12, %f1, %f1;
	mov.f32 	%f13, %f12;
	bra.uni 	BB6_5;

BB6_3:
	mov.b32 	 %r2, %f1;
	and.b32  	%r3, %r2, -2147483648;
	mov.b32 	 %f12, %r3;
	mov.f32 	%f13, %f1;
	bra.uni 	BB6_5;

BB6_4:
	cvt.rzi.f32.f32	%f5, %f1;
	sub.f32 	%f9, %f1, %f5;
	mov.b32 	 %r4, %f9;
	mov.b32 	 %r5, %f1;
	and.b32  	%r6, %r5, -2147483648;
	or.b32  	%r7, %r4, %r6;
	mov.b32 	 %f12, %r7;
	mov.f32 	%f13, %f5;

BB6_5:
	cvt.rzi.s32.f32	%r8, %f13;
	st.u32 	[%rd1], %r8;
	mul.f32 	%f10, %f12, 0f3E800000;
	cvt.f64.f32	%fd5, %f10;
	mov.f64 	%fd6, 0d3FD0000000000000;
	sub.f64 	%fd7, %fd6, %fd5;
	setp.gt.s32	%p3, %r1, 0;
	selp.f64	%fd8, %fd7, 0d0000000000000000, %p3;
	setp.lt.s32	%p4, %r1, 1;
	selp.f64	%fd9, %fd5, 0d0000000000000000, %p4;
	add.f64 	%fd10, %fd8, %fd9;
	st.param.f64	[func_retval0+0], %fd10;
	ret;
}

.visible .func  (.param .b32 func_retval0) _Z20position_2_voxel_GPUddd(
	.param .b64 _Z20position_2_voxel_GPUddd_param_0,
	.param .b64 _Z20position_2_voxel_GPUddd_param_1,
	.param .b64 _Z20position_2_voxel_GPUddd_param_2
)
{
	.reg .s32 	%r<6>;
	.reg .f64 	%fd<12>;


	ld.param.f64 	%fd1, [_Z20position_2_voxel_GPUddd_param_0];
	ld.param.f64 	%fd2, [_Z20position_2_voxel_GPUddd_param_1];
	ld.param.f64 	%fd3, [_Z20position_2_voxel_GPUddd_param_2];
	add.f64 	%fd4, %fd1, 0d4020000000000000;
	mov.f64 	%fd5, 0d4020000000000000;
	div.rn.f64 	%fd6, %fd4, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r1, %fd6;
	sub.f64 	%fd7, %fd5, %fd2;
	div.rn.f64 	%fd8, %fd7, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r2, %fd8;
	mov.f64 	%fd9, 0d4008000000000000;
	sub.f64 	%fd10, %fd9, %fd3;
	mul.f64 	%fd11, %fd10, 0d4010000000000000;
	cvt.rzi.s32.f64	%r3, %fd11;
	mad.lo.s32 	%r4, %r2, 200, %r1;
	mad.lo.s32 	%r5, %r3, 40000, %r4;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

.visible .func _Z16test_func_deviceRiS_S_(
	.param .b64 _Z16test_func_deviceRiS_S__param_0,
	.param .b64 _Z16test_func_deviceRiS_S__param_1,
	.param .b64 _Z16test_func_deviceRiS_S__param_2
)
{
	.reg .s32 	%r<4>;
	.reg .s64 	%rd<4>;


	ld.param.u64 	%rd1, [_Z16test_func_deviceRiS_S__param_0];
	ld.param.u64 	%rd2, [_Z16test_func_deviceRiS_S__param_1];
	ld.param.u64 	%rd3, [_Z16test_func_deviceRiS_S__param_2];
	mov.u32 	%r1, 2;
	st.u32 	[%rd1], %r1;
	mov.u32 	%r2, 3;
	st.u32 	[%rd2], %r2;
	mov.u32 	%r3, 4;
	st.u32 	[%rd3], %r3;
	ret;
}

.visible .entry _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_(
	.param .u32 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_0,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_1,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_2,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_3,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_4,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_5,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_6,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_7,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_8,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_9,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_10,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_11,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_12,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_13,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_14,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_15,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_16,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_17,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_18,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_19,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_20,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_21,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_22,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_23,
	.param .u64 _Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_24
)
{
	.local .align 4 .b8 	__local_depot9[56];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<145>;
	.reg .s16 	%rs<17>;
	.reg .s32 	%r<217>;
	.reg .f32 	%f<211>;
	.reg .s64 	%rd<206>;
	.reg .f64 	%fd<800>;


	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r58, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_0];
	ld.param.u64 	%rd13, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_1];
	ld.param.u64 	%rd15, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_3];
	ld.param.u64 	%rd16, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_4];
	ld.param.u64 	%rd19, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_7];
	ld.param.u64 	%rd20, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_8];
	mov.u32 	%r59, %ctaid.x;
	shl.b32 	%r60, %r59, 10;
	mov.u32 	%r61, %tid.x;
	add.s32 	%r1, %r60, %r61;
	setp.ge.s32	%p5, %r1, %r58;
	@%p5 bra 	BB9_148;

	cvta.to.global.u64 	%rd37, %rd19;
	cvta.to.global.u64 	%rd38, %rd20;
	cvta.to.global.u64 	%rd39, %rd15;
	cvta.to.global.u64 	%rd40, %rd16;
	cvta.to.global.u64 	%rd41, %rd13;
	mul.wide.s32 	%rd42, %r1, 4;
	add.s64 	%rd43, %rd41, %rd42;
	ld.global.u32 	%r62, [%rd43];
	cvt.rn.f64.s32	%fd200, %r62;
	mul.f64 	%fd1, %fd200, 0d3F91DF46A2529D39;
	add.s64 	%rd44, %rd40, %rd42;
	add.s64 	%rd45, %rd39, %rd42;
	ld.global.f32 	%f32, [%rd45];
	ld.global.f32 	%f1, [%rd44];
	sub.f32 	%f33, %f1, %f32;
	add.s64 	%rd46, %rd38, %rd42;
	add.s64 	%rd47, %rd37, %rd42;
	ld.global.f32 	%f34, [%rd47];
	ld.global.f32 	%f2, [%rd46];
	sub.f32 	%f35, %f2, %f34;
	abs.f32 	%f3, %f35;
	abs.f32 	%f4, %f33;
	setp.eq.f32	%p6, %f3, 0f00000000;
	setp.eq.f32	%p7, %f4, 0f00000000;
	and.pred  	%p8, %p6, %p7;
	mov.b32 	 %r2, %f35;
	mov.b32 	 %r63, %f33;
	and.b32  	%r3, %r63, -2147483648;
	@%p8 bra 	BB9_5;

	setp.eq.f32	%p9, %f3, 0f7F800000;
	setp.eq.f32	%p10, %f4, 0f7F800000;
	and.pred  	%p11, %p9, %p10;
	@%p11 bra 	BB9_4;

	max.f32 	%f36, %f4, %f3;
	min.f32 	%f37, %f4, %f3;
	div.rn.f32 	%f38, %f37, %f36;
	mul.rn.f32 	%f39, %f38, %f38;
	mov.f32 	%f40, 0fC0B59883;
	mov.f32 	%f41, 0fBF52C7EA;
	fma.rn.f32 	%f42, %f39, %f41, %f40;
	mov.f32 	%f43, 0fC0D21907;
	fma.rn.f32 	%f44, %f42, %f39, %f43;
	mul.f32 	%f45, %f44, %f39;
	mul.f32 	%f46, %f45, %f38;
	add.f32 	%f47, %f39, 0f41355DC0;
	mov.f32 	%f48, 0f41E6BD60;
	fma.rn.f32 	%f49, %f47, %f39, %f48;
	mov.f32 	%f50, 0f419D92C8;
	fma.rn.f32 	%f51, %f49, %f39, %f50;
	rcp.rn.f32 	%f52, %f51;
	fma.rn.f32 	%f53, %f46, %f52, %f38;
	mov.f32 	%f54, 0f3FC90FDB;
	sub.f32 	%f55, %f54, %f53;
	setp.gt.f32	%p12, %f4, %f3;
	selp.f32	%f56, %f55, %f53, %p12;
	mov.f32 	%f57, 0f40490FDB;
	sub.f32 	%f58, %f57, %f56;
	setp.lt.s32	%p13, %r2, 0;
	selp.f32	%f59, %f58, %f56, %p13;
	mov.b32 	 %r64, %f59;
	or.b32  	%r65, %r64, %r3;
	mov.b32 	 %f60, %r65;
	add.f32 	%f61, %f3, %f4;
	setp.gtu.f32	%p14, %f61, 0f7F800000;
	selp.f32	%f207, %f61, %f60, %p14;
	bra.uni 	BB9_6;

BB9_4:
	shr.s32 	%r66, %r2, 31;
	and.b32  	%r67, %r66, 13483017;
	add.s32 	%r68, %r67, 1061752795;
	or.b32  	%r69, %r68, %r3;
	mov.b32 	 %f207, %r69;
	bra.uni 	BB9_6;

BB9_5:
	shr.s32 	%r70, %r2, 31;
	and.b32  	%r71, %r70, 1078530011;
	or.b32  	%r72, %r71, %r3;
	mov.b32 	 %f207, %r72;

BB9_6:
	cvt.f64.f32	%fd2, %f207;
	setp.gt.f64	%p15, %fd2, 0d3FE921FB54442D18;
	setp.lt.f64	%p16, %fd2, 0d4002D97C7F3321D2;
	and.pred  	%p17, %p15, %p16;
	@!%p17 bra 	BB9_8;
	bra.uni 	BB9_7;

BB9_7:
	mov.pred 	%p143, -1;
	bra.uni 	BB9_9;

BB9_8:
	setp.gt.f64	%p18, %fd2, 0d400F6A7A2955385E;
	setp.lt.f64	%p19, %fd2, 0d4015FDBBE9BBA775;
	and.pred  	%p143, %p18, %p19;

BB9_9:
	cvt.f64.f32	%fd201, %f2;
	cvt.f64.f32	%fd202, %f1;
	neg.f64 	%fd203, %fd202;
	selp.f64	%fd3, %fd203, %fd201, %p143;
	selp.f64	%fd4, %fd201, %fd202, %p143;
	add.f64 	%fd204, %fd2, 0d3FF921FB54442D18;
	selp.f64	%fd743, %fd204, %fd2, %p143;
	abs.f64 	%fd205, %fd743;
	setp.neu.f64	%p21, %fd205, 0d7FF0000000000000;
	@%p21 bra 	BB9_11;

	mov.f64 	%fd206, 0d0000000000000000;
	mul.rn.f64 	%fd743, %fd743, %fd206;

BB9_11:
	add.u64 	%rd48, %SP, 0;
	mul.f64 	%fd207, %fd743, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r203, %fd207;
	cvta.to.local.u64 	%rd49, %rd48;
	st.local.u32 	[%rd49], %r203;
	cvt.rn.f64.s32	%fd208, %r203;
	neg.f64 	%fd209, %fd208;
	mov.f64 	%fd210, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd211, %fd209, %fd210, %fd743;
	mov.f64 	%fd212, 0d3C91A62633145C00;
	fma.rn.f64 	%fd213, %fd209, %fd212, %fd211;
	mov.f64 	%fd214, 0d397B839A252049C0;
	fma.rn.f64 	%fd744, %fd209, %fd214, %fd213;
	abs.f64 	%fd215, %fd743;
	setp.leu.f64	%p22, %fd215, 0d41E0000000000000;
	@%p22 bra 	BB9_13;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd743;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd48;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd744, [retval0+0];
	}
	// Callseq End 1
	ld.local.u32 	%r203, [%rd49];

BB9_13:
	mul.f64 	%fd216, %fd744, %fd744;
	mov.f64 	%fd217, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd218, 0d3EE48DAC2799BCB9;
	fma.rn.f64 	%fd219, %fd218, %fd216, %fd217;
	mov.f64 	%fd220, 0d3F0980E90FD91E04;
	fma.rn.f64 	%fd221, %fd219, %fd216, %fd220;
	mov.f64 	%fd222, 0dBEFAE2B0417D7E1D;
	fma.rn.f64 	%fd223, %fd221, %fd216, %fd222;
	mov.f64 	%fd224, 0d3F119F5341BFBA57;
	fma.rn.f64 	%fd225, %fd223, %fd216, %fd224;
	mov.f64 	%fd226, 0d3F15E791A00F6919;
	fma.rn.f64 	%fd227, %fd225, %fd216, %fd226;
	mov.f64 	%fd228, 0d3F2FF2E7FADEC73A;
	fma.rn.f64 	%fd229, %fd227, %fd216, %fd228;
	mov.f64 	%fd230, 0d3F434BC1B206DA62;
	fma.rn.f64 	%fd231, %fd229, %fd216, %fd230;
	mov.f64 	%fd232, 0d3F57DB18EF2F83F9;
	fma.rn.f64 	%fd233, %fd231, %fd216, %fd232;
	mov.f64 	%fd234, 0d3F6D6D2E7AE49FBC;
	fma.rn.f64 	%fd235, %fd233, %fd216, %fd234;
	mov.f64 	%fd236, 0d3F8226E3A816A776;
	fma.rn.f64 	%fd237, %fd235, %fd216, %fd236;
	mov.f64 	%fd238, 0d3F9664F485D25660;
	fma.rn.f64 	%fd239, %fd237, %fd216, %fd238;
	mov.f64 	%fd240, 0d3FABA1BA1BABF31D;
	fma.rn.f64 	%fd241, %fd239, %fd216, %fd240;
	mov.f64 	%fd242, 0d3FC11111111105D2;
	fma.rn.f64 	%fd243, %fd241, %fd216, %fd242;
	mov.f64 	%fd244, 0d3FD555555555555E;
	fma.rn.f64 	%fd245, %fd243, %fd216, %fd244;
	mul.f64 	%fd11, %fd245, %fd216;
	fma.rn.f64 	%fd745, %fd11, %fd744, %fd744;
	and.b32  	%r73, %r203, 1;
	setp.eq.b32	%p23, %r73, 1;
	@!%p23 bra 	BB9_15;
	bra.uni 	BB9_14;

BB9_14:
	sub.f64 	%fd248, %fd745, %fd744;
	neg.f64 	%fd249, %fd248;
	fma.rn.f64 	%fd250, %fd11, %fd744, %fd249;
	// inline asm
	cvt.rn.f32.f64     %f62,%fd745;
	// inline asm
	// inline asm
	rcp.approx.ftz.f32 %f63,%f62;
	// inline asm
	// inline asm
	cvt.f64.f32        %fd247,%f63;
	// inline asm
	neg.f64 	%fd251, %fd745;
	mov.f64 	%fd252, 0d3FF0000000000000;
	fma.rn.f64 	%fd253, %fd251, %fd247, %fd252;
	fma.rn.f64 	%fd254, %fd253, %fd253, %fd253;
	fma.rn.f64 	%fd255, %fd254, %fd247, %fd247;
	neg.f64 	%fd256, %fd255;
	fma.rn.f64 	%fd257, %fd745, %fd256, %fd252;
	fma.rn.f64 	%fd258, %fd256, %fd250, %fd257;
	fma.rn.f64 	%fd745, %fd258, %fd256, %fd256;

BB9_15:
	mul.f64 	%fd260, %fd745, %fd745;
	mul.f64 	%fd261, %fd745, %fd3;
	sub.f64 	%fd15, %fd4, %fd261;
	mul.f64 	%fd262, %fd15, %fd15;
	add.f64 	%fd263, %fd745, %fd745;
	mul.f64 	%fd16, %fd263, %fd15;
	fma.rn.f64 	%fd17, %fd260, 0d3FF0000000000000, 0d3FF0000000000000;
	mul.f64 	%fd264, %fd16, %fd16;
	fma.rn.f64 	%fd265, %fd262, 0d3FF0000000000000, 0dC050000000000000;
	mul.f64 	%fd266, %fd17, 0dC010000000000000;
	mul.f64 	%fd267, %fd266, %fd265;
	fma.rn.f64 	%fd18, %fd264, 0d3FF0000000000000, %fd267;
	setp.gt.f64	%p24, %fd18, 0d0000000000000000;
	setp.leu.f64	%p25, %fd18, 0d0000000000000000;
	selp.u16	%rs15, 1, 0, %p24;
	@%p25 bra 	BB9_17;

	sqrt.rn.f64 	%fd268, %fd18;
	sub.f64 	%fd269, %fd268, %fd16;
	add.f64 	%fd270, %fd17, %fd17;
	div.rn.f64 	%fd271, %fd269, %fd270;
	sub.f64 	%fd272, %fd271, %fd3;
	mul.f64 	%fd273, %fd272, %fd272;
	add.f64 	%fd274, %fd268, %fd16;
	div.rn.f64 	%fd275, %fd274, %fd270;
	fma.rn.f64 	%fd276, %fd745, %fd271, %fd15;
	mul.f64 	%fd277, %fd745, %fd275;
	sub.f64 	%fd278, %fd277, %fd15;
	sub.f64 	%fd279, %fd276, %fd4;
	mul.f64 	%fd280, %fd279, %fd279;
	mul.f64 	%fd281, %fd280, 0d3FF0000000000000;
	add.f64 	%fd282, %fd275, %fd3;
	mul.f64 	%fd283, %fd282, %fd282;
	fma.rn.f64 	%fd284, %fd273, 0d3FF0000000000000, %fd281;
	add.f64 	%fd285, %fd278, %fd4;
	mul.f64 	%fd286, %fd285, %fd285;
	mul.f64 	%fd287, %fd286, 0d3FF0000000000000;
	fma.rn.f64 	%fd288, %fd283, 0d3FF0000000000000, %fd287;
	setp.le.f64	%p26, %fd284, %fd288;
	selp.f64	%fd289, %fd271, 0d0000000000000000, %p26;
	setp.gt.f64	%p27, %fd284, %fd288;
	selp.f64	%fd290, %fd275, 0d0000000000000000, %p27;
	sub.f64 	%fd747, %fd289, %fd290;
	selp.f64	%fd291, %fd276, 0d0000000000000000, %p26;
	selp.f64	%fd292, %fd278, 0d0000000000000000, %p27;
	sub.f64 	%fd746, %fd291, %fd292;

BB9_17:
	ld.param.u64 	%rd185, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_21];
	neg.f64 	%fd293, %fd747;
	selp.f64	%fd23, %fd293, %fd746, %p143;
	selp.f64	%fd24, %fd746, %fd747, %p143;
	add.f64 	%fd294, %fd2, %fd1;
	cvt.rn.f32.f64	%f66, %fd294;
	cvta.to.global.u64 	%rd52, %rd185;
	mul.wide.s32 	%rd53, %r1, 4;
	add.s64 	%rd54, %rd52, %rd53;
	st.global.f32 	[%rd54], %f66;
	abs.f64 	%fd25, %fd1;
	setp.neu.f64	%p28, %fd25, 0d7FF0000000000000;
	mov.f64 	%fd797, %fd1;
	@%p28 bra 	BB9_19;

	mov.f64 	%fd295, 0d0000000000000000;
	mul.rn.f64 	%fd26, %fd1, %fd295;
	mov.f64 	%fd797, %fd26;

BB9_19:
	mov.f64 	%fd27, %fd797;
	add.u64 	%rd55, %SP, 4;
	mul.f64 	%fd296, %fd27, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r204, %fd296;
	cvta.to.local.u64 	%rd56, %rd55;
	st.local.u32 	[%rd56], %r204;
	cvt.rn.f64.s32	%fd297, %r204;
	neg.f64 	%fd298, %fd297;
	fma.rn.f64 	%fd300, %fd298, %fd210, %fd27;
	fma.rn.f64 	%fd302, %fd298, %fd212, %fd300;
	fma.rn.f64 	%fd748, %fd298, %fd214, %fd302;
	abs.f64 	%fd304, %fd27;
	setp.leu.f64	%p29, %fd304, 0d41E0000000000000;
	@%p29 bra 	BB9_21;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd27;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd55;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd748, [retval0+0];
	}
	// Callseq End 2
	ld.local.u32 	%r204, [%rd56];

BB9_21:
	add.s32 	%r10, %r204, 1;
	shl.b32 	%r78, %r10, 3;
	and.b32  	%r79, %r78, 8;
	and.b32  	%r80, %r10, 1;
	setp.eq.b32	%p30, %r80, 1;
	not.pred 	%p31, %p30;
	selp.f64	%fd305, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p31;
	mul.wide.u32 	%rd59, %r79, 8;
	mov.u64 	%rd60, __cudart_sin_cos_coeffs;
	add.s64 	%rd61, %rd60, %rd59;
	ld.const.f64 	%fd306, [%rd61+8];
	mul.rn.f64 	%fd31, %fd748, %fd748;
	fma.rn.f64 	%fd307, %fd305, %fd31, %fd306;
	ld.const.f64 	%fd308, [%rd61+16];
	fma.rn.f64 	%fd309, %fd307, %fd31, %fd308;
	ld.const.f64 	%fd310, [%rd61+24];
	fma.rn.f64 	%fd311, %fd309, %fd31, %fd310;
	ld.const.f64 	%fd312, [%rd61+32];
	fma.rn.f64 	%fd313, %fd311, %fd31, %fd312;
	ld.const.f64 	%fd314, [%rd61+40];
	fma.rn.f64 	%fd315, %fd313, %fd31, %fd314;
	ld.const.f64 	%fd316, [%rd61+48];
	fma.rn.f64 	%fd32, %fd315, %fd31, %fd316;
	fma.rn.f64 	%fd749, %fd32, %fd748, %fd748;
	@%p31 bra 	BB9_23;

	mov.f64 	%fd317, 0d3FF0000000000000;
	fma.rn.f64 	%fd749, %fd32, %fd31, %fd317;

BB9_23:
	and.b32  	%r81, %r10, 2;
	setp.eq.s32	%p32, %r81, 0;
	@%p32 bra 	BB9_25;

	mov.f64 	%fd318, 0d0000000000000000;
	mov.f64 	%fd319, 0dBFF0000000000000;
	fma.rn.f64 	%fd749, %fd749, %fd319, %fd318;

BB9_25:
	mov.f64 	%fd796, %fd1;
	@%p28 bra 	BB9_27;

	mov.f64 	%fd320, 0d0000000000000000;
	mul.rn.f64 	%fd796, %fd1, %fd320;

BB9_27:
	add.u64 	%rd62, %SP, 8;
	mul.f64 	%fd321, %fd796, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r205, %fd321;
	cvta.to.local.u64 	%rd63, %rd62;
	st.local.u32 	[%rd63], %r205;
	cvt.rn.f64.s32	%fd322, %r205;
	neg.f64 	%fd323, %fd322;
	fma.rn.f64 	%fd325, %fd323, %fd210, %fd796;
	fma.rn.f64 	%fd327, %fd323, %fd212, %fd325;
	fma.rn.f64 	%fd750, %fd323, %fd214, %fd327;
	abs.f64 	%fd329, %fd796;
	setp.leu.f64	%p34, %fd329, 0d41E0000000000000;
	@%p34 bra 	BB9_29;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd796;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd62;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd750, [retval0+0];
	}
	// Callseq End 3
	ld.local.u32 	%r205, [%rd63];

BB9_29:
	shl.b32 	%r82, %r205, 3;
	and.b32  	%r83, %r82, 8;
	and.b32  	%r84, %r205, 1;
	setp.eq.b32	%p35, %r84, 1;
	not.pred 	%p36, %p35;
	selp.f64	%fd330, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p36;
	mul.wide.u32 	%rd66, %r83, 8;
	add.s64 	%rd68, %rd60, %rd66;
	ld.const.f64 	%fd331, [%rd68+8];
	mul.rn.f64 	%fd43, %fd750, %fd750;
	fma.rn.f64 	%fd332, %fd330, %fd43, %fd331;
	ld.const.f64 	%fd333, [%rd68+16];
	fma.rn.f64 	%fd334, %fd332, %fd43, %fd333;
	ld.const.f64 	%fd335, [%rd68+24];
	fma.rn.f64 	%fd336, %fd334, %fd43, %fd335;
	ld.const.f64 	%fd337, [%rd68+32];
	fma.rn.f64 	%fd338, %fd336, %fd43, %fd337;
	ld.const.f64 	%fd339, [%rd68+40];
	fma.rn.f64 	%fd340, %fd338, %fd43, %fd339;
	ld.const.f64 	%fd341, [%rd68+48];
	fma.rn.f64 	%fd44, %fd340, %fd43, %fd341;
	fma.rn.f64 	%fd751, %fd44, %fd750, %fd750;
	@%p36 bra 	BB9_31;

	mov.f64 	%fd342, 0d3FF0000000000000;
	fma.rn.f64 	%fd751, %fd44, %fd43, %fd342;

BB9_31:
	and.b32  	%r85, %r205, 2;
	setp.eq.s32	%p37, %r85, 0;
	@%p37 bra 	BB9_33;

	mov.f64 	%fd343, 0d0000000000000000;
	mov.f64 	%fd344, 0dBFF0000000000000;
	fma.rn.f64 	%fd751, %fd751, %fd344, %fd343;

BB9_33:
	ld.param.u64 	%rd186, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_15];
	mul.f64 	%fd345, %fd749, %fd24;
	mul.f64 	%fd346, %fd751, %fd23;
	sub.f64 	%fd347, %fd345, %fd346;
	cvt.rn.f32.f64	%f67, %fd347;
	cvta.to.global.u64 	%rd69, %rd186;
	mul.wide.s32 	%rd70, %r1, 4;
	add.s64 	%rd1, %rd69, %rd70;
	st.global.f32 	[%rd1], %f67;
	mov.f64 	%fd795, %fd1;
	@%p28 bra 	BB9_35;

	mov.f64 	%fd348, 0d0000000000000000;
	mul.rn.f64 	%fd795, %fd1, %fd348;

BB9_35:
	add.u64 	%rd71, %SP, 12;
	mul.f64 	%fd349, %fd795, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r206, %fd349;
	cvta.to.local.u64 	%rd72, %rd71;
	st.local.u32 	[%rd72], %r206;
	cvt.rn.f64.s32	%fd350, %r206;
	neg.f64 	%fd351, %fd350;
	fma.rn.f64 	%fd353, %fd351, %fd210, %fd795;
	fma.rn.f64 	%fd355, %fd351, %fd212, %fd353;
	fma.rn.f64 	%fd752, %fd351, %fd214, %fd355;
	abs.f64 	%fd357, %fd795;
	setp.leu.f64	%p39, %fd357, 0d41E0000000000000;
	@%p39 bra 	BB9_37;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd795;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd71;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd752, [retval0+0];
	}
	// Callseq End 4
	ld.local.u32 	%r206, [%rd72];

BB9_37:
	shl.b32 	%r90, %r206, 3;
	and.b32  	%r91, %r90, 8;
	and.b32  	%r92, %r206, 1;
	setp.eq.b32	%p40, %r92, 1;
	not.pred 	%p41, %p40;
	selp.f64	%fd358, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p41;
	mul.wide.u32 	%rd75, %r91, 8;
	add.s64 	%rd77, %rd60, %rd75;
	ld.const.f64 	%fd359, [%rd77+8];
	mul.rn.f64 	%fd55, %fd752, %fd752;
	fma.rn.f64 	%fd360, %fd358, %fd55, %fd359;
	ld.const.f64 	%fd361, [%rd77+16];
	fma.rn.f64 	%fd362, %fd360, %fd55, %fd361;
	ld.const.f64 	%fd363, [%rd77+24];
	fma.rn.f64 	%fd364, %fd362, %fd55, %fd363;
	ld.const.f64 	%fd365, [%rd77+32];
	fma.rn.f64 	%fd366, %fd364, %fd55, %fd365;
	ld.const.f64 	%fd367, [%rd77+40];
	fma.rn.f64 	%fd368, %fd366, %fd55, %fd367;
	ld.const.f64 	%fd369, [%rd77+48];
	fma.rn.f64 	%fd56, %fd368, %fd55, %fd369;
	fma.rn.f64 	%fd753, %fd56, %fd752, %fd752;
	@%p41 bra 	BB9_39;

	mov.f64 	%fd370, 0d3FF0000000000000;
	fma.rn.f64 	%fd753, %fd56, %fd55, %fd370;

BB9_39:
	and.b32  	%r93, %r206, 2;
	setp.eq.s32	%p42, %r93, 0;
	@%p42 bra 	BB9_41;

	mov.f64 	%fd371, 0d0000000000000000;
	mov.f64 	%fd372, 0dBFF0000000000000;
	fma.rn.f64 	%fd753, %fd753, %fd372, %fd371;

BB9_41:
	mul.f64 	%fd62, %fd753, %fd24;
	mov.f64 	%fd794, %fd1;
	@%p28 bra 	BB9_43;

	mov.f64 	%fd373, 0d0000000000000000;
	mul.rn.f64 	%fd794, %fd1, %fd373;

BB9_43:
	add.u64 	%rd78, %SP, 16;
	mul.f64 	%fd374, %fd794, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r207, %fd374;
	cvta.to.local.u64 	%rd79, %rd78;
	st.local.u32 	[%rd79], %r207;
	cvt.rn.f64.s32	%fd375, %r207;
	neg.f64 	%fd376, %fd375;
	fma.rn.f64 	%fd378, %fd376, %fd210, %fd794;
	fma.rn.f64 	%fd380, %fd376, %fd212, %fd378;
	fma.rn.f64 	%fd754, %fd376, %fd214, %fd380;
	abs.f64 	%fd382, %fd794;
	setp.leu.f64	%p44, %fd382, 0d41E0000000000000;
	@%p44 bra 	BB9_45;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd794;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd78;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd754, [retval0+0];
	}
	// Callseq End 5
	ld.local.u32 	%r207, [%rd79];

BB9_45:
	add.s32 	%r20, %r207, 1;
	shl.b32 	%r94, %r20, 3;
	and.b32  	%r95, %r94, 8;
	and.b32  	%r96, %r20, 1;
	setp.eq.b32	%p45, %r96, 1;
	not.pred 	%p46, %p45;
	selp.f64	%fd383, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p46;
	mul.wide.u32 	%rd82, %r95, 8;
	add.s64 	%rd84, %rd60, %rd82;
	ld.const.f64 	%fd384, [%rd84+8];
	mul.rn.f64 	%fd68, %fd754, %fd754;
	fma.rn.f64 	%fd385, %fd383, %fd68, %fd384;
	ld.const.f64 	%fd386, [%rd84+16];
	fma.rn.f64 	%fd387, %fd385, %fd68, %fd386;
	ld.const.f64 	%fd388, [%rd84+24];
	fma.rn.f64 	%fd389, %fd387, %fd68, %fd388;
	ld.const.f64 	%fd390, [%rd84+32];
	fma.rn.f64 	%fd391, %fd389, %fd68, %fd390;
	ld.const.f64 	%fd392, [%rd84+40];
	fma.rn.f64 	%fd393, %fd391, %fd68, %fd392;
	ld.const.f64 	%fd394, [%rd84+48];
	fma.rn.f64 	%fd69, %fd393, %fd68, %fd394;
	fma.rn.f64 	%fd755, %fd69, %fd754, %fd754;
	@%p46 bra 	BB9_47;

	mov.f64 	%fd395, 0d3FF0000000000000;
	fma.rn.f64 	%fd755, %fd69, %fd68, %fd395;

BB9_47:
	and.b32  	%r97, %r20, 2;
	setp.eq.s32	%p47, %r97, 0;
	@%p47 bra 	BB9_49;

	mov.f64 	%fd396, 0d0000000000000000;
	mov.f64 	%fd397, 0dBFF0000000000000;
	fma.rn.f64 	%fd755, %fd755, %fd397, %fd396;

BB9_49:
	ld.param.u64 	%rd191, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_9];
	ld.param.u64 	%rd190, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_10];
	ld.param.u64 	%rd189, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_5];
	ld.param.u64 	%rd188, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_6];
	ld.param.u64 	%rd187, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_16];
	fma.rn.f64 	%fd398, %fd755, %fd23, %fd62;
	cvt.rn.f32.f64	%f68, %fd398;
	cvta.to.global.u64 	%rd85, %rd187;
	mul.wide.s32 	%rd86, %r1, 4;
	add.s64 	%rd2, %rd85, %rd86;
	st.global.f32 	[%rd2], %f68;
	cvta.to.global.u64 	%rd87, %rd188;
	add.s64 	%rd88, %rd87, %rd86;
	cvta.to.global.u64 	%rd89, %rd189;
	add.s64 	%rd3, %rd89, %rd86;
	ld.global.f32 	%f9, [%rd3];
	ld.global.f32 	%f69, [%rd88];
	sub.f32 	%f70, %f69, %f9;
	cvta.to.global.u64 	%rd90, %rd190;
	add.s64 	%rd4, %rd90, %rd86;
	cvta.to.global.u64 	%rd91, %rd191;
	add.s64 	%rd5, %rd91, %rd86;
	ld.global.f32 	%f10, [%rd5];
	ld.global.f32 	%f71, [%rd4];
	sub.f32 	%f72, %f71, %f10;
	abs.f32 	%f11, %f72;
	abs.f32 	%f12, %f70;
	setp.eq.f32	%p48, %f11, 0f00000000;
	setp.eq.f32	%p49, %f12, 0f00000000;
	and.pred  	%p50, %p48, %p49;
	mov.b32 	 %r21, %f72;
	mov.b32 	 %r102, %f70;
	and.b32  	%r22, %r102, -2147483648;
	@%p50 bra 	BB9_53;

	setp.eq.f32	%p51, %f11, 0f7F800000;
	setp.eq.f32	%p52, %f12, 0f7F800000;
	and.pred  	%p53, %p51, %p52;
	@%p53 bra 	BB9_52;

	max.f32 	%f73, %f12, %f11;
	min.f32 	%f74, %f12, %f11;
	div.rn.f32 	%f75, %f74, %f73;
	mul.rn.f32 	%f76, %f75, %f75;
	mov.f32 	%f77, 0fC0B59883;
	mov.f32 	%f78, 0fBF52C7EA;
	fma.rn.f32 	%f79, %f76, %f78, %f77;
	mov.f32 	%f80, 0fC0D21907;
	fma.rn.f32 	%f81, %f79, %f76, %f80;
	mul.f32 	%f82, %f81, %f76;
	mul.f32 	%f83, %f82, %f75;
	add.f32 	%f84, %f76, 0f41355DC0;
	mov.f32 	%f85, 0f41E6BD60;
	fma.rn.f32 	%f86, %f84, %f76, %f85;
	mov.f32 	%f87, 0f419D92C8;
	fma.rn.f32 	%f88, %f86, %f76, %f87;
	rcp.rn.f32 	%f89, %f88;
	fma.rn.f32 	%f90, %f83, %f89, %f75;
	mov.f32 	%f91, 0f3FC90FDB;
	sub.f32 	%f92, %f91, %f90;
	setp.gt.f32	%p54, %f12, %f11;
	selp.f32	%f93, %f92, %f90, %p54;
	mov.f32 	%f94, 0f40490FDB;
	sub.f32 	%f95, %f94, %f93;
	setp.lt.s32	%p55, %r21, 0;
	selp.f32	%f96, %f95, %f93, %p55;
	mov.b32 	 %r103, %f96;
	or.b32  	%r104, %r103, %r22;
	mov.b32 	 %f97, %r104;
	add.f32 	%f98, %f11, %f12;
	setp.gtu.f32	%p56, %f98, 0f7F800000;
	selp.f32	%f208, %f98, %f97, %p56;
	bra.uni 	BB9_54;

BB9_52:
	shr.s32 	%r105, %r21, 31;
	and.b32  	%r106, %r105, 13483017;
	add.s32 	%r107, %r106, 1061752795;
	or.b32  	%r108, %r107, %r22;
	mov.b32 	 %f208, %r108;
	bra.uni 	BB9_54;

BB9_53:
	shr.s32 	%r109, %r21, 31;
	and.b32  	%r110, %r109, 1078530011;
	or.b32  	%r111, %r110, %r22;
	mov.b32 	 %f208, %r111;

BB9_54:
	cvt.f64.f32	%fd75, %f208;
	setp.gt.f64	%p57, %fd75, 0d3FE921FB54442D18;
	setp.lt.f64	%p58, %fd75, 0d4002D97C7F3321D2;
	and.pred  	%p59, %p57, %p58;
	@!%p59 bra 	BB9_56;
	bra.uni 	BB9_55;

BB9_55:
	mov.pred 	%p144, -1;
	bra.uni 	BB9_57;

BB9_56:
	setp.gt.f64	%p60, %fd75, 0d400F6A7A2955385E;
	setp.lt.f64	%p61, %fd75, 0d4015FDBBE9BBA775;
	and.pred  	%p144, %p60, %p61;

BB9_57:
	cvt.f64.f32	%fd399, %f10;
	cvt.f64.f32	%fd400, %f9;
	neg.f64 	%fd401, %fd400;
	selp.f64	%fd76, %fd401, %fd399, %p144;
	selp.f64	%fd77, %fd399, %fd400, %p144;
	add.f64 	%fd402, %fd75, 0d3FF921FB54442D18;
	selp.f64	%fd756, %fd402, %fd75, %p144;
	abs.f64 	%fd403, %fd756;
	setp.neu.f64	%p63, %fd403, 0d7FF0000000000000;
	@%p63 bra 	BB9_59;

	mov.f64 	%fd404, 0d0000000000000000;
	mul.rn.f64 	%fd756, %fd756, %fd404;

BB9_59:
	add.u64 	%rd92, %SP, 20;
	mul.f64 	%fd405, %fd756, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r208, %fd405;
	cvta.to.local.u64 	%rd93, %rd92;
	st.local.u32 	[%rd93], %r208;
	cvt.rn.f64.s32	%fd406, %r208;
	neg.f64 	%fd407, %fd406;
	fma.rn.f64 	%fd409, %fd407, %fd210, %fd756;
	fma.rn.f64 	%fd411, %fd407, %fd212, %fd409;
	fma.rn.f64 	%fd757, %fd407, %fd214, %fd411;
	abs.f64 	%fd413, %fd756;
	setp.leu.f64	%p64, %fd413, 0d41E0000000000000;
	@%p64 bra 	BB9_61;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd756;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd92;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd757, [retval0+0];
	}
	// Callseq End 6
	ld.local.u32 	%r208, [%rd93];

BB9_61:
	mov.f64 	%fd742, 0d3FD555555555555E;
	mov.f64 	%fd741, 0d3FC11111111105D2;
	mov.f64 	%fd740, 0d3FABA1BA1BABF31D;
	mov.f64 	%fd739, 0d3F9664F485D25660;
	mov.f64 	%fd738, 0d3F8226E3A816A776;
	mov.f64 	%fd737, 0d3F6D6D2E7AE49FBC;
	mov.f64 	%fd736, 0d3F57DB18EF2F83F9;
	mov.f64 	%fd735, 0d3F434BC1B206DA62;
	mov.f64 	%fd734, 0d3F2FF2E7FADEC73A;
	mov.f64 	%fd733, 0d3F15E791A00F6919;
	mov.f64 	%fd732, 0d3F119F5341BFBA57;
	mov.f64 	%fd731, 0dBEFAE2B0417D7E1D;
	mov.f64 	%fd730, 0d3F0980E90FD91E04;
	mov.f64 	%fd729, 0dBEF9757C5B27EBB1;
	mov.f64 	%fd728, 0d3EE48DAC2799BCB9;
	mul.f64 	%fd414, %fd757, %fd757;
	fma.rn.f64 	%fd417, %fd728, %fd414, %fd729;
	fma.rn.f64 	%fd419, %fd417, %fd414, %fd730;
	fma.rn.f64 	%fd421, %fd419, %fd414, %fd731;
	fma.rn.f64 	%fd423, %fd421, %fd414, %fd732;
	fma.rn.f64 	%fd425, %fd423, %fd414, %fd733;
	fma.rn.f64 	%fd427, %fd425, %fd414, %fd734;
	fma.rn.f64 	%fd429, %fd427, %fd414, %fd735;
	fma.rn.f64 	%fd431, %fd429, %fd414, %fd736;
	fma.rn.f64 	%fd433, %fd431, %fd414, %fd737;
	fma.rn.f64 	%fd435, %fd433, %fd414, %fd738;
	fma.rn.f64 	%fd437, %fd435, %fd414, %fd739;
	fma.rn.f64 	%fd439, %fd437, %fd414, %fd740;
	fma.rn.f64 	%fd441, %fd439, %fd414, %fd741;
	fma.rn.f64 	%fd443, %fd441, %fd414, %fd742;
	mul.f64 	%fd84, %fd443, %fd414;
	fma.rn.f64 	%fd758, %fd84, %fd757, %fd757;
	and.b32  	%r112, %r208, 1;
	setp.eq.b32	%p65, %r112, 1;
	@!%p65 bra 	BB9_63;
	bra.uni 	BB9_62;

BB9_62:
	sub.f64 	%fd446, %fd758, %fd757;
	neg.f64 	%fd447, %fd446;
	fma.rn.f64 	%fd448, %fd84, %fd757, %fd447;
	// inline asm
	cvt.rn.f32.f64     %f99,%fd758;
	// inline asm
	// inline asm
	rcp.approx.ftz.f32 %f100,%f99;
	// inline asm
	// inline asm
	cvt.f64.f32        %fd445,%f100;
	// inline asm
	neg.f64 	%fd449, %fd758;
	mov.f64 	%fd450, 0d3FF0000000000000;
	fma.rn.f64 	%fd451, %fd449, %fd445, %fd450;
	fma.rn.f64 	%fd452, %fd451, %fd451, %fd451;
	fma.rn.f64 	%fd453, %fd452, %fd445, %fd445;
	neg.f64 	%fd454, %fd453;
	fma.rn.f64 	%fd455, %fd758, %fd454, %fd450;
	fma.rn.f64 	%fd456, %fd454, %fd448, %fd455;
	fma.rn.f64 	%fd758, %fd456, %fd454, %fd454;

BB9_63:
	mul.f64 	%fd458, %fd758, %fd758;
	mul.f64 	%fd459, %fd758, %fd76;
	sub.f64 	%fd88, %fd77, %fd459;
	mul.f64 	%fd460, %fd88, %fd88;
	add.f64 	%fd461, %fd758, %fd758;
	mul.f64 	%fd89, %fd461, %fd88;
	fma.rn.f64 	%fd90, %fd458, 0d3FF0000000000000, 0d3FF0000000000000;
	mul.f64 	%fd462, %fd89, %fd89;
	fma.rn.f64 	%fd463, %fd460, 0d3FF0000000000000, 0dC050000000000000;
	mul.f64 	%fd464, %fd90, 0dC010000000000000;
	mul.f64 	%fd465, %fd464, %fd463;
	fma.rn.f64 	%fd91, %fd462, 0d3FF0000000000000, %fd465;
	setp.gt.f64	%p66, %fd91, 0d0000000000000000;
	setp.leu.f64	%p67, %fd91, 0d0000000000000000;
	selp.u16	%rs16, 1, 0, %p66;
	@%p67 bra 	BB9_65;

	sqrt.rn.f64 	%fd466, %fd91;
	sub.f64 	%fd467, %fd466, %fd89;
	add.f64 	%fd468, %fd90, %fd90;
	div.rn.f64 	%fd469, %fd467, %fd468;
	sub.f64 	%fd470, %fd469, %fd76;
	mul.f64 	%fd471, %fd470, %fd470;
	add.f64 	%fd472, %fd466, %fd89;
	div.rn.f64 	%fd473, %fd472, %fd468;
	fma.rn.f64 	%fd474, %fd758, %fd469, %fd88;
	mul.f64 	%fd475, %fd758, %fd473;
	sub.f64 	%fd476, %fd475, %fd88;
	sub.f64 	%fd477, %fd474, %fd77;
	mul.f64 	%fd478, %fd477, %fd477;
	mul.f64 	%fd479, %fd478, 0d3FF0000000000000;
	add.f64 	%fd480, %fd473, %fd76;
	mul.f64 	%fd481, %fd480, %fd480;
	fma.rn.f64 	%fd482, %fd471, 0d3FF0000000000000, %fd479;
	add.f64 	%fd483, %fd476, %fd77;
	mul.f64 	%fd484, %fd483, %fd483;
	mul.f64 	%fd485, %fd484, 0d3FF0000000000000;
	fma.rn.f64 	%fd486, %fd481, 0d3FF0000000000000, %fd485;
	setp.le.f64	%p68, %fd482, %fd486;
	selp.f64	%fd487, %fd469, 0d0000000000000000, %p68;
	setp.gt.f64	%p69, %fd482, %fd486;
	selp.f64	%fd488, %fd473, 0d0000000000000000, %p69;
	sub.f64 	%fd760, %fd487, %fd488;
	selp.f64	%fd489, %fd474, 0d0000000000000000, %p68;
	selp.f64	%fd490, %fd476, 0d0000000000000000, %p69;
	sub.f64 	%fd759, %fd489, %fd490;

BB9_65:
	ld.param.u64 	%rd192, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_23];
	neg.f64 	%fd491, %fd760;
	selp.f64	%fd96, %fd491, %fd759, %p144;
	selp.f64	%fd97, %fd759, %fd760, %p144;
	add.f64 	%fd492, %fd75, %fd1;
	cvt.rn.f32.f64	%f103, %fd492;
	cvta.to.global.u64 	%rd96, %rd192;
	mul.wide.s32 	%rd97, %r1, 4;
	add.s64 	%rd98, %rd96, %rd97;
	st.global.f32 	[%rd98], %f103;
	mov.f64 	%fd793, %fd1;
	@%p28 bra 	BB9_67;

	mov.f64 	%fd493, 0d0000000000000000;
	mul.rn.f64 	%fd793, %fd1, %fd493;

BB9_67:
	add.u64 	%rd99, %SP, 24;
	mul.f64 	%fd494, %fd793, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r209, %fd494;
	cvta.to.local.u64 	%rd100, %rd99;
	st.local.u32 	[%rd100], %r209;
	cvt.rn.f64.s32	%fd495, %r209;
	neg.f64 	%fd496, %fd495;
	fma.rn.f64 	%fd498, %fd496, %fd210, %fd793;
	fma.rn.f64 	%fd500, %fd496, %fd212, %fd498;
	fma.rn.f64 	%fd761, %fd496, %fd214, %fd500;
	abs.f64 	%fd502, %fd793;
	setp.leu.f64	%p71, %fd502, 0d41E0000000000000;
	@%p71 bra 	BB9_69;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd793;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd99;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd761, [retval0+0];
	}
	// Callseq End 7
	ld.local.u32 	%r209, [%rd100];

BB9_69:
	add.s32 	%r29, %r209, 1;
	shl.b32 	%r117, %r29, 3;
	and.b32  	%r118, %r117, 8;
	and.b32  	%r119, %r29, 1;
	setp.eq.b32	%p72, %r119, 1;
	not.pred 	%p73, %p72;
	selp.f64	%fd503, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p73;
	mul.wide.u32 	%rd103, %r118, 8;
	add.s64 	%rd105, %rd60, %rd103;
	ld.const.f64 	%fd504, [%rd105+8];
	mul.rn.f64 	%fd103, %fd761, %fd761;
	fma.rn.f64 	%fd505, %fd503, %fd103, %fd504;
	ld.const.f64 	%fd506, [%rd105+16];
	fma.rn.f64 	%fd507, %fd505, %fd103, %fd506;
	ld.const.f64 	%fd508, [%rd105+24];
	fma.rn.f64 	%fd509, %fd507, %fd103, %fd508;
	ld.const.f64 	%fd510, [%rd105+32];
	fma.rn.f64 	%fd511, %fd509, %fd103, %fd510;
	ld.const.f64 	%fd512, [%rd105+40];
	fma.rn.f64 	%fd513, %fd511, %fd103, %fd512;
	ld.const.f64 	%fd514, [%rd105+48];
	fma.rn.f64 	%fd104, %fd513, %fd103, %fd514;
	fma.rn.f64 	%fd762, %fd104, %fd761, %fd761;
	@%p73 bra 	BB9_71;

	mov.f64 	%fd515, 0d3FF0000000000000;
	fma.rn.f64 	%fd762, %fd104, %fd103, %fd515;

BB9_71:
	and.b32  	%r120, %r29, 2;
	setp.eq.s32	%p74, %r120, 0;
	@%p74 bra 	BB9_73;

	mov.f64 	%fd516, 0d0000000000000000;
	mov.f64 	%fd517, 0dBFF0000000000000;
	fma.rn.f64 	%fd762, %fd762, %fd517, %fd516;

BB9_73:
	mov.f64 	%fd792, %fd1;
	@%p28 bra 	BB9_75;

	mov.f64 	%fd518, 0d0000000000000000;
	mul.rn.f64 	%fd792, %fd1, %fd518;

BB9_75:
	add.u64 	%rd106, %SP, 28;
	mul.f64 	%fd519, %fd792, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r210, %fd519;
	cvta.to.local.u64 	%rd107, %rd106;
	st.local.u32 	[%rd107], %r210;
	cvt.rn.f64.s32	%fd520, %r210;
	neg.f64 	%fd521, %fd520;
	fma.rn.f64 	%fd523, %fd521, %fd210, %fd792;
	fma.rn.f64 	%fd525, %fd521, %fd212, %fd523;
	fma.rn.f64 	%fd763, %fd521, %fd214, %fd525;
	abs.f64 	%fd527, %fd792;
	setp.leu.f64	%p76, %fd527, 0d41E0000000000000;
	@%p76 bra 	BB9_77;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd792;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd106;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd763, [retval0+0];
	}
	// Callseq End 8
	ld.local.u32 	%r210, [%rd107];

BB9_77:
	shl.b32 	%r121, %r210, 3;
	and.b32  	%r122, %r121, 8;
	and.b32  	%r123, %r210, 1;
	setp.eq.b32	%p77, %r123, 1;
	not.pred 	%p78, %p77;
	selp.f64	%fd528, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p78;
	mul.wide.u32 	%rd110, %r122, 8;
	add.s64 	%rd112, %rd60, %rd110;
	ld.const.f64 	%fd529, [%rd112+8];
	mul.rn.f64 	%fd115, %fd763, %fd763;
	fma.rn.f64 	%fd530, %fd528, %fd115, %fd529;
	ld.const.f64 	%fd531, [%rd112+16];
	fma.rn.f64 	%fd532, %fd530, %fd115, %fd531;
	ld.const.f64 	%fd533, [%rd112+24];
	fma.rn.f64 	%fd534, %fd532, %fd115, %fd533;
	ld.const.f64 	%fd535, [%rd112+32];
	fma.rn.f64 	%fd536, %fd534, %fd115, %fd535;
	ld.const.f64 	%fd537, [%rd112+40];
	fma.rn.f64 	%fd538, %fd536, %fd115, %fd537;
	ld.const.f64 	%fd539, [%rd112+48];
	fma.rn.f64 	%fd116, %fd538, %fd115, %fd539;
	fma.rn.f64 	%fd764, %fd116, %fd763, %fd763;
	@%p78 bra 	BB9_79;

	mov.f64 	%fd540, 0d3FF0000000000000;
	fma.rn.f64 	%fd764, %fd116, %fd115, %fd540;

BB9_79:
	and.b32  	%r124, %r210, 2;
	setp.eq.s32	%p79, %r124, 0;
	@%p79 bra 	BB9_81;

	mov.f64 	%fd541, 0d0000000000000000;
	mov.f64 	%fd542, 0dBFF0000000000000;
	fma.rn.f64 	%fd764, %fd764, %fd542, %fd541;

BB9_81:
	ld.param.u64 	%rd193, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_18];
	mul.f64 	%fd543, %fd762, %fd97;
	mul.f64 	%fd544, %fd764, %fd96;
	sub.f64 	%fd545, %fd543, %fd544;
	cvt.rn.f32.f64	%f104, %fd545;
	cvta.to.global.u64 	%rd113, %rd193;
	mul.wide.s32 	%rd114, %r1, 4;
	add.s64 	%rd6, %rd113, %rd114;
	st.global.f32 	[%rd6], %f104;
	mov.f64 	%fd791, %fd1;
	@%p28 bra 	BB9_83;

	mov.f64 	%fd546, 0d0000000000000000;
	mul.rn.f64 	%fd791, %fd1, %fd546;

BB9_83:
	add.u64 	%rd115, %SP, 32;
	mul.f64 	%fd547, %fd791, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r211, %fd547;
	cvta.to.local.u64 	%rd116, %rd115;
	st.local.u32 	[%rd116], %r211;
	cvt.rn.f64.s32	%fd548, %r211;
	neg.f64 	%fd549, %fd548;
	fma.rn.f64 	%fd551, %fd549, %fd210, %fd791;
	fma.rn.f64 	%fd553, %fd549, %fd212, %fd551;
	fma.rn.f64 	%fd765, %fd549, %fd214, %fd553;
	abs.f64 	%fd555, %fd791;
	setp.leu.f64	%p81, %fd555, 0d41E0000000000000;
	@%p81 bra 	BB9_85;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd791;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd115;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd765, [retval0+0];
	}
	// Callseq End 9
	ld.local.u32 	%r211, [%rd116];

BB9_85:
	shl.b32 	%r129, %r211, 3;
	and.b32  	%r130, %r129, 8;
	and.b32  	%r131, %r211, 1;
	setp.eq.b32	%p82, %r131, 1;
	not.pred 	%p83, %p82;
	selp.f64	%fd556, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p83;
	mul.wide.u32 	%rd119, %r130, 8;
	add.s64 	%rd121, %rd60, %rd119;
	ld.const.f64 	%fd557, [%rd121+8];
	mul.rn.f64 	%fd127, %fd765, %fd765;
	fma.rn.f64 	%fd558, %fd556, %fd127, %fd557;
	ld.const.f64 	%fd559, [%rd121+16];
	fma.rn.f64 	%fd560, %fd558, %fd127, %fd559;
	ld.const.f64 	%fd561, [%rd121+24];
	fma.rn.f64 	%fd562, %fd560, %fd127, %fd561;
	ld.const.f64 	%fd563, [%rd121+32];
	fma.rn.f64 	%fd564, %fd562, %fd127, %fd563;
	ld.const.f64 	%fd565, [%rd121+40];
	fma.rn.f64 	%fd566, %fd564, %fd127, %fd565;
	ld.const.f64 	%fd567, [%rd121+48];
	fma.rn.f64 	%fd128, %fd566, %fd127, %fd567;
	fma.rn.f64 	%fd766, %fd128, %fd765, %fd765;
	@%p83 bra 	BB9_87;

	mov.f64 	%fd568, 0d3FF0000000000000;
	fma.rn.f64 	%fd766, %fd128, %fd127, %fd568;

BB9_87:
	and.b32  	%r132, %r211, 2;
	setp.eq.s32	%p84, %r132, 0;
	@%p84 bra 	BB9_89;

	mov.f64 	%fd569, 0d0000000000000000;
	mov.f64 	%fd570, 0dBFF0000000000000;
	fma.rn.f64 	%fd766, %fd766, %fd570, %fd569;

BB9_89:
	mul.f64 	%fd134, %fd766, %fd97;
	mov.f64 	%fd790, %fd1;
	@%p28 bra 	BB9_91;

	mov.f64 	%fd571, 0d0000000000000000;
	mul.rn.f64 	%fd790, %fd1, %fd571;

BB9_91:
	add.u64 	%rd122, %SP, 36;
	mul.f64 	%fd572, %fd790, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r212, %fd572;
	cvta.to.local.u64 	%rd123, %rd122;
	st.local.u32 	[%rd123], %r212;
	cvt.rn.f64.s32	%fd573, %r212;
	neg.f64 	%fd574, %fd573;
	fma.rn.f64 	%fd576, %fd574, %fd210, %fd790;
	fma.rn.f64 	%fd578, %fd574, %fd212, %fd576;
	fma.rn.f64 	%fd767, %fd574, %fd214, %fd578;
	abs.f64 	%fd580, %fd790;
	setp.leu.f64	%p86, %fd580, 0d41E0000000000000;
	@%p86 bra 	BB9_93;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd790;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd122;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd767, [retval0+0];
	}
	// Callseq End 10
	ld.local.u32 	%r212, [%rd123];

BB9_93:
	add.s32 	%r39, %r212, 1;
	shl.b32 	%r133, %r39, 3;
	and.b32  	%r134, %r133, 8;
	and.b32  	%r135, %r39, 1;
	setp.eq.b32	%p87, %r135, 1;
	not.pred 	%p88, %p87;
	selp.f64	%fd581, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p88;
	mul.wide.u32 	%rd126, %r134, 8;
	add.s64 	%rd128, %rd60, %rd126;
	ld.const.f64 	%fd582, [%rd128+8];
	mul.rn.f64 	%fd140, %fd767, %fd767;
	fma.rn.f64 	%fd583, %fd581, %fd140, %fd582;
	ld.const.f64 	%fd584, [%rd128+16];
	fma.rn.f64 	%fd585, %fd583, %fd140, %fd584;
	ld.const.f64 	%fd586, [%rd128+24];
	fma.rn.f64 	%fd587, %fd585, %fd140, %fd586;
	ld.const.f64 	%fd588, [%rd128+32];
	fma.rn.f64 	%fd589, %fd587, %fd140, %fd588;
	ld.const.f64 	%fd590, [%rd128+40];
	fma.rn.f64 	%fd591, %fd589, %fd140, %fd590;
	ld.const.f64 	%fd592, [%rd128+48];
	fma.rn.f64 	%fd141, %fd591, %fd140, %fd592;
	fma.rn.f64 	%fd768, %fd141, %fd767, %fd767;
	@%p88 bra 	BB9_95;

	mov.f64 	%fd593, 0d3FF0000000000000;
	fma.rn.f64 	%fd768, %fd141, %fd140, %fd593;

BB9_95:
	and.b32  	%r136, %r39, 2;
	setp.eq.s32	%p89, %r136, 0;
	@%p89 bra 	BB9_97;

	mov.f64 	%fd594, 0d0000000000000000;
	mov.f64 	%fd595, 0dBFF0000000000000;
	fma.rn.f64 	%fd768, %fd768, %fd595, %fd594;

BB9_97:
	ld.param.u64 	%rd198, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_13];
	ld.param.u64 	%rd197, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_14];
	ld.param.u64 	%rd196, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_11];
	ld.param.u64 	%rd195, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_12];
	ld.param.u64 	%rd194, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_19];
	fma.rn.f64 	%fd596, %fd768, %fd96, %fd134;
	cvt.rn.f32.f64	%f105, %fd596;
	cvta.to.global.u64 	%rd129, %rd194;
	mul.wide.s32 	%rd130, %r1, 4;
	add.s64 	%rd7, %rd129, %rd130;
	st.global.f32 	[%rd7], %f105;
	cvta.to.global.u64 	%rd131, %rd195;
	add.s64 	%rd8, %rd131, %rd130;
	cvta.to.global.u64 	%rd132, %rd196;
	add.s64 	%rd133, %rd132, %rd130;
	ld.global.f32 	%f106, [%rd133];
	ld.global.f32 	%f107, [%rd8];
	sub.f32 	%f108, %f107, %f106;
	add.s64 	%rd135, %rd38, %rd130;
	add.s64 	%rd137, %rd37, %rd130;
	ld.global.f32 	%f109, [%rd137];
	ld.global.f32 	%f110, [%rd135];
	sub.f32 	%f111, %f110, %f109;
	div.rn.f32 	%f112, %f108, %f111;
	cvt.f64.f32	%fd147, %f112;
	cvta.to.global.u64 	%rd138, %rd197;
	add.s64 	%rd9, %rd138, %rd130;
	cvta.to.global.u64 	%rd139, %rd198;
	add.s64 	%rd10, %rd139, %rd130;
	ld.global.f32 	%f113, [%rd10];
	ld.global.f32 	%f114, [%rd9];
	sub.f32 	%f115, %f114, %f113;
	ld.global.f32 	%f116, [%rd5];
	ld.global.f32 	%f117, [%rd4];
	sub.f32 	%f118, %f117, %f116;
	div.rn.f32 	%f119, %f115, %f118;
	cvt.f64.f32	%fd148, %f119;
	abs.f32 	%f17, %f111;
	abs.f32 	%f18, %f108;
	setp.eq.f32	%p90, %f17, 0f00000000;
	setp.eq.f32	%p91, %f18, 0f00000000;
	and.pred  	%p92, %p90, %p91;
	mov.b32 	 %r40, %f111;
	mov.b32 	 %r141, %f108;
	and.b32  	%r41, %r141, -2147483648;
	@%p92 bra 	BB9_101;

	setp.eq.f32	%p93, %f17, 0f7F800000;
	setp.eq.f32	%p94, %f18, 0f7F800000;
	and.pred  	%p95, %p93, %p94;
	@%p95 bra 	BB9_100;

	max.f32 	%f120, %f18, %f17;
	min.f32 	%f121, %f18, %f17;
	div.rn.f32 	%f122, %f121, %f120;
	mul.rn.f32 	%f123, %f122, %f122;
	mov.f32 	%f124, 0fC0B59883;
	mov.f32 	%f125, 0fBF52C7EA;
	fma.rn.f32 	%f126, %f123, %f125, %f124;
	mov.f32 	%f127, 0fC0D21907;
	fma.rn.f32 	%f128, %f126, %f123, %f127;
	mul.f32 	%f129, %f128, %f123;
	mul.f32 	%f130, %f129, %f122;
	add.f32 	%f131, %f123, 0f41355DC0;
	mov.f32 	%f132, 0f41E6BD60;
	fma.rn.f32 	%f133, %f131, %f123, %f132;
	mov.f32 	%f134, 0f419D92C8;
	fma.rn.f32 	%f135, %f133, %f123, %f134;
	rcp.rn.f32 	%f136, %f135;
	fma.rn.f32 	%f137, %f130, %f136, %f122;
	mov.f32 	%f138, 0f3FC90FDB;
	sub.f32 	%f139, %f138, %f137;
	setp.gt.f32	%p96, %f18, %f17;
	selp.f32	%f140, %f139, %f137, %p96;
	mov.f32 	%f141, 0f40490FDB;
	sub.f32 	%f142, %f141, %f140;
	setp.lt.s32	%p97, %r40, 0;
	selp.f32	%f143, %f142, %f140, %p97;
	mov.b32 	 %r142, %f143;
	or.b32  	%r143, %r142, %r41;
	mov.b32 	 %f144, %r143;
	add.f32 	%f145, %f17, %f18;
	setp.gtu.f32	%p98, %f145, 0f7F800000;
	selp.f32	%f209, %f145, %f144, %p98;
	bra.uni 	BB9_102;

BB9_100:
	shr.s32 	%r144, %r40, 31;
	and.b32  	%r145, %r144, 13483017;
	add.s32 	%r146, %r145, 1061752795;
	or.b32  	%r147, %r146, %r41;
	mov.b32 	 %f209, %r147;
	bra.uni 	BB9_102;

BB9_101:
	shr.s32 	%r148, %r40, 31;
	and.b32  	%r149, %r148, 1078530011;
	or.b32  	%r150, %r149, %r41;
	mov.b32 	 %f209, %r150;

BB9_102:
	ld.param.u64 	%rd199, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_22];
	cvta.to.global.u64 	%rd140, %rd199;
	mul.wide.s32 	%rd141, %r1, 4;
	add.s64 	%rd142, %rd140, %rd141;
	st.global.f32 	[%rd142], %f209;
	ld.global.f32 	%f146, [%rd10];
	ld.global.f32 	%f147, [%rd9];
	sub.f32 	%f148, %f147, %f146;
	ld.global.f32 	%f149, [%rd5];
	ld.global.f32 	%f150, [%rd4];
	sub.f32 	%f151, %f150, %f149;
	abs.f32 	%f23, %f151;
	abs.f32 	%f24, %f148;
	setp.eq.f32	%p99, %f23, 0f00000000;
	setp.eq.f32	%p100, %f24, 0f00000000;
	and.pred  	%p101, %p99, %p100;
	mov.b32 	 %r42, %f151;
	mov.b32 	 %r155, %f148;
	and.b32  	%r43, %r155, -2147483648;
	@%p101 bra 	BB9_106;

	setp.eq.f32	%p102, %f23, 0f7F800000;
	setp.eq.f32	%p103, %f24, 0f7F800000;
	and.pred  	%p104, %p102, %p103;
	@%p104 bra 	BB9_105;

	max.f32 	%f152, %f24, %f23;
	min.f32 	%f153, %f24, %f23;
	div.rn.f32 	%f154, %f153, %f152;
	mul.rn.f32 	%f155, %f154, %f154;
	mov.f32 	%f156, 0fC0B59883;
	mov.f32 	%f157, 0fBF52C7EA;
	fma.rn.f32 	%f158, %f155, %f157, %f156;
	mov.f32 	%f159, 0fC0D21907;
	fma.rn.f32 	%f160, %f158, %f155, %f159;
	mul.f32 	%f161, %f160, %f155;
	mul.f32 	%f162, %f161, %f154;
	add.f32 	%f163, %f155, 0f41355DC0;
	mov.f32 	%f164, 0f41E6BD60;
	fma.rn.f32 	%f165, %f163, %f155, %f164;
	mov.f32 	%f166, 0f419D92C8;
	fma.rn.f32 	%f167, %f165, %f155, %f166;
	rcp.rn.f32 	%f168, %f167;
	fma.rn.f32 	%f169, %f162, %f168, %f154;
	mov.f32 	%f170, 0f3FC90FDB;
	sub.f32 	%f171, %f170, %f169;
	setp.gt.f32	%p105, %f24, %f23;
	selp.f32	%f172, %f171, %f169, %p105;
	mov.f32 	%f173, 0f40490FDB;
	sub.f32 	%f174, %f173, %f172;
	setp.lt.s32	%p106, %r42, 0;
	selp.f32	%f175, %f174, %f172, %p106;
	mov.b32 	 %r156, %f175;
	or.b32  	%r157, %r156, %r43;
	mov.b32 	 %f176, %r157;
	add.f32 	%f177, %f23, %f24;
	setp.gtu.f32	%p107, %f177, 0f7F800000;
	selp.f32	%f210, %f177, %f176, %p107;
	bra.uni 	BB9_107;

BB9_105:
	shr.s32 	%r158, %r42, 31;
	and.b32  	%r159, %r158, 13483017;
	add.s32 	%r160, %r159, 1061752795;
	or.b32  	%r161, %r160, %r43;
	mov.b32 	 %f210, %r161;
	bra.uni 	BB9_107;

BB9_106:
	shr.s32 	%r162, %r42, 31;
	and.b32  	%r163, %r162, 1078530011;
	or.b32  	%r164, %r163, %r43;
	mov.b32 	 %f210, %r164;

BB9_107:
	ld.param.u64 	%rd200, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_24];
	cvta.to.global.u64 	%rd143, %rd200;
	mul.wide.s32 	%rd144, %r1, 4;
	add.s64 	%rd145, %rd143, %rd144;
	st.global.f32 	[%rd145], %f210;
	mov.f64 	%fd789, %fd1;
	@%p28 bra 	BB9_109;

	mov.f64 	%fd597, 0d0000000000000000;
	mul.rn.f64 	%fd789, %fd1, %fd597;

BB9_109:
	add.u64 	%rd146, %SP, 40;
	mul.f64 	%fd598, %fd789, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r213, %fd598;
	cvta.to.local.u64 	%rd147, %rd146;
	st.local.u32 	[%rd147], %r213;
	cvt.rn.f64.s32	%fd599, %r213;
	neg.f64 	%fd600, %fd599;
	fma.rn.f64 	%fd602, %fd600, %fd210, %fd789;
	fma.rn.f64 	%fd604, %fd600, %fd212, %fd602;
	fma.rn.f64 	%fd769, %fd600, %fd214, %fd604;
	abs.f64 	%fd606, %fd789;
	setp.leu.f64	%p109, %fd606, 0d41E0000000000000;
	@%p109 bra 	BB9_111;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd789;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd146;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd769, [retval0+0];
	}
	// Callseq End 11
	ld.local.u32 	%r213, [%rd147];

BB9_111:
	add.s32 	%r47, %r213, 1;
	shl.b32 	%r169, %r47, 3;
	and.b32  	%r170, %r169, 8;
	and.b32  	%r171, %r47, 1;
	setp.eq.b32	%p110, %r171, 1;
	not.pred 	%p111, %p110;
	selp.f64	%fd607, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p111;
	mul.wide.u32 	%rd150, %r170, 8;
	add.s64 	%rd152, %rd60, %rd150;
	ld.const.f64 	%fd608, [%rd152+8];
	mul.rn.f64 	%fd154, %fd769, %fd769;
	fma.rn.f64 	%fd609, %fd607, %fd154, %fd608;
	ld.const.f64 	%fd610, [%rd152+16];
	fma.rn.f64 	%fd611, %fd609, %fd154, %fd610;
	ld.const.f64 	%fd612, [%rd152+24];
	fma.rn.f64 	%fd613, %fd611, %fd154, %fd612;
	ld.const.f64 	%fd614, [%rd152+32];
	fma.rn.f64 	%fd615, %fd613, %fd154, %fd614;
	ld.const.f64 	%fd616, [%rd152+40];
	fma.rn.f64 	%fd617, %fd615, %fd154, %fd616;
	ld.const.f64 	%fd618, [%rd152+48];
	fma.rn.f64 	%fd155, %fd617, %fd154, %fd618;
	fma.rn.f64 	%fd770, %fd155, %fd769, %fd769;
	@%p111 bra 	BB9_113;

	mov.f64 	%fd619, 0d3FF0000000000000;
	fma.rn.f64 	%fd770, %fd155, %fd154, %fd619;

BB9_113:
	and.b32  	%r172, %r47, 2;
	setp.eq.s32	%p112, %r172, 0;
	@%p112 bra 	BB9_115;

	mov.f64 	%fd620, 0d0000000000000000;
	mov.f64 	%fd621, 0dBFF0000000000000;
	fma.rn.f64 	%fd770, %fd770, %fd621, %fd620;

BB9_115:
	ld.global.f32 	%f178, [%rd1];
	cvt.f64.f32	%fd622, %f178;
	mul.f64 	%fd161, %fd770, %fd622;
	mov.f64 	%fd788, %fd1;
	@%p28 bra 	BB9_117;

	mov.f64 	%fd623, 0d0000000000000000;
	mul.rn.f64 	%fd788, %fd1, %fd623;

BB9_117:
	add.u64 	%rd153, %SP, 44;
	mul.f64 	%fd624, %fd788, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r214, %fd624;
	cvta.to.local.u64 	%rd154, %rd153;
	st.local.u32 	[%rd154], %r214;
	cvt.rn.f64.s32	%fd625, %r214;
	neg.f64 	%fd626, %fd625;
	fma.rn.f64 	%fd628, %fd626, %fd210, %fd788;
	fma.rn.f64 	%fd630, %fd626, %fd212, %fd628;
	fma.rn.f64 	%fd771, %fd626, %fd214, %fd630;
	abs.f64 	%fd632, %fd788;
	setp.leu.f64	%p114, %fd632, 0d41E0000000000000;
	@%p114 bra 	BB9_119;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd788;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd153;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd771, [retval0+0];
	}
	// Callseq End 12
	ld.local.u32 	%r214, [%rd154];

BB9_119:
	shl.b32 	%r173, %r214, 3;
	and.b32  	%r174, %r173, 8;
	and.b32  	%r175, %r214, 1;
	setp.eq.b32	%p115, %r175, 1;
	not.pred 	%p116, %p115;
	selp.f64	%fd633, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p116;
	mul.wide.u32 	%rd157, %r174, 8;
	add.s64 	%rd159, %rd60, %rd157;
	ld.const.f64 	%fd634, [%rd159+8];
	mul.rn.f64 	%fd167, %fd771, %fd771;
	fma.rn.f64 	%fd635, %fd633, %fd167, %fd634;
	ld.const.f64 	%fd636, [%rd159+16];
	fma.rn.f64 	%fd637, %fd635, %fd167, %fd636;
	ld.const.f64 	%fd638, [%rd159+24];
	fma.rn.f64 	%fd639, %fd637, %fd167, %fd638;
	ld.const.f64 	%fd640, [%rd159+32];
	fma.rn.f64 	%fd641, %fd639, %fd167, %fd640;
	ld.const.f64 	%fd642, [%rd159+40];
	fma.rn.f64 	%fd643, %fd641, %fd167, %fd642;
	ld.const.f64 	%fd644, [%rd159+48];
	fma.rn.f64 	%fd168, %fd643, %fd167, %fd644;
	fma.rn.f64 	%fd772, %fd168, %fd771, %fd771;
	@%p116 bra 	BB9_121;

	mov.f64 	%fd645, 0d3FF0000000000000;
	fma.rn.f64 	%fd772, %fd168, %fd167, %fd645;

BB9_121:
	and.b32  	%r176, %r214, 2;
	setp.eq.s32	%p117, %r176, 0;
	@%p117 bra 	BB9_123;

	mov.f64 	%fd646, 0d0000000000000000;
	mov.f64 	%fd647, 0dBFF0000000000000;
	fma.rn.f64 	%fd772, %fd772, %fd647, %fd646;

BB9_123:
	ld.global.f32 	%f179, [%rd2];
	cvt.f64.f32	%fd648, %f179;
	fma.rn.f64 	%fd174, %fd772, %fd648, %fd161;
	mov.f64 	%fd787, %fd1;
	@%p28 bra 	BB9_125;

	mov.f64 	%fd649, 0d0000000000000000;
	mul.rn.f64 	%fd787, %fd1, %fd649;

BB9_125:
	add.u64 	%rd160, %SP, 48;
	mul.f64 	%fd650, %fd787, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r215, %fd650;
	cvta.to.local.u64 	%rd161, %rd160;
	st.local.u32 	[%rd161], %r215;
	cvt.rn.f64.s32	%fd651, %r215;
	neg.f64 	%fd652, %fd651;
	fma.rn.f64 	%fd654, %fd652, %fd210, %fd787;
	fma.rn.f64 	%fd656, %fd652, %fd212, %fd654;
	fma.rn.f64 	%fd773, %fd652, %fd214, %fd656;
	abs.f64 	%fd658, %fd787;
	setp.leu.f64	%p119, %fd658, 0d41E0000000000000;
	@%p119 bra 	BB9_127;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd787;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd160;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd773, [retval0+0];
	}
	// Callseq End 13
	ld.local.u32 	%r215, [%rd161];

BB9_127:
	add.s32 	%r54, %r215, 1;
	shl.b32 	%r177, %r54, 3;
	and.b32  	%r178, %r177, 8;
	and.b32  	%r179, %r54, 1;
	setp.eq.b32	%p120, %r179, 1;
	not.pred 	%p121, %p120;
	selp.f64	%fd659, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p121;
	mul.wide.u32 	%rd164, %r178, 8;
	add.s64 	%rd166, %rd60, %rd164;
	ld.const.f64 	%fd660, [%rd166+8];
	mul.rn.f64 	%fd180, %fd773, %fd773;
	fma.rn.f64 	%fd661, %fd659, %fd180, %fd660;
	ld.const.f64 	%fd662, [%rd166+16];
	fma.rn.f64 	%fd663, %fd661, %fd180, %fd662;
	ld.const.f64 	%fd664, [%rd166+24];
	fma.rn.f64 	%fd665, %fd663, %fd180, %fd664;
	ld.const.f64 	%fd666, [%rd166+32];
	fma.rn.f64 	%fd667, %fd665, %fd180, %fd666;
	ld.const.f64 	%fd668, [%rd166+40];
	fma.rn.f64 	%fd669, %fd667, %fd180, %fd668;
	ld.const.f64 	%fd670, [%rd166+48];
	fma.rn.f64 	%fd181, %fd669, %fd180, %fd670;
	fma.rn.f64 	%fd774, %fd181, %fd773, %fd773;
	@%p121 bra 	BB9_129;

	mov.f64 	%fd671, 0d3FF0000000000000;
	fma.rn.f64 	%fd774, %fd181, %fd180, %fd671;

BB9_129:
	and.b32  	%r180, %r54, 2;
	setp.eq.s32	%p122, %r180, 0;
	@%p122 bra 	BB9_131;

	mov.f64 	%fd672, 0d0000000000000000;
	mov.f64 	%fd673, 0dBFF0000000000000;
	fma.rn.f64 	%fd774, %fd774, %fd673, %fd672;

BB9_131:
	ld.global.f32 	%f180, [%rd6];
	cvt.f64.f32	%fd674, %f180;
	mul.f64 	%fd187, %fd774, %fd674;
	mov.f64 	%fd786, %fd1;
	@%p28 bra 	BB9_133;

	mov.f64 	%fd675, 0d0000000000000000;
	mul.rn.f64 	%fd786, %fd1, %fd675;

BB9_133:
	add.u64 	%rd167, %SP, 52;
	mul.f64 	%fd676, %fd786, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r216, %fd676;
	cvta.to.local.u64 	%rd168, %rd167;
	st.local.u32 	[%rd168], %r216;
	cvt.rn.f64.s32	%fd677, %r216;
	neg.f64 	%fd678, %fd677;
	fma.rn.f64 	%fd680, %fd678, %fd210, %fd786;
	fma.rn.f64 	%fd682, %fd678, %fd212, %fd680;
	fma.rn.f64 	%fd798, %fd678, %fd214, %fd682;
	abs.f64 	%fd684, %fd786;
	setp.leu.f64	%p124, %fd684, 0d41E0000000000000;
	@%p124 bra 	BB9_135;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd786;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd167;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd798, [retval0+0];
	}
	// Callseq End 14
	ld.local.u32 	%r216, [%rd168];

BB9_135:
	shl.b32 	%r181, %r216, 3;
	and.b32  	%r182, %r181, 8;
	and.b32  	%r183, %r216, 1;
	setp.eq.b32	%p125, %r183, 1;
	not.pred 	%p126, %p125;
	selp.f64	%fd685, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p126;
	mul.wide.u32 	%rd171, %r182, 8;
	add.s64 	%rd173, %rd60, %rd171;
	ld.const.f64 	%fd686, [%rd173+8];
	mul.rn.f64 	%fd193, %fd798, %fd798;
	fma.rn.f64 	%fd687, %fd685, %fd193, %fd686;
	ld.const.f64 	%fd688, [%rd173+16];
	fma.rn.f64 	%fd689, %fd687, %fd193, %fd688;
	ld.const.f64 	%fd690, [%rd173+24];
	fma.rn.f64 	%fd691, %fd689, %fd193, %fd690;
	ld.const.f64 	%fd692, [%rd173+32];
	fma.rn.f64 	%fd693, %fd691, %fd193, %fd692;
	ld.const.f64 	%fd694, [%rd173+40];
	fma.rn.f64 	%fd695, %fd693, %fd193, %fd694;
	ld.const.f64 	%fd696, [%rd173+48];
	fma.rn.f64 	%fd194, %fd695, %fd193, %fd696;
	fma.rn.f64 	%fd799, %fd194, %fd798, %fd798;
	@%p126 bra 	BB9_137;

	mov.f64 	%fd697, 0d3FF0000000000000;
	fma.rn.f64 	%fd799, %fd194, %fd193, %fd697;

BB9_137:
	and.b32  	%r184, %r216, 2;
	setp.eq.s32	%p127, %r184, 0;
	@%p127 bra 	BB9_139;

	mov.f64 	%fd698, 0d0000000000000000;
	mov.f64 	%fd699, 0dBFF0000000000000;
	fma.rn.f64 	%fd799, %fd799, %fd699, %fd698;

BB9_139:
	ld.param.u64 	%rd202, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_20];
	ld.param.u64 	%rd201, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_17];
	ld.global.f32 	%f181, [%rd7];
	cvt.f64.f32	%fd700, %f181;
	fma.rn.f64 	%fd701, %fd799, %fd700, %fd187;
	mul.wide.s32 	%rd175, %r1, 4;
	add.s64 	%rd176, %rd38, %rd175;
	ld.global.f32 	%f182, [%rd176];
	cvt.f64.f32	%fd702, %f182;
	sub.f64 	%fd703, %fd174, %fd702;
	ld.global.f32 	%f183, [%rd8];
	cvt.f64.f32	%fd704, %f183;
	fma.rn.f64 	%fd705, %fd147, %fd703, %fd704;
	cvt.rn.f32.f64	%f184, %fd705;
	cvta.to.global.u64 	%rd177, %rd201;
	add.s64 	%rd11, %rd177, %rd175;
	st.global.f32 	[%rd11], %f184;
	ld.global.f32 	%f185, [%rd5];
	cvt.f64.f32	%fd706, %f185;
	sub.f64 	%fd707, %fd706, %fd701;
	mul.f64 	%fd708, %fd148, %fd707;
	ld.global.f32 	%f186, [%rd10];
	cvt.f64.f32	%fd709, %f186;
	sub.f64 	%fd710, %fd709, %fd708;
	cvt.rn.f32.f64	%f29, %fd710;
	cvta.to.global.u64 	%rd178, %rd202;
	add.s64 	%rd12, %rd178, %rd175;
	st.global.f32 	[%rd12], %f29;
	and.pred  	%p130, %p24, %p66;
	@!%p130 bra 	BB9_147;
	bra.uni 	BB9_140;

BB9_140:
	ld.global.f32 	%f30, [%rd11];
	abs.f32 	%f31, %f30;
	setp.geu.f32	%p131, %f31, 0f40400000;
	@%p131 bra 	BB9_142;

	abs.f32 	%f187, %f29;
	setp.gt.f32	%p132, %f187, 0f40400000;
	@%p132 bra 	BB9_143;

BB9_142:
	setp.gt.f32	%p133, %f31, 0f40400000;
	selp.b16	%rs15, 0, %rs15, %p133;
	selp.b16	%rs16, 0, %rs16, %p133;
	bra.uni 	BB9_144;

BB9_143:
	setp.ge.f32	%p134, %f29, 0f00000000;
	selp.u32	%r189, 1, 0, %p134;
	setp.lt.f32	%p135, %f29, 0f00000000;
	selp.u32	%r190, 1, 0, %p135;
	sub.s32 	%r191, %r189, %r190;
	cvt.rn.f64.s32	%fd711, %r191;
	mul.f64 	%fd712, %fd711, 0d4018000000000000;
	mul.f64 	%fd713, %fd712, 0d3FE0000000000000;
	cvt.f64.f32	%fd714, %f30;
	sub.f64 	%fd715, %fd713, %fd714;
	sub.f32 	%f188, %f29, %f30;
	cvt.f64.f32	%fd716, %f188;
	div.rn.f64 	%fd717, %fd715, %fd716;
	abs.f64 	%fd718, %fd717;
	ld.global.f32 	%f189, [%rd1];
	cvt.f64.f32	%fd719, %f189;
	ld.global.f32 	%f190, [%rd6];
	sub.f32 	%f191, %f190, %f189;
	cvt.f64.f32	%fd720, %f191;
	fma.rn.f64 	%fd721, %fd718, %fd720, %fd719;
	cvt.rn.f32.f64	%f192, %fd721;
	st.global.f32 	[%rd6], %f192;
	ld.global.f32 	%f193, [%rd2];
	cvt.f64.f32	%fd722, %f193;
	ld.global.f32 	%f194, [%rd7];
	sub.f32 	%f195, %f194, %f193;
	cvt.f64.f32	%fd723, %f195;
	fma.rn.f64 	%fd724, %fd718, %fd723, %fd722;
	cvt.rn.f32.f64	%f196, %fd724;
	st.global.f32 	[%rd7], %f196;
	ld.global.f32 	%f197, [%rd12];
	setp.ge.f32	%p136, %f197, 0f00000000;
	selp.u32	%r192, 1, 0, %p136;
	setp.lt.f32	%p137, %f197, 0f00000000;
	selp.u32	%r193, 1, 0, %p137;
	sub.s32 	%r194, %r192, %r193;
	cvt.rn.f64.s32	%fd725, %r194;
	mul.f64 	%fd726, %fd725, 0d4018000000000000;
	mul.f64 	%fd727, %fd726, 0d3FE0000000000000;
	cvt.rn.f32.f64	%f198, %fd727;
	st.global.f32 	[%rd12], %f198;

BB9_144:
	ld.param.u64 	%rd205, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_4];
	cvta.to.global.u64 	%rd204, %rd205;
	mul.wide.s32 	%rd180, %r1, 4;
	add.s64 	%rd181, %rd204, %rd180;
	ld.global.f32 	%f199, [%rd181];
	ld.global.f32 	%f200, [%rd3];
	sub.f32 	%f201, %f200, %f199;
	abs.f32 	%f202, %f201;
	setp.gt.f32	%p138, %f202, 0f40A00000;
	@%p138 bra 	BB9_146;

	ld.global.f32 	%f203, [%rd10];
	ld.global.f32 	%f204, [%rd8];
	sub.f32 	%f205, %f203, %f204;
	abs.f32 	%f206, %f205;
	setp.leu.f32	%p139, %f206, 0f40A00000;
	@%p139 bra 	BB9_147;

BB9_146:
	mov.u16 	%rs16, 0;
	mov.u16 	%rs15, %rs16;

BB9_147:
	ld.param.u64 	%rd203, [_Z30recon_volume_intersections_GPUiPiPbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_2];
	cvta.to.global.u64 	%rd182, %rd203;
	and.b16  	%rs11, %rs16, 1;
	setp.eq.b16	%p140, %rs11, 1;
	and.b16  	%rs12, %rs15, 1;
	setp.eq.b16	%p141, %rs12, 1;
	and.pred  	%p142, %p141, %p140;
	selp.u16	%rs13, 1, 0, %p142;
	xor.b16  	%rs14, %rs13, 1;
	cvt.s64.s32	%rd183, %r1;
	add.s64 	%rd184, %rd182, %rd183;
	st.global.u8 	[%rd184], %rs14;

BB9_148:
	ret;
}

.visible .entry _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_(
	.param .u32 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_0,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_1,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_2,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_3,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_4,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_5,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_6,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_7,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_8,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_9,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_10,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_11,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_12,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_13,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_14,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_15,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_16,
	.param .u64 _Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_17
)
{
	.local .align 4 .b8 	__local_depot10[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<62>;
	.reg .f32 	%f<60>;
	.reg .s64 	%rd<77>;
	.reg .f64 	%fd<111>;


	mov.u64 	%SPL, __local_depot10;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r14, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_0];
	ld.param.u64 	%rd2, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_1];
	ld.param.u64 	%rd3, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_2];
	ld.param.u64 	%rd4, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_3];
	ld.param.u64 	%rd5, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_4];
	ld.param.u64 	%rd6, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_5];
	ld.param.u64 	%rd7, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_6];
	ld.param.u64 	%rd8, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_7];
	ld.param.u64 	%rd9, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_8];
	ld.param.u64 	%rd10, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_9];
	ld.param.u64 	%rd11, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_10];
	ld.param.u64 	%rd12, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_11];
	ld.param.u64 	%rd13, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_13];
	ld.param.u64 	%rd14, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_14];
	ld.param.u64 	%rd15, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_15];
	ld.param.u64 	%rd16, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_16];
	ld.param.u64 	%rd17, [_Z11binning_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1__param_17];
	mov.u32 	%r15, %ctaid.x;
	shl.b32 	%r16, %r15, 10;
	mov.u32 	%r17, %tid.x;
	add.s32 	%r18, %r16, %r17;
	setp.ge.s32	%p1, %r18, %r14;
	@%p1 bra 	BB10_26;

	cvta.to.global.u64 	%rd18, %rd5;
	mul.wide.s32 	%rd19, %r18, 4;
	add.s64 	%rd20, %rd18, %rd19;
	cvta.to.global.u64 	%rd21, %rd8;
	add.s64 	%rd22, %rd21, %rd19;
	ld.global.f32 	%f7, [%rd22];
	ld.global.f32 	%f8, [%rd20];
	add.f32 	%f9, %f8, %f7;
	mul.f32 	%f10, %f9, 0f3F000000;
	cvt.f64.f32	%fd1, %f10;
	cvta.to.global.u64 	%rd23, %rd6;
	add.s64 	%rd24, %rd23, %rd19;
	cvta.to.global.u64 	%rd25, %rd9;
	add.s64 	%rd26, %rd25, %rd19;
	ld.global.f32 	%f11, [%rd26];
	ld.global.f32 	%f12, [%rd24];
	add.f32 	%f13, %f12, %f11;
	mul.f32 	%f14, %f13, 0f3F000000;
	cvt.f64.f32	%fd2, %f14;
	cvta.to.global.u64 	%rd27, %rd7;
	add.s64 	%rd28, %rd27, %rd19;
	cvta.to.global.u64 	%rd29, %rd10;
	add.s64 	%rd30, %rd29, %rd19;
	ld.global.f32 	%f15, [%rd30];
	ld.global.f32 	%f16, [%rd28];
	add.f32 	%f17, %f16, %f15;
	mul.f32 	%f18, %f17, 0f3F000000;
	cvt.f64.f32	%fd3, %f18;
	sub.f32 	%f19, %f11, %f12;
	sub.f32 	%f20, %f7, %f8;
	abs.f32 	%f1, %f20;
	abs.f32 	%f2, %f19;
	setp.eq.f32	%p2, %f1, 0f00000000;
	setp.eq.f32	%p3, %f2, 0f00000000;
	and.pred  	%p4, %p2, %p3;
	mov.b32 	 %r1, %f20;
	mov.b32 	 %r23, %f19;
	and.b32  	%r2, %r23, -2147483648;
	@%p4 bra 	BB10_5;

	setp.eq.f32	%p5, %f1, 0f7F800000;
	setp.eq.f32	%p6, %f2, 0f7F800000;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	BB10_4;

	max.f32 	%f21, %f2, %f1;
	min.f32 	%f22, %f2, %f1;
	div.rn.f32 	%f23, %f22, %f21;
	mul.rn.f32 	%f24, %f23, %f23;
	mov.f32 	%f25, 0fC0B59883;
	mov.f32 	%f26, 0fBF52C7EA;
	fma.rn.f32 	%f27, %f24, %f26, %f25;
	mov.f32 	%f28, 0fC0D21907;
	fma.rn.f32 	%f29, %f27, %f24, %f28;
	mul.f32 	%f30, %f29, %f24;
	mul.f32 	%f31, %f30, %f23;
	add.f32 	%f32, %f24, 0f41355DC0;
	mov.f32 	%f33, 0f41E6BD60;
	fma.rn.f32 	%f34, %f32, %f24, %f33;
	mov.f32 	%f35, 0f419D92C8;
	fma.rn.f32 	%f36, %f34, %f24, %f35;
	rcp.rn.f32 	%f37, %f36;
	fma.rn.f32 	%f38, %f31, %f37, %f23;
	mov.f32 	%f39, 0f3FC90FDB;
	sub.f32 	%f40, %f39, %f38;
	setp.gt.f32	%p8, %f2, %f1;
	selp.f32	%f41, %f40, %f38, %p8;
	mov.f32 	%f42, 0f40490FDB;
	sub.f32 	%f43, %f42, %f41;
	setp.lt.s32	%p9, %r1, 0;
	selp.f32	%f44, %f43, %f41, %p9;
	mov.b32 	 %r24, %f44;
	or.b32  	%r25, %r24, %r2;
	mov.b32 	 %f45, %r25;
	add.f32 	%f46, %f1, %f2;
	setp.gtu.f32	%p10, %f46, 0f7F800000;
	selp.f32	%f59, %f46, %f45, %p10;
	bra.uni 	BB10_6;

BB10_4:
	shr.s32 	%r26, %r1, 31;
	and.b32  	%r27, %r26, 13483017;
	add.s32 	%r28, %r27, 1061752795;
	or.b32  	%r29, %r28, %r2;
	mov.b32 	 %f59, %r29;
	bra.uni 	BB10_6;

BB10_5:
	shr.s32 	%r30, %r1, 31;
	and.b32  	%r31, %r30, 1078530011;
	or.b32  	%r32, %r2, %r31;
	mov.b32 	 %f59, %r32;

BB10_6:
	cvt.f64.f32	%fd31, %f59;
	add.f64 	%fd32, %fd31, 0d401921FB54442D18;
	setp.lt.f32	%p11, %f59, 0f00000000;
	selp.f64	%fd33, %fd32, %fd31, %p11;
	mul.f64 	%fd34, %fd33, 0d404CA5DC1A63C1F8;
	div.rn.f64 	%fd35, %fd34, 0d4018000000000000;
	add.f64 	%fd36, %fd35, 0d3FE0000000000000;
	cvt.rzi.s32.f64	%r33, %fd36;
	mul.wide.s32 	%rd31, %r33, -2004318071;
	shr.u64 	%rd32, %rd31, 32;
	cvt.u32.u64	%r34, %rd32;
	add.s32 	%r35, %r34, %r33;
	shr.u32 	%r36, %r35, 31;
	shr.s32 	%r37, %r35, 5;
	add.s32 	%r38, %r37, %r36;
	mul.lo.s32 	%r39, %r38, 60;
	sub.s32 	%r3, %r33, %r39;
	cvt.rn.f64.s32	%fd37, %r3;
	mul.f64 	%fd38, %fd37, 0d4018000000000000;
	mul.f64 	%fd4, %fd38, 0d3F91DF46A2529D39;
	abs.f64 	%fd5, %fd4;
	setp.neu.f64	%p12, %fd5, 0d7FF0000000000000;
	mov.f64 	%fd108, %fd4;
	@%p12 bra 	BB10_8;

	mov.f64 	%fd39, 0d0000000000000000;
	mul.rn.f64 	%fd6, %fd4, %fd39;
	mov.f64 	%fd108, %fd6;

BB10_8:
	mov.f64 	%fd7, %fd108;
	add.u64 	%rd33, %SP, 0;
	mul.f64 	%fd40, %fd7, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r60, %fd40;
	cvta.to.local.u64 	%rd34, %rd33;
	st.local.u32 	[%rd34], %r60;
	cvt.rn.f64.s32	%fd41, %r60;
	neg.f64 	%fd42, %fd41;
	mov.f64 	%fd43, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd44, %fd42, %fd43, %fd7;
	mov.f64 	%fd45, 0d3C91A62633145C00;
	fma.rn.f64 	%fd46, %fd42, %fd45, %fd44;
	mov.f64 	%fd47, 0d397B839A252049C0;
	fma.rn.f64 	%fd104, %fd42, %fd47, %fd46;
	abs.f64 	%fd48, %fd7;
	setp.leu.f64	%p13, %fd48, 0d41E0000000000000;
	@%p13 bra 	BB10_10;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd7;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd33;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd104, [retval0+0];
	}
	// Callseq End 15
	ld.local.u32 	%r60, [%rd34];

BB10_10:
	add.s32 	%r7, %r60, 1;
	shl.b32 	%r40, %r7, 3;
	and.b32  	%r41, %r40, 8;
	and.b32  	%r42, %r7, 1;
	setp.eq.b32	%p14, %r42, 1;
	not.pred 	%p15, %p14;
	selp.f64	%fd49, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p15;
	mul.wide.u32 	%rd37, %r41, 8;
	mov.u64 	%rd38, __cudart_sin_cos_coeffs;
	add.s64 	%rd39, %rd38, %rd37;
	ld.const.f64 	%fd50, [%rd39+8];
	mul.rn.f64 	%fd11, %fd104, %fd104;
	fma.rn.f64 	%fd51, %fd49, %fd11, %fd50;
	ld.const.f64 	%fd52, [%rd39+16];
	fma.rn.f64 	%fd53, %fd51, %fd11, %fd52;
	ld.const.f64 	%fd54, [%rd39+24];
	fma.rn.f64 	%fd55, %fd53, %fd11, %fd54;
	ld.const.f64 	%fd56, [%rd39+32];
	fma.rn.f64 	%fd57, %fd55, %fd11, %fd56;
	ld.const.f64 	%fd58, [%rd39+40];
	fma.rn.f64 	%fd59, %fd57, %fd11, %fd58;
	ld.const.f64 	%fd60, [%rd39+48];
	fma.rn.f64 	%fd12, %fd59, %fd11, %fd60;
	fma.rn.f64 	%fd105, %fd12, %fd104, %fd104;
	@%p15 bra 	BB10_12;

	mov.f64 	%fd61, 0d3FF0000000000000;
	fma.rn.f64 	%fd105, %fd12, %fd11, %fd61;

BB10_12:
	and.b32  	%r43, %r7, 2;
	setp.eq.s32	%p16, %r43, 0;
	@%p16 bra 	BB10_14;

	mov.f64 	%fd62, 0d0000000000000000;
	mov.f64 	%fd63, 0dBFF0000000000000;
	fma.rn.f64 	%fd105, %fd105, %fd63, %fd62;

BB10_14:
	mul.f64 	%fd18, %fd2, %fd105;
	mov.f64 	%fd107, %fd4;
	@%p12 bra 	BB10_16;

	mov.f64 	%fd64, 0d0000000000000000;
	mul.rn.f64 	%fd107, %fd4, %fd64;

BB10_16:
	add.u64 	%rd40, %SP, 4;
	mul.f64 	%fd65, %fd107, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r61, %fd65;
	cvta.to.local.u64 	%rd41, %rd40;
	st.local.u32 	[%rd41], %r61;
	cvt.rn.f64.s32	%fd66, %r61;
	neg.f64 	%fd67, %fd66;
	fma.rn.f64 	%fd69, %fd67, %fd43, %fd107;
	fma.rn.f64 	%fd71, %fd67, %fd45, %fd69;
	fma.rn.f64 	%fd109, %fd67, %fd47, %fd71;
	abs.f64 	%fd73, %fd107;
	setp.leu.f64	%p18, %fd73, 0d41E0000000000000;
	@%p18 bra 	BB10_18;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd107;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd40;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd109, [retval0+0];
	}
	// Callseq End 16
	ld.local.u32 	%r61, [%rd41];

BB10_18:
	shl.b32 	%r44, %r61, 3;
	and.b32  	%r45, %r44, 8;
	and.b32  	%r46, %r61, 1;
	setp.eq.b32	%p19, %r46, 1;
	not.pred 	%p20, %p19;
	selp.f64	%fd74, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p20;
	mul.wide.u32 	%rd44, %r45, 8;
	add.s64 	%rd46, %rd38, %rd44;
	ld.const.f64 	%fd75, [%rd46+8];
	mul.rn.f64 	%fd24, %fd109, %fd109;
	fma.rn.f64 	%fd76, %fd74, %fd24, %fd75;
	ld.const.f64 	%fd77, [%rd46+16];
	fma.rn.f64 	%fd78, %fd76, %fd24, %fd77;
	ld.const.f64 	%fd79, [%rd46+24];
	fma.rn.f64 	%fd80, %fd78, %fd24, %fd79;
	ld.const.f64 	%fd81, [%rd46+32];
	fma.rn.f64 	%fd82, %fd80, %fd24, %fd81;
	ld.const.f64 	%fd83, [%rd46+40];
	fma.rn.f64 	%fd84, %fd82, %fd24, %fd83;
	ld.const.f64 	%fd85, [%rd46+48];
	fma.rn.f64 	%fd25, %fd84, %fd24, %fd85;
	fma.rn.f64 	%fd110, %fd25, %fd109, %fd109;
	@%p20 bra 	BB10_20;

	mov.f64 	%fd86, 0d3FF0000000000000;
	fma.rn.f64 	%fd110, %fd25, %fd24, %fd86;

BB10_20:
	and.b32  	%r47, %r61, 2;
	setp.eq.s32	%p21, %r47, 0;
	@%p21 bra 	BB10_22;

	mov.f64 	%fd87, 0d0000000000000000;
	mov.f64 	%fd88, 0dBFF0000000000000;
	fma.rn.f64 	%fd110, %fd110, %fd88, %fd87;

BB10_22:
	mul.f64 	%fd89, %fd1, %fd110;
	sub.f64 	%fd90, %fd18, %fd89;
	div.rn.f64 	%fd91, %fd90, 0d3FB999999999999A;
	add.f64 	%fd92, %fd91, 0d4065E00000000000;
	cvt.rzi.s32.f64	%r11, %fd92;
	fma.rn.f64 	%fd93, %fd3, 0d4010000000000000, 0d4032000000000000;
	cvt.rzi.s32.f64	%r12, %fd93;
	or.b32  	%r48, %r12, %r11;
	setp.gt.s32	%p22, %r48, -1;
	setp.lt.s32	%p23, %r11, 350;
	and.pred  	%p24, %p22, %p23;
	setp.lt.s32	%p25, %r12, 36;
	and.pred  	%p26, %p24, %p25;
	@!%p26 bra 	BB10_26;
	bra.uni 	BB10_23;

BB10_23:
	cvta.to.global.u64 	%rd47, %rd4;
	mad.lo.s32 	%r49, %r3, 350, %r11;
	mad.lo.s32 	%r13, %r12, 21000, %r49;
	cvt.s64.s32	%rd48, %r18;
	cvta.to.global.u64 	%rd49, %rd3;
	mul.wide.s32 	%rd50, %r18, 4;
	add.s64 	%rd1, %rd49, %rd50;
	st.global.u32 	[%rd1], %r13;
	add.s64 	%rd51, %rd47, %rd48;
	ld.global.u8 	%rs1, [%rd51];
	setp.eq.s16	%p27, %rs1, 0;
	@%p27 bra 	BB10_25;

	mov.u32 	%r54, -1;
	st.global.u32 	[%rd1], %r54;
	bra.uni 	BB10_26;

BB10_25:
	cvta.to.global.u64 	%rd52, %rd16;
	mul.wide.s32 	%rd53, %r18, 4;
	add.s64 	%rd54, %rd52, %rd53;
	cvta.to.global.u64 	%rd55, %rd14;
	add.s64 	%rd56, %rd55, %rd53;
	ld.global.f32 	%f47, [%rd56];
	ld.global.f32 	%f48, [%rd54];
	sub.f32 	%f49, %f48, %f47;
	cvt.f64.f32	%fd94, %f49;
	setp.gt.f64	%p28, %fd94, 0d400921FB54442D18;
	add.f64 	%fd95, %fd94, 0dC01921FB54442D18;
	selp.f64	%fd96, %fd95, %fd94, %p28;
	setp.lt.f64	%p29, %fd96, 0dC00921FB54442D18;
	add.f64 	%fd97, %fd96, 0d401921FB54442D18;
	selp.f64	%fd98, %fd97, %fd96, %p29;
	cvta.to.global.u64 	%rd57, %rd17;
	add.s64 	%rd58, %rd57, %rd53;
	cvta.to.global.u64 	%rd59, %rd15;
	add.s64 	%rd60, %rd59, %rd53;
	ld.global.f32 	%f50, [%rd60];
	ld.global.f32 	%f51, [%rd58];
	sub.f32 	%f52, %f51, %f50;
	cvt.f64.f32	%fd99, %f52;
	setp.gt.f64	%p30, %fd99, 0d400921FB54442D18;
	add.f64 	%fd100, %fd99, 0dC01921FB54442D18;
	selp.f64	%fd101, %fd100, %fd99, %p30;
	setp.lt.f64	%p31, %fd101, 0dC00921FB54442D18;
	add.f64 	%fd102, %fd101, 0d401921FB54442D18;
	selp.f64	%fd103, %fd102, %fd101, %p31;
	cvta.to.global.u64 	%rd61, %rd2;
	mul.wide.s32 	%rd62, %r13, 4;
	add.s64 	%rd63, %rd61, %rd62;
	atom.global.add.u32 	%r59, [%rd63], 1;
	ld.global.s32 	%rd64, [%rd1];
	cvta.to.global.u64 	%rd65, %rd11;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	cvta.to.global.u64 	%rd68, %rd13;
	add.s64 	%rd69, %rd68, %rd53;
	ld.global.f32 	%f53, [%rd69];
	atom.global.add.f32 	%f54, [%rd67], %f53;
	ld.global.s32 	%rd70, [%rd1];
	cvta.to.global.u64 	%rd71, %rd12;
	shl.b64 	%rd72, %rd70, 2;
	add.s64 	%rd73, %rd71, %rd72;
	cvt.rn.f32.f64	%f55, %fd98;
	atom.global.add.f32 	%f56, [%rd73], %f55;
	ld.global.s32 	%rd74, [%rd1];
	shl.b64 	%rd75, %rd74, 2;
	add.s64 	%rd76, %rd71, %rd75;
	cvt.rn.f32.f64	%f57, %fd103;
	atom.global.add.f32 	%f58, [%rd76], %f57;

BB10_26:
	ret;
}

.visible .entry _Z19calculate_means_GPUPiPfS0_S0_(
	.param .u64 _Z19calculate_means_GPUPiPfS0_S0__param_0,
	.param .u64 _Z19calculate_means_GPUPiPfS0_S0__param_1,
	.param .u64 _Z19calculate_means_GPUPiPfS0_S0__param_2,
	.param .u64 _Z19calculate_means_GPUPiPfS0_S0__param_3
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<10>;
	.reg .f32 	%f<10>;
	.reg .s64 	%rd<16>;


	ld.param.u64 	%rd6, [_Z19calculate_means_GPUPiPfS0_S0__param_0];
	ld.param.u64 	%rd3, [_Z19calculate_means_GPUPiPfS0_S0__param_1];
	ld.param.u64 	%rd4, [_Z19calculate_means_GPUPiPfS0_S0__param_2];
	ld.param.u64 	%rd5, [_Z19calculate_means_GPUPiPfS0_S0__param_3];
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %ctaid.x;
	mul.lo.s32 	%r4, %r3, 21000;
	mad.lo.s32 	%r5, %r2, 350, %r4;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	cvt.s64.s32	%rd1, %r7;
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.s32 	%rd8, %r7, 4;
	add.s64 	%rd2, %rd7, %rd8;
	ld.global.u32 	%r1, [%rd2];
	setp.lt.s32	%p1, %r1, 1;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd9, %rd5;
	cvta.to.global.u64 	%rd10, %rd4;
	cvta.to.global.u64 	%rd11, %rd3;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	cvt.rn.f32.s32	%f1, %r1;
	ld.global.f32 	%f2, [%rd13];
	div.rn.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd13], %f3;
	ld.global.u32 	%r8, [%rd2];
	cvt.rn.f32.s32	%f4, %r8;
	add.s64 	%rd14, %rd10, %rd12;
	ld.global.f32 	%f5, [%rd14];
	div.rn.f32 	%f6, %f5, %f4;
	st.global.f32 	[%rd14], %f6;
	ld.global.u32 	%r9, [%rd2];
	cvt.rn.f32.s32	%f7, %r9;
	add.s64 	%rd15, %rd9, %rd12;
	ld.global.f32 	%f8, [%rd15];
	div.rn.f32 	%f9, %f8, %f7;
	st.global.f32 	[%rd15], %f9;

BB11_2:
	ret;
}

.visible .entry _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_(
	.param .u32 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_0,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_1,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_2,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_3,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_4,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_5,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_6,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_7,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_8,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_9,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_10,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_11,
	.param .u64 _Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_12
)
{
	.reg .pred 	%p<6>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<18>;
	.reg .s64 	%rd<44>;
	.reg .f64 	%fd<22>;


	ld.param.u32 	%r2, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_0];
	ld.param.u64 	%rd1, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_1];
	ld.param.u64 	%rd2, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_2];
	ld.param.u64 	%rd3, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_3];
	ld.param.u64 	%rd4, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_4];
	ld.param.u64 	%rd5, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_5];
	ld.param.u64 	%rd6, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_6];
	ld.param.u64 	%rd7, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_7];
	ld.param.u64 	%rd8, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_8];
	ld.param.u64 	%rd9, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_9];
	ld.param.u64 	%rd10, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_10];
	ld.param.u64 	%rd11, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_11];
	ld.param.u64 	%rd12, [_Z26sum_squared_deviations_GPUiPiPfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0__param_12];
	mov.u32 	%r3, %ctaid.x;
	shl.b32 	%r4, %r3, 10;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r1, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB12_2;

	cvta.to.global.u64 	%rd13, %rd12;
	cvta.to.global.u64 	%rd14, %rd11;
	cvta.to.global.u64 	%rd15, %rd10;
	cvta.to.global.u64 	%rd16, %rd4;
	cvta.to.global.u64 	%rd17, %rd3;
	cvta.to.global.u64 	%rd18, %rd2;
	cvta.to.global.u64 	%rd19, %rd1;
	cvta.to.global.u64 	%rd20, %rd5;
	cvta.to.global.u64 	%rd21, %rd7;
	cvta.to.global.u64 	%rd22, %rd8;
	mul.wide.s32 	%rd23, %r1, 4;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd6;
	add.s64 	%rd26, %rd25, %rd23;
	ld.global.f32 	%f1, [%rd26];
	ld.global.f32 	%f2, [%rd24];
	sub.f32 	%f3, %f2, %f1;
	cvt.f64.f32	%fd1, %f3;
	setp.gt.f64	%p2, %fd1, 0d400921FB54442D18;
	add.f64 	%fd2, %fd1, 0dC01921FB54442D18;
	selp.f64	%fd3, %fd2, %fd1, %p2;
	setp.lt.f64	%p3, %fd3, 0dC00921FB54442D18;
	add.f64 	%fd4, %fd3, 0d401921FB54442D18;
	selp.f64	%fd5, %fd4, %fd3, %p3;
	cvta.to.global.u64 	%rd27, %rd9;
	add.s64 	%rd28, %rd27, %rd23;
	add.s64 	%rd29, %rd21, %rd23;
	ld.global.f32 	%f4, [%rd29];
	ld.global.f32 	%f5, [%rd28];
	sub.f32 	%f6, %f5, %f4;
	cvt.f64.f32	%fd6, %f6;
	setp.gt.f64	%p4, %fd6, 0d400921FB54442D18;
	add.f64 	%fd7, %fd6, 0dC01921FB54442D18;
	selp.f64	%fd8, %fd7, %fd6, %p4;
	setp.lt.f64	%p5, %fd8, 0dC00921FB54442D18;
	add.f64 	%fd9, %fd8, 0d401921FB54442D18;
	selp.f64	%fd10, %fd9, %fd8, %p5;
	add.s64 	%rd30, %rd20, %rd23;
	add.s64 	%rd31, %rd19, %rd23;
	ld.global.s32 	%rd32, [%rd31];
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd18, %rd33;
	ld.global.f32 	%f7, [%rd34];
	ld.global.f32 	%f8, [%rd30];
	sub.f32 	%f9, %f8, %f7;
	cvt.f64.f32	%fd11, %f9;
	add.s64 	%rd35, %rd17, %rd33;
	ld.global.f32 	%f10, [%rd35];
	cvt.f64.f32	%fd12, %f10;
	sub.f64 	%fd13, %fd5, %fd12;
	add.s64 	%rd36, %rd16, %rd33;
	ld.global.f32 	%f11, [%rd36];
	cvt.f64.f32	%fd14, %f11;
	sub.f64 	%fd15, %fd10, %fd14;
	add.s64 	%rd37, %rd15, %rd33;
	mul.f64 	%fd16, %fd11, %fd11;
	mul.f64 	%fd17, %fd16, 0d3FF0000000000000;
	cvt.rn.f32.f64	%f12, %fd17;
	atom.global.add.f32 	%f13, [%rd37], %f12;
	ld.global.s32 	%rd38, [%rd31];
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd40, %rd14, %rd39;
	mul.f64 	%fd18, %fd13, %fd13;
	mul.f64 	%fd19, %fd18, 0d3FF0000000000000;
	cvt.rn.f32.f64	%f14, %fd19;
	atom.global.add.f32 	%f15, [%rd40], %f14;
	ld.global.s32 	%rd41, [%rd31];
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd43, %rd13, %rd42;
	mul.f64 	%fd20, %fd15, %fd15;
	mul.f64 	%fd21, %fd20, 0d3FF0000000000000;
	cvt.rn.f32.f64	%f16, %fd21;
	atom.global.add.f32 	%f17, [%rd43], %f16;

BB12_2:
	ret;
}

.visible .entry _Z33calculate_standard_deviations_GPUPiPfS0_S0_(
	.param .u64 _Z33calculate_standard_deviations_GPUPiPfS0_S0__param_0,
	.param .u64 _Z33calculate_standard_deviations_GPUPiPfS0_S0__param_1,
	.param .u64 _Z33calculate_standard_deviations_GPUPiPfS0_S0__param_2,
	.param .u64 _Z33calculate_standard_deviations_GPUPiPfS0_S0__param_3
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .s64 	%rd<16>;


	ld.param.u64 	%rd6, [_Z33calculate_standard_deviations_GPUPiPfS0_S0__param_0];
	ld.param.u64 	%rd3, [_Z33calculate_standard_deviations_GPUPiPfS0_S0__param_1];
	ld.param.u64 	%rd4, [_Z33calculate_standard_deviations_GPUPiPfS0_S0__param_2];
	ld.param.u64 	%rd5, [_Z33calculate_standard_deviations_GPUPiPfS0_S0__param_3];
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %ctaid.x;
	mul.lo.s32 	%r4, %r3, 21000;
	mad.lo.s32 	%r5, %r2, 350, %r4;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	cvt.s64.s32	%rd1, %r7;
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.s32 	%rd8, %r7, 4;
	add.s64 	%rd2, %rd7, %rd8;
	ld.global.u32 	%r1, [%rd2];
	setp.lt.s32	%p1, %r1, 1;
	@%p1 bra 	BB13_2;

	cvta.to.global.u64 	%rd9, %rd5;
	cvta.to.global.u64 	%rd10, %rd4;
	cvta.to.global.u64 	%rd11, %rd3;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	add.s32 	%r8, %r1, -1;
	cvt.rn.f32.s32	%f1, %r8;
	ld.global.f32 	%f2, [%rd13];
	div.rn.f32 	%f3, %f2, %f1;
	sqrt.rn.f32 	%f4, %f3;
	st.global.f32 	[%rd13], %f4;
	ld.global.u32 	%r9, [%rd2];
	add.s32 	%r10, %r9, -1;
	cvt.rn.f32.s32	%f5, %r10;
	add.s64 	%rd14, %rd10, %rd12;
	ld.global.f32 	%f6, [%rd14];
	div.rn.f32 	%f7, %f6, %f5;
	sqrt.rn.f32 	%f8, %f7;
	st.global.f32 	[%rd14], %f8;
	ld.global.u32 	%r11, [%rd2];
	add.s32 	%r12, %r11, -1;
	cvt.rn.f32.s32	%f9, %r12;
	add.s64 	%rd15, %rd9, %rd12;
	ld.global.f32 	%f10, [%rd15];
	div.rn.f32 	%f11, %f10, %f9;
	sqrt.rn.f32 	%f12, %f11;
	st.global.f32 	[%rd15], %f12;

BB13_2:
	bar.sync 	0;
	mov.u32 	%r13, 0;
	st.global.u32 	[%rd2], %r13;
	ret;
}

.visible .entry _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb(
	.param .u32 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_0,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_1,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_2,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_3,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_4,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_5,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_6,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_7,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_8,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_9,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_10,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_11,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_12,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_13,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_14,
	.param .u64 _Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_15
)
{
	.reg .pred 	%p<11>;
	.reg .s16 	%rs<6>;
	.reg .s32 	%r<7>;
	.reg .f32 	%f<21>;
	.reg .s64 	%rd<55>;
	.reg .f64 	%fd<19>;


	ld.param.u32 	%r2, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_0];
	ld.param.u64 	%rd5, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_1];
	ld.param.u64 	%rd6, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_2];
	ld.param.u64 	%rd7, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_3];
	ld.param.u64 	%rd8, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_4];
	ld.param.u64 	%rd9, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_5];
	ld.param.u64 	%rd10, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_6];
	ld.param.u64 	%rd11, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_7];
	ld.param.u64 	%rd12, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_8];
	ld.param.u64 	%rd13, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_9];
	ld.param.u64 	%rd14, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_10];
	ld.param.u64 	%rd15, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_11];
	ld.param.u64 	%rd16, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_12];
	ld.param.u64 	%rd17, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_13];
	ld.param.u64 	%rd18, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_14];
	ld.param.u64 	%rd19, [_Z20statistical_cuts_GPUiPiS_PfS0_S0_S0_S0_S0_S0_S0_S0_S0_S0_S0_Pb_param_15];
	mov.u32 	%r3, %ctaid.x;
	shl.b32 	%r4, %r3, 10;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r1, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB14_6;

	cvta.to.global.u64 	%rd20, %rd8;
	cvta.to.global.u64 	%rd21, %rd18;
	cvta.to.global.u64 	%rd22, %rd15;
	cvta.to.global.u64 	%rd23, %rd17;
	cvt.s64.s32	%rd1, %r1;
	cvta.to.global.u64 	%rd24, %rd11;
	mul.wide.s32 	%rd25, %r1, 4;
	add.s64 	%rd26, %rd24, %rd25;
	cvta.to.global.u64 	%rd27, %rd9;
	add.s64 	%rd28, %rd27, %rd25;
	ld.global.f32 	%f1, [%rd28];
	ld.global.f32 	%f2, [%rd26];
	sub.f32 	%f3, %f2, %f1;
	cvt.f64.f32	%fd1, %f3;
	setp.gt.f64	%p2, %fd1, 0d400921FB54442D18;
	add.f64 	%fd2, %fd1, 0dC01921FB54442D18;
	selp.f64	%fd3, %fd2, %fd1, %p2;
	setp.lt.f64	%p3, %fd3, 0dC00921FB54442D18;
	add.f64 	%fd4, %fd3, 0d401921FB54442D18;
	selp.f64	%fd5, %fd4, %fd3, %p3;
	cvta.to.global.u64 	%rd29, %rd12;
	add.s64 	%rd30, %rd29, %rd25;
	cvta.to.global.u64 	%rd31, %rd10;
	add.s64 	%rd32, %rd31, %rd25;
	ld.global.f32 	%f4, [%rd32];
	ld.global.f32 	%f5, [%rd30];
	sub.f32 	%f6, %f5, %f4;
	cvt.f64.f32	%fd6, %f6;
	setp.gt.f64	%p4, %fd6, 0d400921FB54442D18;
	add.f64 	%fd7, %fd6, 0dC01921FB54442D18;
	selp.f64	%fd8, %fd7, %fd6, %p4;
	setp.lt.f64	%p5, %fd8, 0dC00921FB54442D18;
	add.f64 	%fd9, %fd8, 0d401921FB54442D18;
	selp.f64	%fd10, %fd9, %fd8, %p5;
	cvta.to.global.u64 	%rd33, %rd6;
	add.s64 	%rd2, %rd33, %rd25;
	ld.global.s32 	%rd3, [%rd2];
	cvta.to.global.u64 	%rd34, %rd14;
	shl.b64 	%rd35, %rd3, 2;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.f32 	%f7, [%rd36];
	cvt.f64.f32	%fd11, %f7;
	sub.f64 	%fd12, %fd5, %fd11;
	abs.f64 	%fd13, %fd12;
	add.s64 	%rd37, %rd23, %rd35;
	ld.global.f32 	%f8, [%rd37];
	mul.f32 	%f9, %f8, 0f40400000;
	cvt.f64.f32	%fd14, %f9;
	add.s64 	%rd38, %rd22, %rd35;
	ld.global.f32 	%f10, [%rd38];
	cvt.f64.f32	%fd15, %f10;
	sub.f64 	%fd16, %fd10, %fd15;
	abs.f64 	%fd17, %fd16;
	add.s64 	%rd39, %rd21, %rd35;
	ld.global.f32 	%f11, [%rd39];
	mul.f32 	%f12, %f11, 0f40400000;
	cvt.f64.f32	%fd18, %f12;
	add.s64 	%rd4, %rd20, %rd25;
	setp.geu.f64	%p6, %fd13, %fd14;
	setp.geu.f64	%p7, %fd17, %fd18;
	or.pred  	%p8, %p6, %p7;
	@!%p8 bra 	BB14_3;
	bra.uni 	BB14_2;

BB14_2:
	mov.u16 	%rs5, 1;
	bra.uni 	BB14_4;

BB14_3:
	cvta.to.global.u64 	%rd40, %rd16;
	cvta.to.global.u64 	%rd41, %rd13;
	add.s64 	%rd43, %rd41, %rd35;
	ld.global.f32 	%f13, [%rd4];
	ld.global.f32 	%f14, [%rd43];
	sub.f32 	%f15, %f14, %f13;
	abs.f32 	%f16, %f15;
	add.s64 	%rd44, %rd40, %rd35;
	ld.global.f32 	%f17, [%rd44];
	mul.f32 	%f18, %f17, 0f40400000;
	setp.gtu.f32	%p9, %f16, %f18;
	selp.u16	%rs5, 1, 0, %p9;

BB14_4:
	cvta.to.global.u64 	%rd45, %rd19;
	add.s64 	%rd46, %rd45, %rd1;
	st.global.u8 	[%rd46], %rs5;
	and.b16  	%rs4, %rs5, 1;
	setp.eq.b16	%p10, %rs4, 1;
	@%p10 bra 	BB14_6;

	cvta.to.global.u64 	%rd47, %rd7;
	cvta.to.global.u64 	%rd48, %rd5;
	ld.global.s32 	%rd49, [%rd2];
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd48, %rd50;
	atom.global.add.u32 	%r6, [%rd51], 1;
	ld.global.s32 	%rd52, [%rd2];
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd47, %rd53;
	ld.global.f32 	%f19, [%rd4];
	atom.global.add.f32 	%f20, [%rd54], %f19;

BB14_6:
	ret;
}

.visible .entry _Z22construct_sinogram_GPUPiPf(
	.param .u64 _Z22construct_sinogram_GPUPiPf_param_0,
	.param .u64 _Z22construct_sinogram_GPUPiPf_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<8>;
	.reg .f32 	%f<4>;
	.reg .s64 	%rd<10>;


	ld.param.u64 	%rd3, [_Z22construct_sinogram_GPUPiPf_param_0];
	ld.param.u64 	%rd2, [_Z22construct_sinogram_GPUPiPf_param_1];
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %ctaid.x;
	mul.lo.s32 	%r4, %r3, 21000;
	mad.lo.s32 	%r5, %r2, 350, %r4;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	cvt.s64.s32	%rd1, %r7;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r7, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r1, [%rd6];
	setp.lt.s32	%p1, %r1, 1;
	@%p1 bra 	BB15_2;

	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	cvt.rn.f32.s32	%f1, %r1;
	ld.global.f32 	%f2, [%rd9];
	div.rn.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd9], %f3;

BB15_2:
	ret;
}

.visible .entry _Z10filter_GPUPfS_(
	.param .u64 _Z10filter_GPUPfS__param_0,
	.param .u64 _Z10filter_GPUPfS__param_1
)
{
	.reg .pred 	%p<56>;
	.reg .s32 	%r<82>;
	.reg .f32 	%f<4>;
	.reg .s64 	%rd<12>;
	.reg .f64 	%fd<83>;


	ld.param.u64 	%rd5, [_Z10filter_GPUPfS__param_0];
	ld.param.u64 	%rd4, [_Z10filter_GPUPfS__param_1];
	mov.u32 	%r15, %ctaid.x;
	add.s32 	%r16, %r15, -18;
	cvt.rn.f64.s32	%fd34, %r16;
	fma.rn.f64 	%fd35, %fd34, 0d3FD0000000000000, 0d3FC0000000000000;
	mul.f64 	%fd1, %fd35, %fd35;
	mov.f64 	%fd36, 0d3FF0000000000000;
	cvt.rzi.f64.f64	%fd37, %fd36;
	add.f64 	%fd38, %fd37, %fd37;
	mov.f64 	%fd39, 0d4000000000000000;
	sub.f64 	%fd40, %fd39, %fd38;
	abs.f64 	%fd2, %fd40;
	mov.f64 	%fd41, 0dBFE0000000000000;
	cvt.rzi.f64.f64	%fd42, %fd41;
	add.f64 	%fd43, %fd42, %fd42;
	mov.f64 	%fd44, 0dBFF0000000000000;
	sub.f64 	%fd45, %fd44, %fd43;
	abs.f64 	%fd3, %fd45;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %tid.x;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd44;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r2}, %fd39;
	}
	mov.f64 	%fd46, 0d3FD41B2F769CF0E0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd46;
	}
	mul.lo.s32 	%r19, %r17, 350;
	mad.lo.s32 	%r20, %r15, 21000, %r19;
	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.s32 	%rd7, %r20, 4;
	add.s64 	%rd11, %rd6, %rd7;
	shl.b32 	%r80, %r18, 1;
	mov.u32 	%r81, -175;
	mov.u32 	%r79, 0;
	cvta.to.global.u64 	%rd8, %rd4;

BB16_1:
	cvt.rn.f64.s32	%fd47, %r81;
	fma.rn.f64 	%fd48, %fd47, 0d3FB999999999999A, 0d3FA999999999999A;
	fma.rn.f64 	%fd49, %fd48, %fd48, 0d40F13C47D70A3D70;
	add.f64 	%fd50, %fd49, %fd1;
	sqrt.rn.f64 	%fd4, %fd50;
	mov.pred 	%p1, -1;
	mov.f64 	%fd80, 0d40028365EED39E1C;
	@%p1 bra 	BB16_3;

	mov.f64 	%fd80, 0d3FF0000000000000;
	bra.uni 	BB16_16;

BB16_3:
	abs.f64 	%fd5, %fd46;
	setp.gtu.f64	%p2, %fd5, 0d7FF0000000000000;
	@%p2 bra 	BB16_16;

	abs.f64 	%fd6, %fd39;
	setp.gtu.f64	%p3, %fd6, 0d7FF0000000000000;
	@%p3 bra 	BB16_16;

	setp.eq.f64	%p4, %fd6, 0d7FF0000000000000;
	@%p4 bra 	BB16_15;

	setp.eq.f64	%p5, %fd5, 0d7FF0000000000000;
	@%p5 bra 	BB16_14;

	mov.pred 	%p6, 0;
	@%p6 bra 	BB16_13;

	setp.gt.s32	%p7, %r3, -1;
	@%p7 bra 	BB16_11;

	cvt.rzi.f64.f64	%fd56, %fd39;
	setp.eq.f64	%p8, %fd56, 0d4000000000000000;
	@%p8 bra 	BB16_11;

	mov.f64 	%fd80, 0dFFF8000000000000;
	bra.uni 	BB16_16;

BB16_11:
	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd5;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd80, [retval0+0];
	}
	// Callseq End 17
	setp.eq.f64	%p9, %fd2, 0d3FF0000000000000;
	setp.lt.s32	%p10, %r3, 0;
	and.pred  	%p11, %p10, %p9;
	@!%p11 bra 	BB16_16;
	bra.uni 	BB16_12;

BB16_12:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd80;
	}
	xor.b32  	%r22, %r21, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd80;
	}
	mov.b64 	%fd80, {%r23, %r22};
	bra.uni 	BB16_16;

BB16_13:
	setp.lt.s32	%p12, %r2, 0;
	mov.u32 	%r24, 0;
	setp.eq.f64	%p13, %fd2, 0d3FF0000000000000;
	selp.b32	%r25, %r3, 0, %p13;
	or.b32  	%r26, %r25, 2146435072;
	selp.b32	%r27, %r26, %r25, %p12;
	mov.b64 	%fd80, {%r24, %r27};
	bra.uni 	BB16_16;

BB16_14:
	setp.lt.s32	%p14, %r3, 0;
	mov.u32 	%r28, 0;
	setp.eq.f64	%p15, %fd2, 0d3FF0000000000000;
	and.pred  	%p16, %p14, %p15;
	shr.s32 	%r29, %r2, 31;
	and.b32  	%r30, %r29, -2146435072;
	add.s32 	%r31, %r30, 2146435072;
	or.b32  	%r32, %r31, -2147483648;
	selp.b32	%r33, %r32, %r31, %p16;
	mov.b64 	%fd80, {%r28, %r33};
	bra.uni 	BB16_16;

BB16_15:
	setp.lt.s32	%p17, %r2, 0;
	mov.u32 	%r34, 0;
	setp.gt.f64	%p18, %fd5, 0d3FF0000000000000;
	selp.b32	%r35, 2146435072, 0, %p18;
	xor.b32  	%r36, %r35, 2146435072;
	selp.b32	%r37, %r36, %r35, %p17;
	mov.b64 	%fd80, {%r34, %r37};

BB16_16:
	mov.f64 	%fd60, 0d40709B3333333333;
	div.rn.f64 	%fd13, %fd60, %fd4;
	cvt.rn.f64.s32	%fd14, %r80;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd14;
	}
	abs.f64 	%fd15, %fd14;
	setp.gtu.f64	%p19, %fd15, 0d7FF0000000000000;
	@%p19 bra 	BB16_29;

	abs.f64 	%fd16, %fd39;
	setp.gtu.f64	%p20, %fd16, 0d7FF0000000000000;
	@%p20 bra 	BB16_29;

	setp.eq.f64	%p21, %fd16, 0d7FF0000000000000;
	@%p21 bra 	BB16_28;

	setp.eq.f64	%p22, %fd15, 0d7FF0000000000000;
	@%p22 bra 	BB16_27;

	setp.eq.s32	%p23, %r80, 0;
	@%p23 bra 	BB16_26;

	setp.gt.s32	%p24, %r8, -1;
	@%p24 bra 	BB16_24;

	cvt.rzi.f64.f64	%fd63, %fd39;
	setp.eq.f64	%p25, %fd63, 0d4000000000000000;
	@%p25 bra 	BB16_24;

	mov.f64 	%fd81, 0dFFF8000000000000;
	bra.uni 	BB16_30;

BB16_24:
	setp.lt.s32	%p26, %r8, 0;
	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd15;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd81, [retval0+0];
	}
	// Callseq End 18
	setp.eq.f64	%p27, %fd2, 0d3FF0000000000000;
	and.pred  	%p28, %p26, %p27;
	@!%p28 bra 	BB16_30;
	bra.uni 	BB16_25;

BB16_25:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r38}, %fd81;
	}
	xor.b32  	%r39, %r38, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r40, %temp}, %fd81;
	}
	mov.b64 	%fd81, {%r40, %r39};
	bra.uni 	BB16_30;

BB16_26:
	setp.lt.s32	%p29, %r2, 0;
	mov.u32 	%r41, 0;
	setp.eq.f64	%p30, %fd2, 0d3FF0000000000000;
	selp.b32	%r42, %r8, 0, %p30;
	or.b32  	%r43, %r42, 2146435072;
	selp.b32	%r44, %r43, %r42, %p29;
	mov.b64 	%fd81, {%r41, %r44};
	bra.uni 	BB16_30;

BB16_27:
	setp.eq.f64	%p31, %fd2, 0d3FF0000000000000;
	setp.lt.s32	%p32, %r8, 0;
	mov.u32 	%r45, 0;
	and.pred  	%p33, %p32, %p31;
	shr.s32 	%r46, %r2, 31;
	and.b32  	%r47, %r46, -2146435072;
	add.s32 	%r48, %r47, 2146435072;
	or.b32  	%r49, %r48, -2147483648;
	selp.b32	%r50, %r49, %r48, %p33;
	mov.b64 	%fd81, {%r45, %r50};
	bra.uni 	BB16_30;

BB16_28:
	setp.lt.s32	%p34, %r2, 0;
	mov.u32 	%r51, 0;
	setp.gt.f64	%p35, %fd15, 0d3FF0000000000000;
	selp.b32	%r52, 2146435072, 0, %p35;
	xor.b32  	%r53, %r52, 2146435072;
	selp.b32	%r54, %r53, %r52, %p34;
	mov.b64 	%fd81, {%r51, %r54};
	bra.uni 	BB16_30;

BB16_29:
	add.f64 	%fd81, %fd14, 0d4000000000000000;

BB16_30:
	sub.f64 	%fd67, %fd36, %fd81;
	mul.f64 	%fd24, %fd80, %fd67;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd24;
	}
	setp.neu.f64	%p36, %fd24, 0d3FF0000000000000;
	@%p36 bra 	BB16_32;

	mov.f64 	%fd82, %fd36;
	bra.uni 	BB16_46;

BB16_32:
	abs.f64 	%fd25, %fd24;
	setp.gtu.f64	%p37, %fd25, 0d7FF0000000000000;
	@%p37 bra 	BB16_45;

	abs.f64 	%fd26, %fd44;
	setp.gtu.f64	%p38, %fd26, 0d7FF0000000000000;
	@%p38 bra 	BB16_45;

	setp.eq.f64	%p39, %fd26, 0d7FF0000000000000;
	@%p39 bra 	BB16_44;

	setp.eq.f64	%p40, %fd25, 0d7FF0000000000000;
	@%p40 bra 	BB16_43;

	setp.eq.f64	%p41, %fd24, 0d0000000000000000;
	@%p41 bra 	BB16_42;

	setp.gt.s32	%p42, %r9, -1;
	@%p42 bra 	BB16_40;

	cvt.rzi.f64.f64	%fd70, %fd44;
	setp.eq.f64	%p43, %fd70, 0dBFF0000000000000;
	@%p43 bra 	BB16_40;

	mov.f64 	%fd72, 0dFFF8000000000000;
	mov.f64 	%fd82, %fd72;
	bra.uni 	BB16_46;

BB16_40:
	setp.lt.s32	%p44, %r9, 0;
	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd25;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd27, [retval0+0];
	}
	// Callseq End 19
	setp.eq.f64	%p45, %fd3, 0d3FF0000000000000;
	and.pred  	%p46, %p44, %p45;
	mov.f64 	%fd82, %fd27;
	@!%p46 bra 	BB16_46;
	bra.uni 	BB16_41;

BB16_41:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd27;
	}
	xor.b32  	%r56, %r55, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r57, %temp}, %fd27;
	}
	mov.b64 	%fd28, {%r57, %r56};
	mov.f64 	%fd82, %fd28;
	bra.uni 	BB16_46;

BB16_42:
	setp.lt.s32	%p47, %r1, 0;
	mov.u32 	%r58, 0;
	setp.eq.f64	%p48, %fd3, 0d3FF0000000000000;
	selp.b32	%r59, %r9, 0, %p48;
	or.b32  	%r60, %r59, 2146435072;
	selp.b32	%r61, %r60, %r59, %p47;
	mov.b64 	%fd29, {%r58, %r61};
	mov.f64 	%fd82, %fd29;
	bra.uni 	BB16_46;

BB16_43:
	setp.eq.f64	%p49, %fd3, 0d3FF0000000000000;
	setp.lt.s32	%p50, %r9, 0;
	mov.u32 	%r62, 0;
	and.pred  	%p51, %p50, %p49;
	shr.s32 	%r63, %r1, 31;
	and.b32  	%r64, %r63, -2146435072;
	add.s32 	%r65, %r64, 2146435072;
	or.b32  	%r66, %r65, -2147483648;
	selp.b32	%r67, %r66, %r65, %p51;
	mov.b64 	%fd30, {%r62, %r67};
	mov.f64 	%fd82, %fd30;
	bra.uni 	BB16_46;

BB16_44:
	setp.lt.s32	%p52, %r1, 0;
	mov.u32 	%r68, 0;
	setp.gt.f64	%p53, %fd25, 0d3FF0000000000000;
	selp.b32	%r69, 2146435072, 0, %p53;
	xor.b32  	%r70, %r69, 2146435072;
	selp.b32	%r71, %r70, %r69, %p52;
	setp.eq.f64	%p54, %fd24, 0dBFF0000000000000;
	selp.b32	%r72, 1072693248, %r71, %p54;
	mov.b64 	%fd31, {%r68, %r72};
	mov.f64 	%fd82, %fd31;
	bra.uni 	BB16_46;

BB16_45:
	add.f64 	%fd32, %fd24, 0dBFF0000000000000;
	mov.f64 	%fd82, %fd32;

BB16_46:
	mov.f64 	%fd33, %fd82;
	ld.global.f32 	%f1, [%rd11];
	cvt.f64.f32	%fd74, %f1;
	mul.f64 	%fd75, %fd74, 0d3FB999999999999A;
	mul.f64 	%fd76, %fd75, %fd33;
	mul.lo.s32 	%r75, %r15, 21000;
	mad.lo.s32 	%r76, %r17, 350, %r75;
	add.s32 	%r78, %r76, %r18;
	mul.wide.s32 	%rd9, %r78, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f32 	%f2, [%rd10];
	cvt.f64.f32	%fd77, %f2;
	fma.rn.f64 	%fd78, %fd76, %fd13, %fd77;
	cvt.rn.f32.f64	%f3, %fd78;
	st.global.f32 	[%rd10], %f3;
	add.s32 	%r81, %r81, 1;
	add.s64 	%rd11, %rd11, 4;
	add.s32 	%r80, %r80, -2;
	add.s32 	%r79, %r79, -2;
	setp.ne.s32	%p55, %r79, -700;
	@%p55 bra 	BB16_1;

	ret;
}

.visible .entry _Z18backprojection_GPUPfS_(
	.param .u64 _Z18backprojection_GPUPfS__param_0,
	.param .u64 _Z18backprojection_GPUPfS__param_1
)
{
	.local .align 4 .b8 	__local_depot17[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<29>;
	.reg .s32 	%r<74>;
	.reg .f32 	%f<11>;
	.reg .s64 	%rd<49>;
	.reg .f64 	%fd<236>;


	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd3, [_Z18backprojection_GPUPfS__param_0];
	ld.param.u64 	%rd4, [_Z18backprojection_GPUPfS__param_1];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r21, %r2, 200, %r3;
	mad.lo.s32 	%r4, %r1, 40000, %r21;
	setp.gt.s32	%p1, %r4, 959999;
	@%p1 bra 	BB17_41;

	cvta.to.global.u64 	%rd5, %rd4;
	cvt.rn.f64.s32	%fd64, %r3;
	add.f64 	%fd65, %fd64, 0d3FE0000000000000;
	fma.rn.f64 	%fd1, %fd65, 0d3FB47AE147AE147B, 0dC020000000000000;
	cvt.rn.f64.s32	%fd66, %r2;
	add.f64 	%fd67, %fd66, 0d3FE0000000000000;
	mul.f64 	%fd68, %fd67, 0d3FB47AE147AE147B;
	mov.f64 	%fd69, 0d4020000000000000;
	sub.f64 	%fd2, %fd69, %fd68;
	mul.f64 	%fd70, %fd2, %fd2;
	fma.rn.f64 	%fd71, %fd1, %fd1, %fd70;
	setp.gt.f64	%p2, %fd71, 0d4050000000000000;
	mul.wide.s32 	%rd6, %r4, 4;
	add.s64 	%rd1, %rd5, %rd6;
	@%p2 bra 	BB17_40;

	cvt.rn.f64.s32	%fd73, %r1;
	add.f64 	%fd74, %fd73, 0d3FE0000000000000;
	fma.rn.f64 	%fd3, %fd74, 0d3FD0000000000000, 0dC008000000000000;
	mov.u32 	%r69, 0;
	mov.f64 	%fd219, 0d0000000000000000;
	cvta.to.global.u64 	%rd35, %rd3;

BB17_3:
	mul.f64 	%fd5, %fd219, 0d3FBACEE9F37BEBD6;
	abs.f64 	%fd6, %fd5;
	setp.neu.f64	%p3, %fd6, 0d7FF0000000000000;
	mov.f64 	%fd232, %fd5;
	@%p3 bra 	BB17_5;

	mov.f64 	%fd75, 0d0000000000000000;
	mul.rn.f64 	%fd7, %fd5, %fd75;
	mov.f64 	%fd232, %fd7;

BB17_5:
	mov.f64 	%fd8, %fd232;
	add.u64 	%rd7, %SP, 0;
	mul.f64 	%fd76, %fd8, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r70, %fd76;
	cvta.to.local.u64 	%rd8, %rd7;
	st.local.u32 	[%rd8], %r70;
	cvt.rn.f64.s32	%fd77, %r70;
	neg.f64 	%fd78, %fd77;
	mov.f64 	%fd79, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd80, %fd78, %fd79, %fd8;
	mov.f64 	%fd81, 0d3C91A62633145C00;
	fma.rn.f64 	%fd82, %fd78, %fd81, %fd80;
	mov.f64 	%fd83, 0d397B839A252049C0;
	fma.rn.f64 	%fd220, %fd78, %fd83, %fd82;
	abs.f64 	%fd84, %fd8;
	setp.leu.f64	%p4, %fd84, 0d41E0000000000000;
	@%p4 bra 	BB17_7;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd8;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd7;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd220, [retval0+0];
	}
	// Callseq End 20
	ld.local.u32 	%r70, [%rd8];

BB17_7:
	add.s32 	%r9, %r70, 1;
	shl.b32 	%r23, %r9, 3;
	and.b32  	%r24, %r23, 8;
	and.b32  	%r25, %r9, 1;
	setp.eq.b32	%p5, %r25, 1;
	not.pred 	%p6, %p5;
	selp.f64	%fd85, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p6;
	mul.wide.u32 	%rd11, %r24, 8;
	mov.u64 	%rd12, __cudart_sin_cos_coeffs;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.f64 	%fd86, [%rd13+8];
	mul.rn.f64 	%fd12, %fd220, %fd220;
	fma.rn.f64 	%fd87, %fd85, %fd12, %fd86;
	ld.const.f64 	%fd88, [%rd13+16];
	fma.rn.f64 	%fd89, %fd87, %fd12, %fd88;
	ld.const.f64 	%fd90, [%rd13+24];
	fma.rn.f64 	%fd91, %fd89, %fd12, %fd90;
	ld.const.f64 	%fd92, [%rd13+32];
	fma.rn.f64 	%fd93, %fd91, %fd12, %fd92;
	ld.const.f64 	%fd94, [%rd13+40];
	fma.rn.f64 	%fd95, %fd93, %fd12, %fd94;
	ld.const.f64 	%fd96, [%rd13+48];
	fma.rn.f64 	%fd13, %fd95, %fd12, %fd96;
	fma.rn.f64 	%fd221, %fd13, %fd220, %fd220;
	@%p6 bra 	BB17_9;

	mov.f64 	%fd97, 0d3FF0000000000000;
	fma.rn.f64 	%fd221, %fd13, %fd12, %fd97;

BB17_9:
	and.b32  	%r26, %r9, 2;
	setp.eq.s32	%p7, %r26, 0;
	@%p7 bra 	BB17_11;

	mov.f64 	%fd98, 0d0000000000000000;
	mov.f64 	%fd99, 0dBFF0000000000000;
	fma.rn.f64 	%fd221, %fd221, %fd99, %fd98;

BB17_11:
	mov.f64 	%fd231, %fd5;
	@%p3 bra 	BB17_13;

	mov.f64 	%fd100, 0d0000000000000000;
	mul.rn.f64 	%fd231, %fd5, %fd100;

BB17_13:
	add.u64 	%rd14, %SP, 4;
	mul.f64 	%fd101, %fd231, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r71, %fd101;
	cvta.to.local.u64 	%rd15, %rd14;
	st.local.u32 	[%rd15], %r71;
	cvt.rn.f64.s32	%fd102, %r71;
	neg.f64 	%fd103, %fd102;
	fma.rn.f64 	%fd105, %fd103, %fd79, %fd231;
	fma.rn.f64 	%fd107, %fd103, %fd81, %fd105;
	fma.rn.f64 	%fd222, %fd103, %fd83, %fd107;
	abs.f64 	%fd109, %fd231;
	setp.leu.f64	%p9, %fd109, 0d41E0000000000000;
	@%p9 bra 	BB17_15;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd231;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd14;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd222, [retval0+0];
	}
	// Callseq End 21
	ld.local.u32 	%r71, [%rd15];

BB17_15:
	shl.b32 	%r27, %r71, 3;
	and.b32  	%r28, %r27, 8;
	and.b32  	%r29, %r71, 1;
	setp.eq.b32	%p10, %r29, 1;
	not.pred 	%p11, %p10;
	selp.f64	%fd110, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p11;
	mul.wide.u32 	%rd18, %r28, 8;
	add.s64 	%rd20, %rd12, %rd18;
	ld.const.f64 	%fd111, [%rd20+8];
	mul.rn.f64 	%fd24, %fd222, %fd222;
	fma.rn.f64 	%fd112, %fd110, %fd24, %fd111;
	ld.const.f64 	%fd113, [%rd20+16];
	fma.rn.f64 	%fd114, %fd112, %fd24, %fd113;
	ld.const.f64 	%fd115, [%rd20+24];
	fma.rn.f64 	%fd116, %fd114, %fd24, %fd115;
	ld.const.f64 	%fd117, [%rd20+32];
	fma.rn.f64 	%fd118, %fd116, %fd24, %fd117;
	ld.const.f64 	%fd119, [%rd20+40];
	fma.rn.f64 	%fd120, %fd118, %fd24, %fd119;
	ld.const.f64 	%fd121, [%rd20+48];
	fma.rn.f64 	%fd25, %fd120, %fd24, %fd121;
	fma.rn.f64 	%fd223, %fd25, %fd222, %fd222;
	@%p11 bra 	BB17_17;

	mov.f64 	%fd122, 0d3FF0000000000000;
	fma.rn.f64 	%fd223, %fd25, %fd24, %fd122;

BB17_17:
	and.b32  	%r30, %r71, 2;
	setp.eq.s32	%p12, %r30, 0;
	@%p12 bra 	BB17_19;

	mov.f64 	%fd123, 0d0000000000000000;
	mov.f64 	%fd124, 0dBFF0000000000000;
	fma.rn.f64 	%fd223, %fd223, %fd124, %fd123;

BB17_19:
	mul.f64 	%fd125, %fd2, %fd223;
	fma.rn.f64 	%fd31, %fd1, %fd221, %fd125;
	mov.f64 	%fd230, %fd5;
	@%p3 bra 	BB17_21;

	mov.f64 	%fd126, 0d0000000000000000;
	mul.rn.f64 	%fd230, %fd5, %fd126;

BB17_21:
	add.u64 	%rd21, %SP, 8;
	mul.f64 	%fd127, %fd230, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r72, %fd127;
	cvta.to.local.u64 	%rd22, %rd21;
	st.local.u32 	[%rd22], %r72;
	cvt.rn.f64.s32	%fd128, %r72;
	neg.f64 	%fd129, %fd128;
	fma.rn.f64 	%fd131, %fd129, %fd79, %fd230;
	fma.rn.f64 	%fd133, %fd129, %fd81, %fd131;
	fma.rn.f64 	%fd224, %fd129, %fd83, %fd133;
	abs.f64 	%fd135, %fd230;
	setp.leu.f64	%p14, %fd135, 0d41E0000000000000;
	@%p14 bra 	BB17_23;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd230;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd21;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd224, [retval0+0];
	}
	// Callseq End 22
	ld.local.u32 	%r72, [%rd22];

BB17_23:
	shl.b32 	%r31, %r72, 3;
	and.b32  	%r32, %r31, 8;
	and.b32  	%r33, %r72, 1;
	setp.eq.b32	%p15, %r33, 1;
	not.pred 	%p16, %p15;
	selp.f64	%fd136, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p16;
	mul.wide.u32 	%rd25, %r32, 8;
	add.s64 	%rd27, %rd12, %rd25;
	ld.const.f64 	%fd137, [%rd27+8];
	mul.rn.f64 	%fd37, %fd224, %fd224;
	fma.rn.f64 	%fd138, %fd136, %fd37, %fd137;
	ld.const.f64 	%fd139, [%rd27+16];
	fma.rn.f64 	%fd140, %fd138, %fd37, %fd139;
	ld.const.f64 	%fd141, [%rd27+24];
	fma.rn.f64 	%fd142, %fd140, %fd37, %fd141;
	ld.const.f64 	%fd143, [%rd27+32];
	fma.rn.f64 	%fd144, %fd142, %fd37, %fd143;
	ld.const.f64 	%fd145, [%rd27+40];
	fma.rn.f64 	%fd146, %fd144, %fd37, %fd145;
	ld.const.f64 	%fd147, [%rd27+48];
	fma.rn.f64 	%fd38, %fd146, %fd37, %fd147;
	fma.rn.f64 	%fd225, %fd38, %fd224, %fd224;
	@%p16 bra 	BB17_25;

	mov.f64 	%fd148, 0d3FF0000000000000;
	fma.rn.f64 	%fd225, %fd38, %fd37, %fd148;

BB17_25:
	and.b32  	%r34, %r72, 2;
	setp.eq.s32	%p17, %r34, 0;
	@%p17 bra 	BB17_27;

	mov.f64 	%fd149, 0d0000000000000000;
	mov.f64 	%fd150, 0dBFF0000000000000;
	fma.rn.f64 	%fd225, %fd225, %fd150, %fd149;

BB17_27:
	mov.f64 	%fd229, %fd5;
	@%p3 bra 	BB17_29;

	mov.f64 	%fd151, 0d0000000000000000;
	mul.rn.f64 	%fd229, %fd5, %fd151;

BB17_29:
	add.u64 	%rd28, %SP, 12;
	mul.f64 	%fd152, %fd229, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r73, %fd152;
	cvta.to.local.u64 	%rd29, %rd28;
	st.local.u32 	[%rd29], %r73;
	cvt.rn.f64.s32	%fd153, %r73;
	neg.f64 	%fd154, %fd153;
	fma.rn.f64 	%fd156, %fd154, %fd79, %fd229;
	fma.rn.f64 	%fd158, %fd154, %fd81, %fd156;
	fma.rn.f64 	%fd233, %fd154, %fd83, %fd158;
	abs.f64 	%fd160, %fd229;
	setp.leu.f64	%p19, %fd160, 0d41E0000000000000;
	@%p19 bra 	BB17_31;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd229;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd28;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd233, [retval0+0];
	}
	// Callseq End 23
	ld.local.u32 	%r73, [%rd29];

BB17_31:
	add.s32 	%r19, %r73, 1;
	shl.b32 	%r35, %r19, 3;
	and.b32  	%r36, %r35, 8;
	and.b32  	%r37, %r19, 1;
	setp.eq.b32	%p20, %r37, 1;
	not.pred 	%p21, %p20;
	selp.f64	%fd161, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p21;
	mul.wide.u32 	%rd32, %r36, 8;
	add.s64 	%rd34, %rd12, %rd32;
	ld.const.f64 	%fd162, [%rd34+8];
	mul.rn.f64 	%fd49, %fd233, %fd233;
	fma.rn.f64 	%fd163, %fd161, %fd49, %fd162;
	ld.const.f64 	%fd164, [%rd34+16];
	fma.rn.f64 	%fd165, %fd163, %fd49, %fd164;
	ld.const.f64 	%fd166, [%rd34+24];
	fma.rn.f64 	%fd167, %fd165, %fd49, %fd166;
	ld.const.f64 	%fd168, [%rd34+32];
	fma.rn.f64 	%fd169, %fd167, %fd49, %fd168;
	ld.const.f64 	%fd170, [%rd34+40];
	fma.rn.f64 	%fd171, %fd169, %fd49, %fd170;
	ld.const.f64 	%fd172, [%rd34+48];
	fma.rn.f64 	%fd50, %fd171, %fd49, %fd172;
	fma.rn.f64 	%fd234, %fd50, %fd233, %fd233;
	@%p21 bra 	BB17_33;

	mov.f64 	%fd173, 0d3FF0000000000000;
	fma.rn.f64 	%fd234, %fd50, %fd49, %fd173;

BB17_33:
	and.b32  	%r38, %r19, 2;
	setp.eq.s32	%p22, %r38, 0;
	@%p22 bra 	BB17_35;

	mov.f64 	%fd174, 0d0000000000000000;
	mov.f64 	%fd175, 0dBFF0000000000000;
	fma.rn.f64 	%fd234, %fd234, %fd175, %fd174;

BB17_35:
	add.f64 	%fd176, %fd31, 0d40709B3333333333;
	mov.f64 	%fd177, 0d40709B3333333333;
	div.rn.f64 	%fd178, %fd177, %fd176;
	mul.f64 	%fd179, %fd178, %fd178;
	mul.f64 	%fd56, %fd179, 0d3FF0000000000000;
	mov.f64 	%fd180, 0d3FF0000000000000;
	mul.f64 	%fd181, %fd2, %fd234;
	mul.f64 	%fd182, %fd225, %fd1;
	sub.f64 	%fd183, %fd181, %fd182;
	div.rn.f64 	%fd184, %fd183, %fd176;
	mul.f64 	%fd185, %fd31, %fd184;
	sub.f64 	%fd186, %fd183, %fd185;
	div.rn.f64 	%fd187, %fd186, 0d3FB999999999999A;
	add.f64 	%fd188, %fd187, 0d4065E00000000000;
	cvt.rzi.s32.f64	%r39, %fd188;
	cvt.rn.f64.s32	%fd189, %r39;
	setp.gt.f64	%p23, %fd189, %fd188;
	selp.b32	%r40, -1, 0, %p23;
	add.s32 	%r41, %r40, %r39;
	cvt.rn.f64.s32	%fd190, %r41;
	sub.f64 	%fd57, %fd188, %fd190;
	div.rn.f64 	%fd191, %fd3, %fd176;
	mul.f64 	%fd192, %fd31, %fd191;
	sub.f64 	%fd193, %fd3, %fd192;
	fma.rn.f64 	%fd194, %fd193, 0d4010000000000000, 0d4032000000000000;
	cvt.rzi.s32.f64	%r42, %fd194;
	cvt.rn.f64.s32	%fd195, %r42;
	setp.gt.f64	%p24, %fd195, %fd194;
	selp.b32	%r43, -1, 0, %p24;
	add.s32 	%r44, %r43, %r42;
	cvt.rn.f64.s32	%fd196, %r44;
	sub.f64 	%fd58, %fd194, %fd196;
	setp.eq.s32	%p25, %r44, 35;
	mad.lo.s32 	%r45, %r69, 350, %r39;
	mad.lo.s32 	%r46, %r44, 21000, %r45;
	add.s32 	%r47, %r46, %r40;
	setp.lt.s32	%p26, %r47, 0;
	or.pred  	%p27, %p25, %p26;
	sub.f64 	%fd59, %fd180, %fd57;
	mul.wide.s32 	%rd36, %r47, 4;
	add.s64 	%rd2, %rd35, %rd36;
	@%p27 bra 	BB17_37;

	sub.f64 	%fd198, %fd180, %fd58;
	mul.f64 	%fd199, %fd59, %fd198;
	mul.f64 	%fd200, %fd57, %fd198;
	ld.global.f32 	%f2, [%rd2+4];
	cvt.f64.f32	%fd201, %f2;
	mul.f64 	%fd202, %fd200, %fd201;
	ld.global.f32 	%f3, [%rd2];
	cvt.f64.f32	%fd203, %f3;
	fma.rn.f64 	%fd204, %fd199, %fd203, %fd202;
	mul.f64 	%fd205, %fd59, %fd58;
	ld.global.f32 	%f4, [%rd2+84000];
	cvt.f64.f32	%fd206, %f4;
	fma.rn.f64 	%fd207, %fd205, %fd206, %fd204;
	mul.f64 	%fd208, %fd57, %fd58;
	ld.global.f32 	%f5, [%rd2+84004];
	cvt.f64.f32	%fd209, %f5;
	fma.rn.f64 	%fd210, %fd208, %fd209, %fd207;
	mul.wide.s32 	%rd38, %r4, 4;
	add.s64 	%rd39, %rd5, %rd38;
	ld.global.f32 	%f6, [%rd39];
	cvt.f64.f32	%fd211, %f6;
	fma.rn.f64 	%fd235, %fd56, %fd210, %fd211;
	bra.uni 	BB17_38;

BB17_37:
	ld.global.f32 	%f7, [%rd2+4];
	cvt.f64.f32	%fd212, %f7;
	mul.f64 	%fd213, %fd57, %fd212;
	ld.global.f32 	%f8, [%rd2];
	cvt.f64.f32	%fd214, %f8;
	fma.rn.f64 	%fd215, %fd59, %fd214, %fd213;
	mul.wide.s32 	%rd41, %r4, 4;
	add.s64 	%rd42, %rd5, %rd41;
	ld.global.f32 	%f9, [%rd42];
	cvt.f64.f32	%fd216, %f9;
	fma.rn.f64 	%fd235, %fd56, %fd215, %fd216;

BB17_38:
	mul.wide.s32 	%rd44, %r4, 4;
	add.s64 	%rd45, %rd5, %rd44;
	cvt.rn.f32.f64	%f1, %fd235;
	st.global.f32 	[%rd45], %f1;
	add.f64 	%fd219, %fd219, 0d3FF0000000000000;
	add.s32 	%r69, %r69, 1;
	setp.ne.s32	%p28, %r69, 60;
	@%p28 bra 	BB17_3;

	cvt.f64.f32	%fd217, %f1;
	mul.f64 	%fd218, %fd217, 0d3FBACEE9F37BEBD6;
	cvt.rn.f32.f64	%f10, %fd218;
	mul.wide.s32 	%rd47, %r4, 4;
	add.s64 	%rd48, %rd5, %rd47;
	st.global.f32 	[%rd48], %f10;
	bra.uni 	BB17_41;

BB17_40:
	mov.u32 	%r68, 982785154;
	st.global.u32 	[%rd1], %r68;

BB17_41:
	ret;
}

.visible .entry _Z20FBP_image_2_hull_GPUPfPb(
	.param .u64 _Z20FBP_image_2_hull_GPUPfPb_param_0,
	.param .u64 _Z20FBP_image_2_hull_GPUPfPb_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<2>;
	.reg .s64 	%rd<9>;
	.reg .f64 	%fd<14>;


	ld.param.u64 	%rd3, [_Z20FBP_image_2_hull_GPUPfPb_param_0];
	ld.param.u64 	%rd2, [_Z20FBP_image_2_hull_GPUPfPb_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, 200, %r1;
	mad.lo.s32 	%r5, %r3, 40000, %r4;
	cvt.s64.s32	%rd1, %r5;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r5, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	cvt.f64.f32	%fd1, %f1;
	setp.gt.f64	%p1, %fd1, 0d3FE3333333333333;
	@%p1 bra 	BB18_2;

	mov.u16 	%rs4, 0;
	bra.uni 	BB18_3;

BB18_2:
	cvt.rn.f64.s32	%fd2, %r1;
	add.f64 	%fd3, %fd2, 0d3FE0000000000000;
	fma.rn.f64 	%fd4, %fd3, 0d3FB47AE147AE147B, 0dC020000000000000;
	cvt.rn.f64.s32	%fd5, %r2;
	add.f64 	%fd6, %fd5, 0d3FE0000000000000;
	mul.f64 	%fd7, %fd6, 0d3FB47AE147AE147B;
	mul.f64 	%fd8, %fd4, %fd4;
	mov.f64 	%fd9, 0d4020000000000000;
	sub.f64 	%fd10, %fd9, %fd7;
	mul.f64 	%fd11, %fd10, %fd10;
	mul.f64 	%fd12, %fd11, 0d3FF0000000000000;
	fma.rn.f64 	%fd13, %fd8, 0d3FF0000000000000, %fd12;
	setp.lt.f64	%p2, %fd13, 0d4050000000000000;
	selp.u16	%rs4, 1, 0, %p2;

BB18_3:
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd1;
	st.global.u8 	[%rd8], %rs4;
	ret;
}

.visible .entry _Z17carve_differencesPiS_(
	.param .u64 _Z17carve_differencesPiS__param_0,
	.param .u64 _Z17carve_differencesPiS__param_1
)
{
	.reg .pred 	%p<10>;
	.reg .s32 	%r<38>;
	.reg .s64 	%rd<15>;


	ld.param.u64 	%rd5, [_Z17carve_differencesPiS__param_0];
	ld.param.u64 	%rd6, [_Z17carve_differencesPiS__param_1];
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	setp.ne.s32	%p1, %r1, 199;
	setp.ne.s32	%p2, %r1, 0;
	and.pred  	%p3, %p2, %p1;
	setp.ne.s32	%p4, %r2, 0;
	and.pred  	%p5, %p3, %p4;
	setp.ne.s32	%p6, %r2, 199;
	and.pred  	%p7, %p5, %p6;
	@!%p7 bra 	BB19_6;
	bra.uni 	BB19_1;

BB19_1:
	cvta.to.global.u64 	%rd7, %rd6;
	mul.lo.s32 	%r20, %r1, 200;
	mov.u32 	%r21, %tid.x;
	mul.lo.s32 	%r22, %r21, 40000;
	add.s32 	%r23, %r2, %r20;
	add.s32 	%r24, %r23, %r22;
	add.s32 	%r35, %r1, -1;
	add.s32 	%r4, %r1, 1;
	add.s32 	%r5, %r2, -1;
	add.s32 	%r6, %r2, 1;
	mul.wide.s32 	%rd8, %r24, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%r7, [%rd9];
	add.s64 	%rd1, %rd7, -804;
	add.s32 	%r25, %r2, %r22;
	add.s32 	%r8, %r25, %r20;
	mov.u32 	%r37, 0;
	mov.u32 	%r34, %r37;

BB19_2:
	add.s32 	%r26, %r8, %r34;
	mul.wide.s32 	%rd10, %r26, 4;
	add.s64 	%rd14, %rd1, %rd10;
	mov.u32 	%r36, %r5;

BB19_3:
	mov.u32 	%r12, %r36;
	ld.global.u32 	%r27, [%rd14];
	sub.s32 	%r28, %r7, %r27;
	max.s32 	%r37, %r28, %r37;
	add.s64 	%rd14, %rd14, 4;
	add.s32 	%r15, %r12, 1;
	setp.le.s32	%p8, %r15, %r6;
	mov.u32 	%r36, %r15;
	@%p8 bra 	BB19_3;

	add.s32 	%r35, %r35, 1;
	add.s32 	%r34, %r34, 200;
	setp.le.s32	%p9, %r35, %r4;
	@%p9 bra 	BB19_2;

	cvta.to.global.u64 	%rd11, %rd5;
	mad.lo.s32 	%r31, %r1, 200, %r2;
	mad.lo.s32 	%r33, %r21, 40000, %r31;
	mul.wide.s32 	%rd12, %r33, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %r37;

BB19_6:
	ret;
}

.visible .entry _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1_(
	.param .u32 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_0,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_1,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_2,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_3,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_4,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_5,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_6,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_7,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_8,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_9,
	.param .u64 _Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_10
)
{
	.local .align 8 .b8 	__local_depot20[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<160>;
	.reg .f32 	%f<39>;
	.reg .s64 	%rd<95>;
	.reg .f64 	%fd<169>;


	mov.u64 	%SPL, __local_depot20;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r37, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_0];
	ld.param.u64 	%rd5, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_1];
	ld.param.u64 	%rd6, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_2];
	ld.param.u64 	%rd7, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_3];
	ld.param.u64 	%rd8, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_4];
	ld.param.u64 	%rd9, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_5];
	ld.param.u64 	%rd10, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_6];
	ld.param.u64 	%rd11, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_7];
	ld.param.u64 	%rd12, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_8];
	ld.param.u64 	%rd13, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_9];
	ld.param.u64 	%rd14, [_Z6SC_GPUiPbPiS_PfS1_S1_S1_S1_S1_S1__param_10];
	mov.u32 	%r38, %ctaid.x;
	shl.b32 	%r39, %r38, 10;
	mov.u32 	%r40, %tid.x;
	add.s32 	%r41, %r39, %r40;
	setp.eq.s32	%p3, %r41, 192644;
	setp.lt.s32	%p4, %r41, %r37;
	and.pred  	%p5, %p3, %p4;
	@!%p5 bra 	BB20_53;
	bra.uni 	BB20_1;

BB20_1:
	cvta.to.global.u64 	%rd15, %rd7;
	ld.global.u8 	%rs1, [%rd15+192644];
	setp.ne.s16	%p6, %rs1, 0;
	@%p6 bra 	BB20_53;

	cvta.to.global.u64 	%rd16, %rd8;
	ld.global.f32 	%f15, [%rd16+770576];
	setp.gtu.f32	%p7, %f15, 0f00000000;
	@%p7 bra 	BB20_53;

	cvta.to.global.u64 	%rd17, %rd14;
	cvta.to.global.u64 	%rd18, %rd11;
	cvta.to.global.u64 	%rd19, %rd13;
	cvta.to.global.u64 	%rd20, %rd10;
	cvta.to.global.u64 	%rd21, %rd12;
	cvta.to.global.u64 	%rd22, %rd9;
	ld.global.f32 	%f1, [%rd21+770576];
	ld.global.f32 	%f2, [%rd22+770576];
	setp.le.f32	%p8, %f2, %f1;
	selp.u32	%r42, 1, 0, %p8;
	setp.ge.f32	%p9, %f2, %f1;
	selp.u32	%r43, 1, 0, %p9;
	sub.s32 	%r1, %r42, %r43;
	ld.global.f32 	%f3, [%rd19+770576];
	ld.global.f32 	%f4, [%rd20+770576];
	setp.le.f32	%p10, %f4, %f3;
	selp.u32	%r44, 1, 0, %p10;
	setp.ge.f32	%p11, %f4, %f3;
	selp.u32	%r45, 1, 0, %p11;
	sub.s32 	%r2, %r44, %r45;
	ld.global.f32 	%f5, [%rd17+770576];
	ld.global.f32 	%f6, [%rd18+770576];
	setp.le.f32	%p12, %f6, %f5;
	selp.u32	%r46, 1, 0, %p12;
	setp.ge.f32	%p13, %f6, %f5;
	selp.u32	%r47, 1, 0, %p13;
	sub.s32 	%r3, %r46, %r47;
	cvt.f64.f32	%fd50, %f2;
	add.f64 	%fd51, %fd50, 0d4020000000000000;
	div.rn.f64 	%fd1, %fd51, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p14, %fd2, 0d7FF0000000000000;
	@%p14 bra 	BB20_7;

	setp.eq.f64	%p15, %fd2, 0d7FF0000000000000;
	@%p15 bra 	BB20_6;

	add.f64 	%fd134, %fd1, %fd1;
	mov.f64 	%fd135, %fd134;
	bra.uni 	BB20_8;

BB20_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r48}, %fd1;
	}
	and.b32  	%r49, %r48, -2147483648;
	mov.f64 	%fd52, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd52;
	}
	or.b32  	%r51, %r50, %r49;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd52;
	}
	mov.b64 	%fd134, {%r52, %r51};
	mov.f64 	%fd135, %fd1;
	bra.uni 	BB20_8;

BB20_7:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd53, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd53;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd1;
	}
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r54, %r56;
	mov.b64 	%fd134, {%r53, %r57};
	mov.f64 	%fd135, %fd5;

BB20_8:
	cvt.rzi.s32.f64	%r157, %fd135;
	mul.f64 	%fd54, %fd134, 0d3FB47AE147AE147B;
	mov.f64 	%fd55, 0d3FB47AE147AE147B;
	sub.f64 	%fd56, %fd55, %fd54;
	setp.gt.s32	%p16, %r1, 0;
	selp.f64	%fd57, %fd56, 0d0000000000000000, %p16;
	setp.lt.s32	%p17, %r1, 1;
	selp.f64	%fd58, %fd54, 0d0000000000000000, %p17;
	add.f64 	%fd165, %fd57, %fd58;
	neg.s32 	%r5, %r2;
	cvt.f64.f32	%fd59, %f4;
	mov.f64 	%fd60, 0d4020000000000000;
	sub.f64 	%fd61, %fd60, %fd59;
	div.rn.f64 	%fd10, %fd61, 0d3FB47AE147AE147B;
	abs.f64 	%fd11, %fd10;
	setp.lt.f64	%p18, %fd11, 0d7FF0000000000000;
	@%p18 bra 	BB20_12;

	setp.eq.f64	%p19, %fd11, 0d7FF0000000000000;
	@%p19 bra 	BB20_11;

	add.f64 	%fd137, %fd10, %fd10;
	mov.f64 	%fd138, %fd137;
	bra.uni 	BB20_13;

BB20_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd10;
	}
	and.b32  	%r59, %r58, -2147483648;
	mov.f64 	%fd62, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r60}, %fd62;
	}
	or.b32  	%r61, %r60, %r59;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r62, %temp}, %fd62;
	}
	mov.b64 	%fd137, {%r62, %r61};
	mov.f64 	%fd138, %fd10;
	bra.uni 	BB20_13;

BB20_12:
	cvt.rzi.f64.f64	%fd14, %fd10;
	sub.f64 	%fd63, %fd10, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r63, %temp}, %fd63;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r64}, %fd63;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd10;
	}
	and.b32  	%r66, %r65, -2147483648;
	or.b32  	%r67, %r64, %r66;
	mov.b64 	%fd137, {%r63, %r67};
	mov.f64 	%fd138, %fd14;

BB20_13:
	cvt.rzi.s32.f64	%r158, %fd138;
	shr.u32 	%r68, %r2, 31;
	cvt.rn.f64.s32	%fd64, %r68;
	mul.f64 	%fd65, %fd137, 0d3FB47AE147AE147B;
	sub.f64 	%fd67, %fd55, %fd65;
	setp.lt.s32	%p20, %r5, 1;
	selp.f64	%fd68, %fd65, 0d0000000000000000, %p20;
	fma.rn.f64 	%fd153, %fd64, %fd67, %fd68;
	cvt.f64.f32	%fd69, %f6;
	mov.f64 	%fd70, 0d4008000000000000;
	sub.f64 	%fd71, %fd70, %fd69;
	mul.f64 	%fd72, %fd71, 0d4010000000000000;
	cvt.rn.f32.f64	%f7, %fd72;
	abs.f32 	%f8, %f7;
	setp.lt.f32	%p21, %f8, 0f7F800000;
	@%p21 bra 	BB20_17;

	setp.eq.f32	%p22, %f8, 0f7F800000;
	@%p22 bra 	BB20_16;

	add.f32 	%f37, %f7, %f7;
	mov.f32 	%f38, %f37;
	bra.uni 	BB20_18;

BB20_16:
	mov.b32 	 %r69, %f7;
	and.b32  	%r70, %r69, -2147483648;
	mov.b32 	 %f37, %r70;
	mov.f32 	%f38, %f7;
	bra.uni 	BB20_18;

BB20_17:
	cvt.rzi.f32.f32	%f11, %f7;
	sub.f32 	%f16, %f7, %f11;
	mov.b32 	 %r71, %f16;
	mov.b32 	 %r72, %f7;
	and.b32  	%r73, %r72, -2147483648;
	or.b32  	%r74, %r71, %r73;
	mov.b32 	 %f37, %r74;
	mov.f32 	%f38, %f11;

BB20_18:
	shr.u32 	%r75, %r3, 31;
	cvt.rn.f64.s32	%fd73, %r75;
	mul.f32 	%f17, %f37, 0f3E800000;
	cvt.f64.f32	%fd74, %f17;
	mov.f64 	%fd75, 0d3FD0000000000000;
	sub.f64 	%fd76, %fd75, %fd74;
	neg.s32 	%r76, %r3;
	setp.lt.s32	%p23, %r76, 1;
	selp.f64	%fd77, %fd74, 0d0000000000000000, %p23;
	fma.rn.f64 	%fd143, %fd73, %fd76, %fd77;
	cvt.rzi.s32.f32	%r159, %f38;
	mad.lo.s32 	%r77, %r158, 200, %r157;
	cvt.f64.f32	%fd78, %f1;
	add.f64 	%fd79, %fd78, 0d4020000000000000;
	div.rn.f64 	%fd81, %fd79, 0d3FB47AE147AE147B;
	cvt.f64.f32	%fd82, %f3;
	sub.f64 	%fd83, %fd60, %fd82;
	div.rn.f64 	%fd84, %fd83, 0d3FB47AE147AE147B;
	cvt.f64.f32	%fd85, %f5;
	sub.f64 	%fd87, %fd70, %fd85;
	mul.f64 	%fd88, %fd87, 0d4010000000000000;
	cvt.rzi.s32.f64	%r78, %fd88;
	cvt.rzi.s32.f64	%r79, %fd84;
	cvt.rzi.s32.f64	%r80, %fd81;
	mad.lo.s32 	%r81, %r79, 200, %r80;
	sub.f32 	%f18, %f3, %f4;
	abs.f32 	%f19, %f18;
	sub.f32 	%f20, %f1, %f2;
	abs.f32 	%f21, %f20;
	div.rn.f32 	%f22, %f19, %f21;
	cvt.f64.f32	%fd20, %f22;
	sub.f32 	%f23, %f5, %f6;
	abs.f32 	%f24, %f23;
	div.rn.f32 	%f25, %f24, %f21;
	cvt.f64.f32	%fd21, %f25;
	div.rn.f32 	%f26, %f24, %f19;
	cvt.f64.f32	%fd22, %f26;
	cvta.to.global.u64 	%rd23, %rd6;
	add.u64 	%rd24, %SP, 0;
	cvta.to.local.u64 	%rd25, %rd24;
	cvta.global.u64 	%rd26, $str;
	ld.global.u32 	%r82, [%rd23+770576];
	mov.u32 	%r83, 756000;
	st.local.v2.u32 	[%rd25], {%r82, %r83};
	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd26;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r84, [retval0+0];
	}
	// Callseq End 24
	st.local.v2.u32 	[%rd25], {%r1, %r2};
	st.local.u32 	[%rd25+8], %r3;
	cvta.global.u64 	%rd27, $str1;
	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd27;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r85, [retval0+0];
	}
	// Callseq End 25
	mov.u64 	%rd28, 4590429028186199163;
	st.local.u64 	[%rd25], %rd28;
	st.local.u64 	[%rd25+8], %rd28;
	mov.u64 	%rd29, 4598175219545276416;
	st.local.u64 	[%rd25+16], %rd29;
	cvta.global.u64 	%rd30, $str2;
	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd30;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r86, [retval0+0];
	}
	// Callseq End 26
	st.local.v2.u32 	[%rd25], {%r157, %r158};
	st.local.u32 	[%rd25+8], %r159;
	cvta.global.u64 	%rd31, $str3;
	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd31;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r87, [retval0+0];
	}
	// Callseq End 27
	st.local.v2.u32 	[%rd25], {%r80, %r79};
	st.local.u32 	[%rd25+8], %r78;
	cvta.global.u64 	%rd32, $str4;
	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd32;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r88, [retval0+0];
	}
	// Callseq End 28
	cvta.global.u64 	%rd33, $str5;
	mad.lo.s32 	%r8, %r78, 40000, %r81;
	mad.lo.s32 	%r89, %r159, 40000, %r77;
	st.local.v2.u32 	[%rd25], {%r89, %r8};
	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd33;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r90, [retval0+0];
	}
	// Callseq End 29
	ld.global.f32 	%f27, [%rd18+770576];
	ld.global.f32 	%f28, [%rd20+770576];
	ld.global.f32 	%f29, [%rd22+770576];
	cvt.f64.f32	%fd89, %f29;
	st.local.f64 	[%rd25], %fd89;
	cvt.f64.f32	%fd90, %f28;
	st.local.f64 	[%rd25+8], %fd90;
	cvt.f64.f32	%fd91, %f27;
	st.local.f64 	[%rd25+16], %fd91;
	cvta.global.u64 	%rd37, $str6;
	// Callseq Start 30
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd37;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r91, [retval0+0];
	}
	// Callseq End 30
	ld.global.f32 	%f30, [%rd17+770576];
	ld.global.f32 	%f31, [%rd19+770576];
	st.local.f64 	[%rd25], %fd78;
	cvt.f64.f32	%fd92, %f31;
	st.local.f64 	[%rd25+8], %fd92;
	cvt.f64.f32	%fd93, %f30;
	st.local.f64 	[%rd25+16], %fd93;
	cvta.global.u64 	%rd40, $str7;
	// Callseq Start 31
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r92, [retval0+0];
	}
	// Callseq End 31
	setp.gt.f64	%p24, %fd78, 0dC01D1EB851EB851F;
	selp.u32	%r93, 1, 0, %p24;
	cvta.global.u64 	%rd41, $str8;
	st.local.u32 	[%rd25], %r93;
	// Callseq Start 32
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r94, [retval0+0];
	}
	// Callseq End 32
	setp.lt.f64	%p25, %fd78, 0dC01D1EB851EB851F;
	selp.u32	%r95, 1, 0, %p25;
	cvta.global.u64 	%rd42, $str9;
	st.local.u32 	[%rd25], %r95;
	// Callseq Start 33
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd42;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r96, [retval0+0];
	}
	// Callseq End 33
	setp.eq.f64	%p26, %fd78, 0dC01D1EB851EB851F;
	selp.u32	%r97, 1, 0, %p26;
	cvta.global.u64 	%rd43, $str10;
	st.local.u32 	[%rd25], %r97;
	// Callseq Start 34
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd43;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r98, [retval0+0];
	}
	// Callseq End 34
	ld.global.f32 	%f32, [%rd21+770576];
	cvt.f64.f32	%fd94, %f32;
	setp.gt.f64	%p27, %fd94, 0dC01D1EB851EB851F;
	selp.u32	%r99, 1, 0, %p27;
	st.local.u32 	[%rd25], %r99;
	cvta.global.u64 	%rd45, $str11;
	// Callseq Start 35
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd45;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r100, [retval0+0];
	}
	// Callseq End 35
	ld.global.f32 	%f33, [%rd21+770576];
	cvt.f64.f32	%fd95, %f33;
	setp.lt.f64	%p28, %fd95, 0dC01D1EB851EB851F;
	selp.u32	%r101, 1, 0, %p28;
	st.local.u32 	[%rd25], %r101;
	cvta.global.u64 	%rd46, $str12;
	// Callseq Start 36
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r102, [retval0+0];
	}
	// Callseq End 36
	ld.global.f32 	%f34, [%rd21+770576];
	cvt.f64.f32	%fd96, %f34;
	setp.eq.f64	%p29, %fd96, 0dC01D1EB851EB851F;
	selp.u32	%r103, 1, 0, %p29;
	st.local.u32 	[%rd25], %r103;
	cvta.global.u64 	%rd47, $str13;
	// Callseq Start 37
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd47;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r104, [retval0+0];
	}
	// Callseq End 37
	ld.global.f32 	%f35, [%rd21+770576];
	cvt.f64.f32	%fd97, %f35;
	add.f64 	%fd98, %fd97, 0d4020000000000000;
	div.rn.f64 	%fd99, %fd98, 0d3FB47AE147AE147B;
	st.local.f64 	[%rd25], %fd99;
	cvta.global.u64 	%rd48, $str14;
	// Callseq Start 38
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd48;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r105, [retval0+0];
	}
	// Callseq End 38
	st.local.f64 	[%rd25], %fd20;
	st.local.f64 	[%rd25+8], %fd21;
	st.local.f64 	[%rd25+16], %fd22;
	cvta.global.u64 	%rd49, $str15;
	// Callseq Start 39
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd49;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r106, [retval0+0];
	}
	// Callseq End 39
	st.local.f64 	[%rd25], %fd165;
	st.local.f64 	[%rd25+8], %fd153;
	st.local.f64 	[%rd25+16], %fd143;
	cvta.global.u64 	%rd50, $str16;
	// Callseq Start 40
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd50;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r107, [retval0+0];
	}
	// Callseq End 40
	setp.ne.s32	%p30, %r89, %r8;
	setp.lt.s32	%p31, %r157, 200;
	and.pred  	%p32, %p30, %p31;
	setp.lt.s32	%p33, %r158, 200;
	and.pred  	%p34, %p32, %p33;
	setp.lt.s32	%p35, %r159, 24;
	and.pred  	%p36, %p35, %p34;
	@!%p36 bra 	BB20_20;
	bra.uni 	BB20_19;

BB20_19:
	cvta.to.global.u64 	%rd51, %rd5;
	cvt.s64.s32	%rd52, %r89;
	add.s64 	%rd53, %rd51, %rd52;
	mov.u16 	%rs2, 0;
	st.global.u8 	[%rd53], %rs2;

BB20_20:
	xor.pred  	%p39, %p12, %p13;
	@%p39 bra 	BB20_35;

	@!%p36 bra 	BB20_53;
	bra.uni 	BB20_22;

BB20_22:
	cvta.global.u64 	%rd1, $str28;
	cvta.global.u64 	%rd2, $str26;
	cvta.global.u64 	%rd3, $str27;
	cvta.global.u64 	%rd4, $str25;
	mov.u32 	%r155, 0;
	mov.f64 	%fd155, %fd153;
	mov.f64 	%fd167, %fd165;

BB20_23:
	div.rn.f64 	%fd25, %fd155, %fd20;
	setp.gtu.f64	%p47, %fd167, %fd25;
	@%p47 bra 	BB20_27;

	add.s32 	%r113, %r155, -186;
	setp.lt.u32	%p48, %r113, 64;
	abs.s32 	%r114, %r2;
	cvt.rn.f64.s32	%fd100, %r114;
	mul.f64 	%fd26, %fd20, %fd167;
	mul.f64 	%fd101, %fd26, %fd100;
	sub.f64 	%fd102, %fd155, %fd101;
	add.s32 	%r157, %r157, %r1;
	setp.gtu.f64	%p49, %fd102, 0d3E7AD7F29ABCAF48;
	selp.b32	%r115, 0, %r2, %p49;
	sub.s32 	%r158, %r158, %r115;
	selp.f64	%fd156, %fd102, 0d3FB47AE147AE147B, %p49;
	@%p48 bra 	BB20_26;

	mov.f64 	%fd168, %fd55;
	bra.uni 	BB20_30;

BB20_26:
	mov.u64 	%rd55, 0;
	// Callseq Start 41
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd4;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd55;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r116, [retval0+0];
	}
	// Callseq End 41
	st.local.f64 	[%rd25], %fd167;
	st.local.f64 	[%rd25+8], %fd26;
	// Callseq Start 42
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r117, [retval0+0];
	}
	// Callseq End 42
	st.local.u64 	[%rd25], %rd28;
	st.local.f64 	[%rd25+8], %fd156;
	// Callseq Start 43
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r118, [retval0+0];
	}
	// Callseq End 43
	mov.f64 	%fd158, %fd55;
	mov.f64 	%fd168, %fd158;
	bra.uni 	BB20_30;

BB20_27:
	add.s32 	%r119, %r155, -186;
	setp.lt.u32	%p50, %r119, 64;
	abs.s32 	%r120, %r1;
	cvt.rn.f64.s32	%fd105, %r120;
	mul.f64 	%fd106, %fd25, %fd105;
	sub.f64 	%fd28, %fd167, %fd106;
	sub.s32 	%r158, %r158, %r2;
	mov.f64 	%fd156, 0d3FB47AE147AE147B;
	@%p50 bra 	BB20_29;

	mov.f64 	%fd159, %fd28;
	mov.f64 	%fd168, %fd159;
	bra.uni 	BB20_30;

BB20_29:
	mov.u64 	%rd59, 0;
	// Callseq Start 44
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd1;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd59;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r121, [retval0+0];
	}
	// Callseq End 44
	st.local.f64 	[%rd25], %fd25;
	st.local.f64 	[%rd25+8], %fd155;
	// Callseq Start 45
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r122, [retval0+0];
	}
	// Callseq End 45
	st.local.f64 	[%rd25], %fd28;
	st.local.u64 	[%rd25+8], %rd28;
	// Callseq Start 46
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd3;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r123, [retval0+0];
	}
	// Callseq End 46
	mov.f64 	%fd168, %fd28;

BB20_30:
	mov.f64 	%fd160, %fd168;
	mov.f64 	%fd167, %fd160;
	mov.f64 	%fd155, %fd156;
	add.s32 	%r124, %r155, -186;
	mov.u32 	%r125, 0;
	max.s32 	%r159, %r159, %r125;
	mad.lo.s32 	%r126, %r158, 200, %r157;
	mad.lo.s32 	%r19, %r159, 40000, %r126;
	setp.gt.u32	%p51, %r124, 63;
	@%p51 bra 	BB20_32;

	st.local.v2.u32 	[%rd25], {%r157, %r158};
	st.local.u32 	[%rd25+8], %r159;
	// Callseq Start 47
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd31;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r127, [retval0+0];
	}
	// Callseq End 47

BB20_32:
	setp.ne.s32	%p52, %r19, %r8;
	setp.lt.s32	%p53, %r157, 200;
	and.pred  	%p54, %p52, %p53;
	setp.lt.s32	%p55, %r158, 200;
	and.pred  	%p56, %p54, %p55;
	setp.lt.s32	%p57, %r159, 24;
	and.pred  	%p1, %p57, %p56;
	@!%p1 bra 	BB20_34;
	bra.uni 	BB20_33;

BB20_33:
	cvta.to.global.u64 	%rd65, %rd5;
	cvt.s64.s32	%rd66, %r19;
	add.s64 	%rd67, %rd65, %rd66;
	mov.u16 	%rs3, 0;
	st.global.u8 	[%rd67], %rs3;

BB20_34:
	add.s32 	%r155, %r155, 1;
	@%p1 bra 	BB20_23;
	bra.uni 	BB20_53;

BB20_35:
	@!%p36 bra 	BB20_53;
	bra.uni 	BB20_36;

BB20_36:
	mov.u32 	%r156, 0;

BB20_37:
	mul.f64 	%fd34, %fd22, %fd153;
	setp.le.f64	%p65, %fd143, %fd34;
	mul.f64 	%fd35, %fd21, %fd165;
	setp.le.f64	%p66, %fd143, %fd35;
	and.pred  	%p67, %p66, %p65;
	@%p67 bra 	BB20_45;

	setp.gtu.f64	%p68, %fd35, %fd34;
	abs.s32 	%r131, %r3;
	cvt.rn.f64.s32	%fd36, %r131;
	@%p68 bra 	BB20_42;

	add.s32 	%r132, %r156, -186;
	setp.lt.u32	%p69, %r132, 64;
	abs.s32 	%r133, %r2;
	cvt.rn.f64.s32	%fd109, %r133;
	mul.f64 	%fd37, %fd20, %fd165;
	mul.f64 	%fd110, %fd37, %fd109;
	sub.f64 	%fd111, %fd153, %fd110;
	mul.f64 	%fd112, %fd35, %fd36;
	sub.f64 	%fd38, %fd143, %fd112;
	add.s32 	%r157, %r157, %r1;
	setp.gtu.f64	%p70, %fd111, 0d3E7AD7F29ABCAF48;
	selp.b32	%r134, 0, %r2, %p70;
	sub.s32 	%r158, %r158, %r134;
	selp.f64	%fd154, %fd111, 0d3FB47AE147AE147B, %p70;
	mov.f64 	%fd166, 0d3FB47AE147AE147B;
	@%p69 bra 	BB20_41;

	mov.f64 	%fd139, %fd38;
	mov.f64 	%fd144, %fd139;
	bra.uni 	BB20_48;

BB20_41:
	cvta.global.u64 	%rd69, $str21;
	mov.u64 	%rd70, 0;
	// Callseq Start 48
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd69;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd70;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r135, [retval0+0];
	}
	// Callseq End 48
	st.local.f64 	[%rd25], %fd165;
	st.local.f64 	[%rd25+8], %fd37;
	st.local.f64 	[%rd25+16], %fd35;
	cvta.global.u64 	%rd72, $str19;
	// Callseq Start 49
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd72;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r136, [retval0+0];
	}
	// Callseq End 49
	st.local.u64 	[%rd25], %rd28;
	st.local.f64 	[%rd25+8], %fd154;
	st.local.f64 	[%rd25+16], %fd38;
	// Callseq Start 50
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd50;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r137, [retval0+0];
	}
	// Callseq End 50
	mov.f64 	%fd144, %fd38;
	bra.uni 	BB20_48;

BB20_42:
	add.s32 	%r138, %r156, -186;
	setp.lt.u32	%p71, %r138, 64;
	abs.s32 	%r139, %r1;
	cvt.rn.f64.s32	%fd115, %r139;
	div.rn.f64 	%fd40, %fd153, %fd20;
	mul.f64 	%fd116, %fd40, %fd115;
	sub.f64 	%fd166, %fd165, %fd116;
	mul.f64 	%fd117, %fd34, %fd36;
	sub.f64 	%fd42, %fd143, %fd117;
	sub.s32 	%r158, %r158, %r2;
	mov.f64 	%fd154, 0d3FB47AE147AE147B;
	@%p71 bra 	BB20_44;

	mov.f64 	%fd140, %fd42;
	mov.f64 	%fd144, %fd140;
	bra.uni 	BB20_48;

BB20_44:
	cvta.global.u64 	%rd76, $str22;
	mov.u64 	%rd77, 0;
	// Callseq Start 51
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd76;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd77;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r140, [retval0+0];
	}
	// Callseq End 51
	st.local.f64 	[%rd25], %fd40;
	st.local.f64 	[%rd25+8], %fd153;
	st.local.f64 	[%rd25+16], %fd34;
	cvta.global.u64 	%rd79, $str19;
	// Callseq Start 52
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd79;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r141, [retval0+0];
	}
	// Callseq End 52
	st.local.f64 	[%rd25], %fd166;
	st.local.u64 	[%rd25+8], %rd28;
	st.local.f64 	[%rd25+16], %fd42;
	// Callseq Start 53
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd50;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r142, [retval0+0];
	}
	// Callseq End 53
	mov.f64 	%fd144, %fd42;
	bra.uni 	BB20_48;

BB20_45:
	add.s32 	%r143, %r156, -186;
	setp.lt.u32	%p72, %r143, 64;
	abs.s32 	%r144, %r1;
	cvt.rn.f64.s32	%fd120, %r144;
	div.rn.f64 	%fd43, %fd143, %fd21;
	mul.f64 	%fd121, %fd43, %fd120;
	sub.f64 	%fd122, %fd165, %fd121;
	abs.s32 	%r145, %r2;
	cvt.rn.f64.s32	%fd123, %r145;
	div.rn.f64 	%fd44, %fd143, %fd22;
	mul.f64 	%fd124, %fd44, %fd123;
	sub.f64 	%fd125, %fd153, %fd124;
	sub.s32 	%r159, %r159, %r3;
	setp.gtu.f64	%p73, %fd122, 0d3E7AD7F29ABCAF48;
	selp.b32	%r146, 0, %r1, %p73;
	add.s32 	%r157, %r146, %r157;
	selp.f64	%fd166, %fd122, 0d3FB47AE147AE147B, %p73;
	setp.gtu.f64	%p74, %fd125, 0d3E7AD7F29ABCAF48;
	selp.b32	%r147, 0, %r2, %p74;
	sub.s32 	%r158, %r158, %r147;
	selp.f64	%fd154, %fd125, 0d3FB47AE147AE147B, %p74;
	@%p72 bra 	BB20_47;

	mov.f64 	%fd144, %fd75;
	bra.uni 	BB20_48;

BB20_47:
	cvta.global.u64 	%rd83, $str18;
	mov.u64 	%rd84, 0;
	// Callseq Start 54
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd83;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd84;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r148, [retval0+0];
	}
	// Callseq End 54
	st.local.f64 	[%rd25], %fd43;
	st.local.f64 	[%rd25+8], %fd44;
	st.local.f64 	[%rd25+16], %fd143;
	cvta.global.u64 	%rd86, $str19;
	// Callseq Start 55
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r149, [retval0+0];
	}
	// Callseq End 55
	st.local.f64 	[%rd25], %fd166;
	st.local.f64 	[%rd25+8], %fd154;
	st.local.u64 	[%rd25+16], %rd29;
	cvta.global.u64 	%rd88, $str20;
	// Callseq Start 56
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd88;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r150, [retval0+0];
	}
	// Callseq End 56
	mov.f64 	%fd141, %fd75;
	mov.f64 	%fd144, %fd141;

BB20_48:
	mov.f64 	%fd165, %fd166;
	mov.f64 	%fd153, %fd154;
	mov.f64 	%fd142, %fd144;
	mov.f64 	%fd143, %fd142;
	add.s32 	%r151, %r156, -186;
	mov.u32 	%r152, 0;
	max.s32 	%r159, %r159, %r152;
	mad.lo.s32 	%r153, %r158, 200, %r157;
	mad.lo.s32 	%r35, %r159, 40000, %r153;
	setp.gt.u32	%p75, %r151, 63;
	@%p75 bra 	BB20_50;

	st.local.v2.u32 	[%rd25], {%r157, %r158};
	st.local.v2.u32 	[%rd25+8], {%r159, %r35};
	cvta.global.u64 	%rd91, $str23;
	// Callseq Start 57
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r154, [retval0+0];
	}
	// Callseq End 57

BB20_50:
	setp.ne.s32	%p76, %r35, %r8;
	setp.lt.s32	%p77, %r157, 200;
	and.pred  	%p78, %p76, %p77;
	setp.lt.s32	%p79, %r158, 200;
	and.pred  	%p80, %p78, %p79;
	setp.lt.s32	%p81, %r159, 24;
	and.pred  	%p2, %p81, %p80;
	@!%p2 bra 	BB20_52;
	bra.uni 	BB20_51;

BB20_51:
	cvta.to.global.u64 	%rd92, %rd5;
	cvt.s64.s32	%rd93, %r35;
	add.s64 	%rd94, %rd92, %rd93;
	mov.u16 	%rs4, 0;
	st.global.u8 	[%rd94], %rs4;

BB20_52:
	add.s32 	%r156, %r156, 1;
	@%p2 bra 	BB20_37;

BB20_53:
	ret;
}

.visible .entry _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_(
	.param .u32 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_0,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_1,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_2,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_3,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_4,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_5,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_6,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_7,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_8,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_9,
	.param .u64 _Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_10
)
{
	.reg .pred 	%p<51>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<102>;
	.reg .f32 	%f<30>;
	.reg .s64 	%rd<38>;
	.reg .f64 	%fd<133>;


	ld.param.u32 	%r35, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_0];
	ld.param.u64 	%rd2, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_1];
	ld.param.u64 	%rd3, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_3];
	ld.param.u64 	%rd4, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_4];
	ld.param.u64 	%rd5, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_5];
	ld.param.u64 	%rd6, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_6];
	ld.param.u64 	%rd7, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_7];
	ld.param.u64 	%rd8, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_8];
	ld.param.u64 	%rd9, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_9];
	ld.param.u64 	%rd10, [_Z7MSC_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_10];
	mov.u32 	%r36, %ctaid.x;
	shl.b32 	%r37, %r36, 10;
	mov.u32 	%r38, %tid.x;
	add.s32 	%r1, %r37, %r38;
	setp.ge.s32	%p4, %r1, %r35;
	@%p4 bra 	BB21_38;

	cvta.to.global.u64 	%rd11, %rd3;
	cvt.s64.s32	%rd1, %r1;
	add.s64 	%rd12, %rd11, %rd1;
	ld.global.u8 	%rs1, [%rd12];
	setp.ne.s16	%p5, %rs1, 0;
	@%p5 bra 	BB21_38;

	cvta.to.global.u64 	%rd13, %rd4;
	shl.b64 	%rd14, %rd1, 2;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f15, [%rd15];
	setp.gtu.f32	%p6, %f15, 0f00000000;
	@%p6 bra 	BB21_38;

	cvta.to.global.u64 	%rd16, %rd10;
	cvta.to.global.u64 	%rd17, %rd7;
	cvta.to.global.u64 	%rd18, %rd9;
	cvta.to.global.u64 	%rd19, %rd6;
	cvta.to.global.u64 	%rd20, %rd8;
	cvta.to.global.u64 	%rd21, %rd5;
	add.s64 	%rd23, %rd21, %rd14;
	add.s64 	%rd24, %rd20, %rd14;
	ld.global.f32 	%f1, [%rd24];
	ld.global.f32 	%f2, [%rd23];
	setp.le.f32	%p7, %f2, %f1;
	selp.u32	%r39, 1, 0, %p7;
	setp.ge.f32	%p8, %f2, %f1;
	selp.u32	%r40, 1, 0, %p8;
	sub.s32 	%r2, %r39, %r40;
	add.s64 	%rd25, %rd18, %rd14;
	ld.global.f32 	%f3, [%rd25];
	add.s64 	%rd26, %rd19, %rd14;
	ld.global.f32 	%f4, [%rd26];
	setp.le.f32	%p9, %f4, %f3;
	selp.u32	%r41, 1, 0, %p9;
	setp.ge.f32	%p10, %f4, %f3;
	selp.u32	%r42, 1, 0, %p10;
	sub.s32 	%r3, %r41, %r42;
	add.s64 	%rd27, %rd16, %rd14;
	ld.global.f32 	%f5, [%rd27];
	add.s64 	%rd28, %rd17, %rd14;
	ld.global.f32 	%f6, [%rd28];
	setp.le.f32	%p11, %f6, %f5;
	selp.u32	%r43, 1, 0, %p11;
	setp.ge.f32	%p12, %f6, %f5;
	selp.u32	%r44, 1, 0, %p12;
	sub.s32 	%r4, %r43, %r44;
	cvt.f64.f32	%fd47, %f2;
	add.f64 	%fd48, %fd47, 0d4020000000000000;
	div.rn.f64 	%fd1, %fd48, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p13, %fd2, 0d7FF0000000000000;
	@%p13 bra 	BB21_7;

	setp.eq.f64	%p14, %fd2, 0d7FF0000000000000;
	@%p14 bra 	BB21_6;

	add.f64 	%fd114, %fd1, %fd1;
	mov.f64 	%fd115, %fd114;
	bra.uni 	BB21_8;

BB21_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd1;
	}
	and.b32  	%r46, %r45, -2147483648;
	mov.f64 	%fd49, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd49;
	}
	or.b32  	%r48, %r47, %r46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd49;
	}
	mov.b64 	%fd114, {%r49, %r48};
	mov.f64 	%fd115, %fd1;
	bra.uni 	BB21_8;

BB21_7:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd50, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd50;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd50;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd1;
	}
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r51, %r53;
	mov.b64 	%fd114, {%r50, %r54};
	mov.f64 	%fd115, %fd5;

BB21_8:
	cvt.rzi.s32.f64	%r96, %fd115;
	mul.f64 	%fd51, %fd114, 0d3FB47AE147AE147B;
	mov.f64 	%fd52, 0d3FB47AE147AE147B;
	sub.f64 	%fd53, %fd52, %fd51;
	setp.gt.s32	%p15, %r2, 0;
	selp.f64	%fd54, %fd53, 0d0000000000000000, %p15;
	setp.lt.s32	%p16, %r2, 1;
	selp.f64	%fd55, %fd51, 0d0000000000000000, %p16;
	add.f64 	%fd129, %fd54, %fd55;
	neg.s32 	%r6, %r3;
	cvt.f64.f32	%fd56, %f4;
	mov.f64 	%fd57, 0d4020000000000000;
	sub.f64 	%fd58, %fd57, %fd56;
	div.rn.f64 	%fd10, %fd58, 0d3FB47AE147AE147B;
	abs.f64 	%fd11, %fd10;
	setp.lt.f64	%p17, %fd11, 0d7FF0000000000000;
	@%p17 bra 	BB21_12;

	setp.eq.f64	%p18, %fd11, 0d7FF0000000000000;
	@%p18 bra 	BB21_11;

	add.f64 	%fd117, %fd10, %fd10;
	mov.f64 	%fd118, %fd117;
	bra.uni 	BB21_13;

BB21_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd10;
	}
	and.b32  	%r56, %r55, -2147483648;
	mov.f64 	%fd59, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd59;
	}
	or.b32  	%r58, %r57, %r56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd59;
	}
	mov.b64 	%fd117, {%r59, %r58};
	mov.f64 	%fd118, %fd10;
	bra.uni 	BB21_13;

BB21_12:
	cvt.rzi.f64.f64	%fd14, %fd10;
	sub.f64 	%fd60, %fd10, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r60, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd10;
	}
	and.b32  	%r63, %r62, -2147483648;
	or.b32  	%r64, %r61, %r63;
	mov.b64 	%fd117, {%r60, %r64};
	mov.f64 	%fd118, %fd14;

BB21_13:
	cvt.rzi.s32.f64	%r94, %fd118;
	shr.u32 	%r65, %r3, 31;
	cvt.rn.f64.s32	%fd61, %r65;
	mul.f64 	%fd62, %fd117, 0d3FB47AE147AE147B;
	sub.f64 	%fd64, %fd52, %fd62;
	setp.lt.s32	%p19, %r6, 1;
	selp.f64	%fd65, %fd62, 0d0000000000000000, %p19;
	fma.rn.f64 	%fd122, %fd61, %fd64, %fd65;
	neg.s32 	%r8, %r4;
	cvt.f64.f32	%fd66, %f6;
	mov.f64 	%fd67, 0d4008000000000000;
	sub.f64 	%fd68, %fd67, %fd66;
	mul.f64 	%fd69, %fd68, 0d4010000000000000;
	cvt.rn.f32.f64	%f7, %fd69;
	abs.f32 	%f8, %f7;
	setp.lt.f32	%p20, %f8, 0f7F800000;
	@%p20 bra 	BB21_17;

	setp.eq.f32	%p21, %f8, 0f7F800000;
	@%p21 bra 	BB21_16;

	add.f32 	%f28, %f7, %f7;
	mov.f32 	%f29, %f28;
	bra.uni 	BB21_18;

BB21_16:
	mov.b32 	 %r66, %f7;
	and.b32  	%r67, %r66, -2147483648;
	mov.b32 	 %f28, %r67;
	mov.f32 	%f29, %f7;
	bra.uni 	BB21_18;

BB21_17:
	cvt.rzi.f32.f32	%f11, %f7;
	sub.f32 	%f16, %f7, %f11;
	mov.b32 	 %r68, %f16;
	mov.b32 	 %r69, %f7;
	and.b32  	%r70, %r69, -2147483648;
	or.b32  	%r71, %r68, %r70;
	mov.b32 	 %f28, %r71;
	mov.f32 	%f29, %f11;

BB21_18:
	shr.u32 	%r72, %r4, 31;
	cvt.rn.f64.s32	%fd70, %r72;
	mul.f32 	%f17, %f28, 0f3E800000;
	cvt.f64.f32	%fd71, %f17;
	mov.f64 	%fd72, 0d3FD0000000000000;
	sub.f64 	%fd73, %fd72, %fd71;
	setp.lt.s32	%p22, %r8, 1;
	selp.f64	%fd74, %fd71, 0d0000000000000000, %p22;
	fma.rn.f64 	%fd120, %fd70, %fd73, %fd74;
	cvt.rzi.s32.f32	%r95, %f29;
	mul.lo.s32 	%r10, %r95, 40000;
	mad.lo.s32 	%r73, %r94, 200, %r96;
	add.s32 	%r74, %r73, %r10;
	cvt.f64.f32	%fd75, %f1;
	add.f64 	%fd76, %fd75, 0d4020000000000000;
	div.rn.f64 	%fd78, %fd76, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r75, %fd78;
	cvt.f64.f32	%fd79, %f3;
	sub.f64 	%fd80, %fd57, %fd79;
	div.rn.f64 	%fd81, %fd80, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r76, %fd81;
	cvt.f64.f32	%fd82, %f5;
	sub.f64 	%fd84, %fd67, %fd82;
	mul.f64 	%fd85, %fd84, 0d4010000000000000;
	cvt.rzi.s32.f64	%r77, %fd85;
	mad.lo.s32 	%r78, %r76, 200, %r75;
	mad.lo.s32 	%r11, %r77, 40000, %r78;
	sub.f32 	%f18, %f3, %f4;
	abs.f32 	%f19, %f18;
	sub.f32 	%f20, %f1, %f2;
	abs.f32 	%f21, %f20;
	div.rn.f32 	%f22, %f19, %f21;
	cvt.f64.f32	%fd20, %f22;
	sub.f32 	%f23, %f5, %f6;
	abs.f32 	%f24, %f23;
	div.rn.f32 	%f25, %f24, %f21;
	cvt.f64.f32	%fd21, %f25;
	div.rn.f32 	%f26, %f24, %f19;
	cvt.f64.f32	%fd22, %f26;
	setp.ne.s32	%p23, %r74, %r11;
	setp.lt.s32	%p24, %r96, 200;
	and.pred  	%p25, %p23, %p24;
	setp.lt.s32	%p26, %r94, 200;
	and.pred  	%p27, %p25, %p26;
	setp.lt.s32	%p28, %r95, 24;
	and.pred  	%p1, %p28, %p27;
	@!%p1 bra 	BB21_20;
	bra.uni 	BB21_19;

BB21_19:
	cvta.to.global.u64 	%rd29, %rd2;
	mul.wide.s32 	%rd30, %r74, 4;
	add.s64 	%rd31, %rd29, %rd30;
	atom.global.add.u32 	%r81, [%rd31], 1;

BB21_20:
	xor.pred  	%p31, %p11, %p12;
	@%p31 bra 	BB21_28;

	@!%p1 bra 	BB21_38;
	bra.uni 	BB21_22;

BB21_22:
	div.rn.f64 	%fd25, %fd122, %fd20;
	setp.gtu.f64	%p32, %fd129, %fd25;
	@%p32 bra 	BB21_24;

	mul.f64 	%fd87, %fd20, %fd129;
	abs.s32 	%r82, %r3;
	cvt.rn.f64.s32	%fd88, %r82;
	mul.f64 	%fd89, %fd87, %fd88;
	sub.f64 	%fd90, %fd122, %fd89;
	add.s32 	%r96, %r96, %r2;
	setp.eq.f64	%p33, %fd90, 0d0000000000000000;
	selp.b32	%r101, %r3, 0, %p33;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd90, %p33;
	mov.f64 	%fd130, %fd52;
	bra.uni 	BB21_25;

BB21_24:
	abs.s32 	%r83, %r2;
	cvt.rn.f64.s32	%fd92, %r83;
	mul.f64 	%fd93, %fd25, %fd92;
	sub.f64 	%fd27, %fd129, %fd93;
	mov.f64 	%fd122, 0d3FB47AE147AE147B;
	mov.f64 	%fd130, %fd27;
	mov.u32 	%r101, %r3;

BB21_25:
	mov.f64 	%fd124, %fd130;
	mov.f64 	%fd129, %fd124;
	sub.s32 	%r94, %r94, %r101;
	add.s32 	%r84, %r96, %r10;
	mad.lo.s32 	%r19, %r94, 200, %r84;
	setp.eq.s32	%p34, %r19, %r11;
	setp.gt.s32	%p35, %r96, 199;
	or.pred  	%p36, %p34, %p35;
	setp.gt.s32	%p37, %r94, 199;
	or.pred  	%p2, %p36, %p37;
	@%p2 bra 	BB21_27;

	cvta.to.global.u64 	%rd32, %rd2;
	mul.wide.s32 	%rd33, %r19, 4;
	add.s64 	%rd34, %rd32, %rd33;
	atom.global.add.u32 	%r85, [%rd34], 1;

BB21_27:
	@%p2 bra 	BB21_38;
	bra.uni 	BB21_22;

BB21_28:
	mov.f64 	%fd127, %fd129;
	@!%p1 bra 	BB21_38;
	bra.uni 	BB21_29;

BB21_29:
	mul.f64 	%fd33, %fd22, %fd122;
	setp.le.f64	%p38, %fd120, %fd33;
	mul.f64 	%fd34, %fd21, %fd127;
	setp.le.f64	%p39, %fd120, %fd34;
	and.pred  	%p40, %p39, %p38;
	@%p40 bra 	BB21_34;

	setp.gtu.f64	%p41, %fd34, %fd33;
	abs.s32 	%r86, %r4;
	cvt.rn.f64.s32	%fd35, %r86;
	@%p41 bra 	BB21_32;

	mul.f64 	%fd95, %fd20, %fd127;
	abs.s32 	%r87, %r3;
	cvt.rn.f64.s32	%fd96, %r87;
	mul.f64 	%fd97, %fd95, %fd96;
	sub.f64 	%fd98, %fd122, %fd97;
	add.s32 	%r96, %r96, %r2;
	setp.eq.f64	%p42, %fd98, 0d0000000000000000;
	selp.b32	%r24, %r3, 0, %p42;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd98, %p42;
	mov.f64 	%fd131, 0d3FB47AE147AE147B;
	mov.f64 	%fd132, %fd34;
	mov.u32 	%r100, %r24;
	bra.uni 	BB21_33;

BB21_32:
	div.rn.f64 	%fd100, %fd122, %fd20;
	abs.s32 	%r88, %r2;
	cvt.rn.f64.s32	%fd101, %r88;
	mul.f64 	%fd102, %fd100, %fd101;
	sub.f64 	%fd131, %fd127, %fd102;
	mov.f64 	%fd122, 0d3FB47AE147AE147B;
	mov.f64 	%fd132, %fd33;
	mov.u32 	%r100, %r3;

BB21_33:
	mov.u32 	%r98, %r100;
	mov.u32 	%r99, %r98;
	mov.f64 	%fd128, %fd131;
	mul.f64 	%fd103, %fd132, %fd35;
	sub.f64 	%fd41, %fd120, %fd103;
	mov.f64 	%fd121, %fd41;
	bra.uni 	BB21_35;

BB21_34:
	div.rn.f64 	%fd105, %fd120, %fd21;
	abs.s32 	%r89, %r2;
	cvt.rn.f64.s32	%fd106, %r89;
	mul.f64 	%fd107, %fd105, %fd106;
	sub.f64 	%fd108, %fd127, %fd107;
	abs.s32 	%r90, %r3;
	cvt.rn.f64.s32	%fd109, %r90;
	div.rn.f64 	%fd110, %fd120, %fd22;
	mul.f64 	%fd111, %fd110, %fd109;
	sub.f64 	%fd112, %fd122, %fd111;
	sub.s32 	%r95, %r95, %r4;
	setp.eq.f64	%p43, %fd108, 0d0000000000000000;
	selp.b32	%r91, %r2, 0, %p43;
	add.s32 	%r96, %r91, %r96;
	selp.f64	%fd128, 0d3FB47AE147AE147B, %fd108, %p43;
	setp.eq.f64	%p44, %fd112, 0d0000000000000000;
	selp.b32	%r99, %r3, 0, %p44;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd112, %p44;
	mov.f64 	%fd121, %fd72;

BB21_35:
	mov.f64 	%fd127, %fd128;
	mov.f64 	%fd119, %fd121;
	mov.f64 	%fd120, %fd119;
	sub.s32 	%r94, %r94, %r99;
	mad.lo.s32 	%r92, %r95, 40000, %r96;
	mad.lo.s32 	%r34, %r94, 200, %r92;
	setp.ne.s32	%p45, %r34, %r11;
	setp.lt.s32	%p46, %r96, 200;
	and.pred  	%p47, %p45, %p46;
	setp.lt.s32	%p48, %r94, 200;
	and.pred  	%p49, %p47, %p48;
	setp.lt.s32	%p50, %r95, 24;
	and.pred  	%p3, %p50, %p49;
	@!%p3 bra 	BB21_37;
	bra.uni 	BB21_36;

BB21_36:
	cvta.to.global.u64 	%rd35, %rd2;
	mul.wide.s32 	%rd36, %r34, 4;
	add.s64 	%rd37, %rd35, %rd36;
	atom.global.add.u32 	%r93, [%rd37], 1;

BB21_37:
	@%p3 bra 	BB21_29;

BB21_38:
	ret;
}

.visible .entry _Z22MSC_edge_detection_GPUPi(
	.param .u64 _Z22MSC_edge_detection_GPUPi_param_0
)
{
	.reg .pred 	%p<16>;
	.reg .s32 	%r<43>;
	.reg .f32 	%f<7>;
	.reg .s64 	%rd<15>;
	.reg .f64 	%fd<13>;


	ld.param.u64 	%rd4, [_Z22MSC_edge_detection_GPUPi_param_0];
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	setp.ne.s32	%p3, %r1, 199;
	setp.ne.s32	%p4, %r1, 0;
	and.pred  	%p5, %p4, %p3;
	setp.ne.s32	%p6, %r2, 0;
	and.pred  	%p7, %p5, %p6;
	setp.ne.s32	%p8, %r2, 199;
	and.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB22_2;

	mov.pred 	%p15, -1;
	bra.uni 	BB22_7;

BB22_2:
	cvta.to.global.u64 	%rd5, %rd4;
	mul.lo.s32 	%r20, %r1, 200;
	mov.u32 	%r21, %tid.x;
	mul.lo.s32 	%r22, %r21, 40000;
	add.s32 	%r23, %r2, %r20;
	add.s32 	%r24, %r23, %r22;
	mul.wide.s32 	%rd6, %r24, 4;
	add.s64 	%rd7, %rd5, %rd6;
	add.s32 	%r40, %r1, -1;
	add.s32 	%r4, %r1, 1;
	add.s32 	%r5, %r2, 1;
	ld.global.u32 	%r6, [%rd7];
	add.s32 	%r25, %r2, %r22;
	add.s32 	%r7, %r25, %r20;
	mov.u32 	%r42, 0;
	mov.u32 	%r39, %r42;

BB22_3:
	add.s32 	%r26, %r7, %r39;
	mul.wide.s32 	%rd9, %r26, 4;
	add.s64 	%rd10, %rd5, %rd9;
	add.s64 	%rd14, %rd10, -804;
	add.s32 	%r41, %r2, -1;

BB22_4:
	ld.global.u32 	%r28, [%rd14];
	sub.s32 	%r29, %r6, %r28;
	max.s32 	%r42, %r29, %r42;
	add.s64 	%rd14, %rd14, 4;
	add.s32 	%r41, %r41, 1;
	setp.le.s32	%p11, %r41, %r5;
	@%p11 bra 	BB22_4;

	add.s32 	%r40, %r40, 1;
	add.s32 	%r39, %r39, 200;
	setp.le.s32	%p12, %r40, %r4;
	@%p12 bra 	BB22_3;

	setp.lt.s32	%p15, %r42, 51;

BB22_7:
	add.s32 	%r30, %r2, -100;
	cvt.rn.f64.s32	%fd1, %r30;
	add.f64 	%fd2, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd3, %fd2, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f1, %fd3;
	mov.u32 	%r31, 100;
	sub.s32 	%r32, %r31, %r1;
	cvt.rn.f64.s32	%fd4, %r32;
	add.f64 	%fd5, %fd4, 0dBFE0000000000000;
	mul.f64 	%fd6, %fd5, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f2, %fd6;
	bar.sync 	0;
	mul.f32 	%f3, %f1, %f1;
	mul.f32 	%f4, %f2, %f2;
	mul.f32 	%f5, %f4, 0f3F800000;
	fma.rn.f32 	%f6, %f3, 0f3F800000, %f5;
	mov.f64 	%fd7, 0d3FB47AE147AE147B;
	max.f64 	%fd8, %fd7, %fd7;
	fma.rn.f64 	%fd9, %fd8, 0dBFE0000000000000, 0d4020000000000000;
	mul.f64 	%fd10, %fd9, %fd9;
	mul.f64 	%fd11, %fd10, 0d3FF0000000000000;
	cvt.f64.f32	%fd12, %f6;
	setp.ltu.f64	%p13, %fd12, %fd11;
	and.pred  	%p14, %p15, %p13;
	selp.u32	%r33, 1, 0, %p14;
	mad.lo.s32 	%r36, %r1, 200, %r2;
	mov.u32 	%r37, %tid.x;
	mad.lo.s32 	%r38, %r37, 40000, %r36;
	cvta.to.global.u64 	%rd11, %rd4;
	mul.wide.s32 	%rd12, %r38, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %r33;
	ret;
}

.visible .entry _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1_(
	.param .u32 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_0,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_1,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_2,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_3,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_4,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_5,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_6,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_7,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_8,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_9,
	.param .u64 _Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_10
)
{
	.reg .pred 	%p<51>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<102>;
	.reg .f32 	%f<30>;
	.reg .s64 	%rd<38>;
	.reg .f64 	%fd<133>;


	ld.param.u32 	%r35, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_0];
	ld.param.u64 	%rd2, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_1];
	ld.param.u64 	%rd3, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_3];
	ld.param.u64 	%rd4, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_4];
	ld.param.u64 	%rd5, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_5];
	ld.param.u64 	%rd6, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_6];
	ld.param.u64 	%rd7, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_7];
	ld.param.u64 	%rd8, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_8];
	ld.param.u64 	%rd9, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_9];
	ld.param.u64 	%rd10, [_Z6SM_GPUiPiS_PbPfS1_S1_S1_S1_S1_S1__param_10];
	mov.u32 	%r36, %ctaid.x;
	shl.b32 	%r37, %r36, 10;
	mov.u32 	%r38, %tid.x;
	add.s32 	%r1, %r37, %r38;
	setp.ge.s32	%p4, %r1, %r35;
	@%p4 bra 	BB23_38;

	cvta.to.global.u64 	%rd11, %rd3;
	cvt.s64.s32	%rd1, %r1;
	add.s64 	%rd12, %rd11, %rd1;
	ld.global.u8 	%rs1, [%rd12];
	setp.ne.s16	%p5, %rs1, 0;
	@%p5 bra 	BB23_38;

	cvta.to.global.u64 	%rd13, %rd4;
	shl.b64 	%rd14, %rd1, 2;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f15, [%rd15];
	setp.ltu.f32	%p6, %f15, 0f40C00000;
	@%p6 bra 	BB23_38;

	cvta.to.global.u64 	%rd16, %rd10;
	cvta.to.global.u64 	%rd17, %rd7;
	cvta.to.global.u64 	%rd18, %rd9;
	cvta.to.global.u64 	%rd19, %rd6;
	cvta.to.global.u64 	%rd20, %rd8;
	cvta.to.global.u64 	%rd21, %rd5;
	add.s64 	%rd23, %rd21, %rd14;
	add.s64 	%rd24, %rd20, %rd14;
	ld.global.f32 	%f1, [%rd24];
	ld.global.f32 	%f2, [%rd23];
	setp.le.f32	%p7, %f2, %f1;
	selp.u32	%r39, 1, 0, %p7;
	setp.ge.f32	%p8, %f2, %f1;
	selp.u32	%r40, 1, 0, %p8;
	sub.s32 	%r2, %r39, %r40;
	add.s64 	%rd25, %rd18, %rd14;
	ld.global.f32 	%f3, [%rd25];
	add.s64 	%rd26, %rd19, %rd14;
	ld.global.f32 	%f4, [%rd26];
	setp.le.f32	%p9, %f4, %f3;
	selp.u32	%r41, 1, 0, %p9;
	setp.ge.f32	%p10, %f4, %f3;
	selp.u32	%r42, 1, 0, %p10;
	sub.s32 	%r3, %r41, %r42;
	add.s64 	%rd27, %rd16, %rd14;
	ld.global.f32 	%f5, [%rd27];
	add.s64 	%rd28, %rd17, %rd14;
	ld.global.f32 	%f6, [%rd28];
	setp.le.f32	%p11, %f6, %f5;
	selp.u32	%r43, 1, 0, %p11;
	setp.ge.f32	%p12, %f6, %f5;
	selp.u32	%r44, 1, 0, %p12;
	sub.s32 	%r4, %r43, %r44;
	cvt.f64.f32	%fd47, %f2;
	add.f64 	%fd48, %fd47, 0d4020000000000000;
	div.rn.f64 	%fd1, %fd48, 0d3FB47AE147AE147B;
	abs.f64 	%fd2, %fd1;
	setp.lt.f64	%p13, %fd2, 0d7FF0000000000000;
	@%p13 bra 	BB23_7;

	setp.eq.f64	%p14, %fd2, 0d7FF0000000000000;
	@%p14 bra 	BB23_6;

	add.f64 	%fd114, %fd1, %fd1;
	mov.f64 	%fd115, %fd114;
	bra.uni 	BB23_8;

BB23_6:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd1;
	}
	and.b32  	%r46, %r45, -2147483648;
	mov.f64 	%fd49, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r47}, %fd49;
	}
	or.b32  	%r48, %r47, %r46;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd49;
	}
	mov.b64 	%fd114, {%r49, %r48};
	mov.f64 	%fd115, %fd1;
	bra.uni 	BB23_8;

BB23_7:
	cvt.rzi.f64.f64	%fd5, %fd1;
	sub.f64 	%fd50, %fd1, %fd5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r50, %temp}, %fd50;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd50;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd1;
	}
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r51, %r53;
	mov.b64 	%fd114, {%r50, %r54};
	mov.f64 	%fd115, %fd5;

BB23_8:
	cvt.rzi.s32.f64	%r96, %fd115;
	mul.f64 	%fd51, %fd114, 0d3FB47AE147AE147B;
	mov.f64 	%fd52, 0d3FB47AE147AE147B;
	sub.f64 	%fd53, %fd52, %fd51;
	setp.gt.s32	%p15, %r2, 0;
	selp.f64	%fd54, %fd53, 0d0000000000000000, %p15;
	setp.lt.s32	%p16, %r2, 1;
	selp.f64	%fd55, %fd51, 0d0000000000000000, %p16;
	add.f64 	%fd129, %fd54, %fd55;
	neg.s32 	%r6, %r3;
	cvt.f64.f32	%fd56, %f4;
	mov.f64 	%fd57, 0d4020000000000000;
	sub.f64 	%fd58, %fd57, %fd56;
	div.rn.f64 	%fd10, %fd58, 0d3FB47AE147AE147B;
	abs.f64 	%fd11, %fd10;
	setp.lt.f64	%p17, %fd11, 0d7FF0000000000000;
	@%p17 bra 	BB23_12;

	setp.eq.f64	%p18, %fd11, 0d7FF0000000000000;
	@%p18 bra 	BB23_11;

	add.f64 	%fd117, %fd10, %fd10;
	mov.f64 	%fd118, %fd117;
	bra.uni 	BB23_13;

BB23_11:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd10;
	}
	and.b32  	%r56, %r55, -2147483648;
	mov.f64 	%fd59, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd59;
	}
	or.b32  	%r58, %r57, %r56;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r59, %temp}, %fd59;
	}
	mov.b64 	%fd117, {%r59, %r58};
	mov.f64 	%fd118, %fd10;
	bra.uni 	BB23_13;

BB23_12:
	cvt.rzi.f64.f64	%fd14, %fd10;
	sub.f64 	%fd60, %fd10, %fd14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r60, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd10;
	}
	and.b32  	%r63, %r62, -2147483648;
	or.b32  	%r64, %r61, %r63;
	mov.b64 	%fd117, {%r60, %r64};
	mov.f64 	%fd118, %fd14;

BB23_13:
	cvt.rzi.s32.f64	%r94, %fd118;
	shr.u32 	%r65, %r3, 31;
	cvt.rn.f64.s32	%fd61, %r65;
	mul.f64 	%fd62, %fd117, 0d3FB47AE147AE147B;
	sub.f64 	%fd64, %fd52, %fd62;
	setp.lt.s32	%p19, %r6, 1;
	selp.f64	%fd65, %fd62, 0d0000000000000000, %p19;
	fma.rn.f64 	%fd122, %fd61, %fd64, %fd65;
	neg.s32 	%r8, %r4;
	cvt.f64.f32	%fd66, %f6;
	mov.f64 	%fd67, 0d4008000000000000;
	sub.f64 	%fd68, %fd67, %fd66;
	mul.f64 	%fd69, %fd68, 0d4010000000000000;
	cvt.rn.f32.f64	%f7, %fd69;
	abs.f32 	%f8, %f7;
	setp.lt.f32	%p20, %f8, 0f7F800000;
	@%p20 bra 	BB23_17;

	setp.eq.f32	%p21, %f8, 0f7F800000;
	@%p21 bra 	BB23_16;

	add.f32 	%f28, %f7, %f7;
	mov.f32 	%f29, %f28;
	bra.uni 	BB23_18;

BB23_16:
	mov.b32 	 %r66, %f7;
	and.b32  	%r67, %r66, -2147483648;
	mov.b32 	 %f28, %r67;
	mov.f32 	%f29, %f7;
	bra.uni 	BB23_18;

BB23_17:
	cvt.rzi.f32.f32	%f11, %f7;
	sub.f32 	%f16, %f7, %f11;
	mov.b32 	 %r68, %f16;
	mov.b32 	 %r69, %f7;
	and.b32  	%r70, %r69, -2147483648;
	or.b32  	%r71, %r68, %r70;
	mov.b32 	 %f28, %r71;
	mov.f32 	%f29, %f11;

BB23_18:
	shr.u32 	%r72, %r4, 31;
	cvt.rn.f64.s32	%fd70, %r72;
	mul.f32 	%f17, %f28, 0f3E800000;
	cvt.f64.f32	%fd71, %f17;
	mov.f64 	%fd72, 0d3FD0000000000000;
	sub.f64 	%fd73, %fd72, %fd71;
	setp.lt.s32	%p22, %r8, 1;
	selp.f64	%fd74, %fd71, 0d0000000000000000, %p22;
	fma.rn.f64 	%fd120, %fd70, %fd73, %fd74;
	cvt.rzi.s32.f32	%r95, %f29;
	mul.lo.s32 	%r10, %r95, 40000;
	mad.lo.s32 	%r73, %r94, 200, %r96;
	add.s32 	%r74, %r73, %r10;
	cvt.f64.f32	%fd75, %f1;
	add.f64 	%fd76, %fd75, 0d4020000000000000;
	div.rn.f64 	%fd78, %fd76, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r75, %fd78;
	cvt.f64.f32	%fd79, %f3;
	sub.f64 	%fd80, %fd57, %fd79;
	div.rn.f64 	%fd81, %fd80, 0d3FB47AE147AE147B;
	cvt.rzi.s32.f64	%r76, %fd81;
	cvt.f64.f32	%fd82, %f5;
	sub.f64 	%fd84, %fd67, %fd82;
	mul.f64 	%fd85, %fd84, 0d4010000000000000;
	cvt.rzi.s32.f64	%r77, %fd85;
	mad.lo.s32 	%r78, %r76, 200, %r75;
	mad.lo.s32 	%r11, %r77, 40000, %r78;
	sub.f32 	%f18, %f3, %f4;
	abs.f32 	%f19, %f18;
	sub.f32 	%f20, %f1, %f2;
	abs.f32 	%f21, %f20;
	div.rn.f32 	%f22, %f19, %f21;
	cvt.f64.f32	%fd20, %f22;
	sub.f32 	%f23, %f5, %f6;
	abs.f32 	%f24, %f23;
	div.rn.f32 	%f25, %f24, %f21;
	cvt.f64.f32	%fd21, %f25;
	div.rn.f32 	%f26, %f24, %f19;
	cvt.f64.f32	%fd22, %f26;
	setp.ne.s32	%p23, %r74, %r11;
	setp.lt.s32	%p24, %r96, 200;
	and.pred  	%p25, %p23, %p24;
	setp.lt.s32	%p26, %r94, 200;
	and.pred  	%p27, %p25, %p26;
	setp.lt.s32	%p28, %r95, 24;
	and.pred  	%p1, %p28, %p27;
	@!%p1 bra 	BB23_20;
	bra.uni 	BB23_19;

BB23_19:
	cvta.to.global.u64 	%rd29, %rd2;
	mul.wide.s32 	%rd30, %r74, 4;
	add.s64 	%rd31, %rd29, %rd30;
	atom.global.add.u32 	%r81, [%rd31], 1;

BB23_20:
	xor.pred  	%p31, %p11, %p12;
	@%p31 bra 	BB23_28;

	@!%p1 bra 	BB23_38;
	bra.uni 	BB23_22;

BB23_22:
	div.rn.f64 	%fd25, %fd122, %fd20;
	setp.gtu.f64	%p32, %fd129, %fd25;
	@%p32 bra 	BB23_24;

	mul.f64 	%fd87, %fd20, %fd129;
	abs.s32 	%r82, %r3;
	cvt.rn.f64.s32	%fd88, %r82;
	mul.f64 	%fd89, %fd87, %fd88;
	sub.f64 	%fd90, %fd122, %fd89;
	add.s32 	%r96, %r96, %r2;
	setp.eq.f64	%p33, %fd90, 0d0000000000000000;
	selp.b32	%r101, %r3, 0, %p33;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd90, %p33;
	mov.f64 	%fd130, %fd52;
	bra.uni 	BB23_25;

BB23_24:
	abs.s32 	%r83, %r2;
	cvt.rn.f64.s32	%fd92, %r83;
	mul.f64 	%fd93, %fd25, %fd92;
	sub.f64 	%fd27, %fd129, %fd93;
	mov.f64 	%fd122, 0d3FB47AE147AE147B;
	mov.f64 	%fd130, %fd27;
	mov.u32 	%r101, %r3;

BB23_25:
	mov.f64 	%fd124, %fd130;
	mov.f64 	%fd129, %fd124;
	sub.s32 	%r94, %r94, %r101;
	add.s32 	%r84, %r96, %r10;
	mad.lo.s32 	%r19, %r94, 200, %r84;
	setp.eq.s32	%p34, %r19, %r11;
	setp.gt.s32	%p35, %r96, 199;
	or.pred  	%p36, %p34, %p35;
	setp.gt.s32	%p37, %r94, 199;
	or.pred  	%p2, %p36, %p37;
	@%p2 bra 	BB23_27;

	cvta.to.global.u64 	%rd32, %rd2;
	mul.wide.s32 	%rd33, %r19, 4;
	add.s64 	%rd34, %rd32, %rd33;
	atom.global.add.u32 	%r85, [%rd34], 1;

BB23_27:
	@%p2 bra 	BB23_38;
	bra.uni 	BB23_22;

BB23_28:
	mov.f64 	%fd127, %fd129;
	@!%p1 bra 	BB23_38;
	bra.uni 	BB23_29;

BB23_29:
	mul.f64 	%fd33, %fd22, %fd122;
	setp.le.f64	%p38, %fd120, %fd33;
	mul.f64 	%fd34, %fd21, %fd127;
	setp.le.f64	%p39, %fd120, %fd34;
	and.pred  	%p40, %p39, %p38;
	@%p40 bra 	BB23_34;

	setp.gtu.f64	%p41, %fd34, %fd33;
	abs.s32 	%r86, %r4;
	cvt.rn.f64.s32	%fd35, %r86;
	@%p41 bra 	BB23_32;

	mul.f64 	%fd95, %fd20, %fd127;
	abs.s32 	%r87, %r3;
	cvt.rn.f64.s32	%fd96, %r87;
	mul.f64 	%fd97, %fd95, %fd96;
	sub.f64 	%fd98, %fd122, %fd97;
	add.s32 	%r96, %r96, %r2;
	setp.eq.f64	%p42, %fd98, 0d0000000000000000;
	selp.b32	%r24, %r3, 0, %p42;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd98, %p42;
	mov.f64 	%fd131, 0d3FB47AE147AE147B;
	mov.f64 	%fd132, %fd34;
	mov.u32 	%r100, %r24;
	bra.uni 	BB23_33;

BB23_32:
	div.rn.f64 	%fd100, %fd122, %fd20;
	abs.s32 	%r88, %r2;
	cvt.rn.f64.s32	%fd101, %r88;
	mul.f64 	%fd102, %fd100, %fd101;
	sub.f64 	%fd131, %fd127, %fd102;
	mov.f64 	%fd122, 0d3FB47AE147AE147B;
	mov.f64 	%fd132, %fd33;
	mov.u32 	%r100, %r3;

BB23_33:
	mov.u32 	%r98, %r100;
	mov.u32 	%r99, %r98;
	mov.f64 	%fd128, %fd131;
	mul.f64 	%fd103, %fd132, %fd35;
	sub.f64 	%fd41, %fd120, %fd103;
	mov.f64 	%fd121, %fd41;
	bra.uni 	BB23_35;

BB23_34:
	div.rn.f64 	%fd105, %fd120, %fd21;
	abs.s32 	%r89, %r2;
	cvt.rn.f64.s32	%fd106, %r89;
	mul.f64 	%fd107, %fd105, %fd106;
	sub.f64 	%fd108, %fd127, %fd107;
	abs.s32 	%r90, %r3;
	cvt.rn.f64.s32	%fd109, %r90;
	div.rn.f64 	%fd110, %fd120, %fd22;
	mul.f64 	%fd111, %fd110, %fd109;
	sub.f64 	%fd112, %fd122, %fd111;
	sub.s32 	%r95, %r95, %r4;
	setp.eq.f64	%p43, %fd108, 0d0000000000000000;
	selp.b32	%r91, %r2, 0, %p43;
	add.s32 	%r96, %r91, %r96;
	selp.f64	%fd128, 0d3FB47AE147AE147B, %fd108, %p43;
	setp.eq.f64	%p44, %fd112, 0d0000000000000000;
	selp.b32	%r99, %r3, 0, %p44;
	selp.f64	%fd122, 0d3FB47AE147AE147B, %fd112, %p44;
	mov.f64 	%fd121, %fd72;

BB23_35:
	mov.f64 	%fd127, %fd128;
	mov.f64 	%fd119, %fd121;
	mov.f64 	%fd120, %fd119;
	sub.s32 	%r94, %r94, %r99;
	mad.lo.s32 	%r92, %r95, 40000, %r96;
	mad.lo.s32 	%r34, %r94, 200, %r92;
	setp.ne.s32	%p45, %r34, %r11;
	setp.lt.s32	%p46, %r96, 200;
	and.pred  	%p47, %p45, %p46;
	setp.lt.s32	%p48, %r94, 200;
	and.pred  	%p49, %p47, %p48;
	setp.lt.s32	%p50, %r95, 24;
	and.pred  	%p3, %p50, %p49;
	@!%p3 bra 	BB23_37;
	bra.uni 	BB23_36;

BB23_36:
	cvta.to.global.u64 	%rd35, %rd2;
	mul.wide.s32 	%rd36, %r34, 4;
	add.s64 	%rd37, %rd35, %rd36;
	atom.global.add.u32 	%r93, [%rd37], 1;

BB23_37:
	@%p3 bra 	BB23_29;

BB23_38:
	ret;
}

.visible .entry _Z21SM_edge_detection_GPUPiS_(
	.param .u64 _Z21SM_edge_detection_GPUPiS__param_0,
	.param .u64 _Z21SM_edge_detection_GPUPiS__param_1
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<13>;
	.reg .f32 	%f<7>;
	.reg .s64 	%rd<9>;
	.reg .f64 	%fd<18>;


	ld.param.u64 	%rd2, [_Z21SM_edge_detection_GPUPiS__param_0];
	ld.param.u64 	%rd3, [_Z21SM_edge_detection_GPUPiS__param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r5, %r2, 200, %r1;
	mad.lo.s32 	%r4, %r3, 40000, %r5;
	setp.gt.s32	%p1, %r4, 959999;
	@%p1 bra 	BB24_3;

	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	add.s32 	%r6, %r1, -100;
	cvt.rn.f64.s32	%fd1, %r6;
	add.f64 	%fd2, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd3, %fd2, 0d3FB47AE147AE147B;
	mov.f64 	%fd4, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f1, %fd3;
	mov.u32 	%r7, 100;
	sub.s32 	%r8, %r7, %r2;
	cvt.rn.f64.s32	%fd5, %r8;
	add.f64 	%fd6, %fd5, 0dBFE0000000000000;
	mul.f64 	%fd7, %fd6, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f2, %fd7;
	mul.wide.s32 	%rd6, %r4, 4;
	add.s64 	%rd1, %rd5, %rd6;
	ld.global.u32 	%r9, [%rd1];
	cvt.rn.f64.s32	%fd8, %r9;
	mul.wide.s32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd4, %rd7;
	ld.global.u32 	%r10, [%rd8];
	cvt.rn.f64.s32	%fd9, %r10;
	mul.f64 	%fd10, %fd9, 0d3FF0000000000000;
	setp.gt.f64	%p2, %fd8, %fd10;
	selp.u32	%r11, 1, 0, %p2;
	st.global.u32 	[%rd1], %r11;
	mul.f32 	%f3, %f1, %f1;
	mul.f32 	%f4, %f2, %f2;
	mul.f32 	%f5, %f4, 0f3F800000;
	fma.rn.f32 	%f6, %f3, 0f3F800000, %f5;
	max.f64 	%fd11, %fd4, %fd4;
	mul.f64 	%fd12, %fd11, 0d3FE0000000000000;
	mov.f64 	%fd13, 0d4020000000000000;
	sub.f64 	%fd14, %fd13, %fd12;
	mul.f64 	%fd15, %fd14, %fd14;
	mul.f64 	%fd16, %fd15, 0d3FF0000000000000;
	cvt.f64.f32	%fd17, %f6;
	setp.ltu.f64	%p3, %fd17, %fd16;
	@%p3 bra 	BB24_3;

	mov.u32 	%r12, 0;
	st.global.u32 	[%rd1], %r12;

BB24_3:
	ret;
}

.visible .entry _Z23SM_edge_detection_GPU_2PiS_(
	.param .u64 _Z23SM_edge_detection_GPU_2PiS__param_0,
	.param .u64 _Z23SM_edge_detection_GPU_2PiS__param_1
)
{
	.reg .pred 	%p<30>;
	.reg .s32 	%r<157>;
	.reg .f32 	%f<7>;
	.reg .s64 	%rd<27>;
	.reg .f64 	%fd<18>;


	ld.param.u64 	%rd8, [_Z23SM_edge_detection_GPU_2PiS__param_0];
	ld.param.u64 	%rd9, [_Z23SM_edge_detection_GPU_2PiS__param_1];
	mov.u32 	%r81, %ctaid.y;
	mov.u32 	%r82, %ctaid.x;
	setp.ne.s32	%p1, %r81, 199;
	setp.ne.s32	%p2, %r81, 0;
	and.pred  	%p3, %p2, %p1;
	setp.ne.s32	%p4, %r82, 0;
	and.pred  	%p5, %p3, %p4;
	setp.ne.s32	%p6, %r82, 199;
	and.pred  	%p7, %p5, %p6;
	@!%p7 bra 	BB25_6;
	bra.uni 	BB25_1;

BB25_1:
	add.s32 	%r118, %r81, -1;
	mad.lo.s32 	%r86, %r81, 200, %r82;
	mov.u32 	%r87, %tid.x;
	mad.lo.s32 	%r88, %r87, 40000, %r86;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.s32 	%rd11, %r88, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%r2, [%rd12];
	mov.u32 	%r120, 0;

BB25_2:
	mul.lo.s32 	%r90, %r87, 40000;
	mad.lo.s32 	%r5, %r118, 200, %r90;
	add.s32 	%r119, %r82, -1;

BB25_3:
	add.s32 	%r92, %r5, %r119;
	mul.wide.s32 	%rd14, %r92, 4;
	add.s64 	%rd15, %rd10, %rd14;
	ld.global.u32 	%r93, [%rd15];
	sub.s32 	%r94, %r2, %r93;
	max.s32 	%r120, %r94, %r120;
	add.s32 	%r96, %r82, 1;
	add.s32 	%r119, %r119, 1;
	setp.le.s32	%p8, %r119, %r96;
	@%p8 bra 	BB25_3;

	add.s32 	%r118, %r118, 1;
	add.s32 	%r98, %r81, 1;
	setp.le.s32	%p9, %r118, %r98;
	@%p9 bra 	BB25_2;

	cvta.to.global.u64 	%rd16, %rd9;
	mul.wide.s32 	%rd17, %r88, 4;
	add.s64 	%rd18, %rd16, %rd17;
	st.global.u32 	[%rd18], %r120;

BB25_6:
	bar.sync 	0;
	mov.u32 	%r107, %tid.x;
	mul.lo.s32 	%r108, %r107, 40000;
	cvta.to.global.u64 	%rd19, %rd9;
	mul.wide.s32 	%rd20, %r108, 4;
	or.b64  	%rd21, %rd20, 32;
	add.s64 	%rd26, %rd19, %rd21;
	cvta.to.global.u64 	%rd22, %rd8;
	add.s64 	%rd25, %rd22, %rd21;
	mov.u32 	%r155, 0;
	mov.u32 	%r121, %r155;

BB25_7:
	mov.u32 	%r137, %r155;
	mov.u32 	%r14, %r137;
	ld.global.u32 	%r15, [%rd26+-32];
	setp.le.s32	%p10, %r15, %r14;
	mov.u32 	%r154, %r14;
	@%p10 bra 	BB25_9;

	ld.global.u32 	%r156, [%rd25+-32];
	mov.u32 	%r154, %r15;

BB25_9:
	mov.u32 	%r17, %r154;
	ld.global.u32 	%r19, [%rd26+-28];
	setp.le.s32	%p11, %r19, %r17;
	mov.u32 	%r153, %r17;
	@%p11 bra 	BB25_11;

	ld.global.u32 	%r156, [%rd25+-28];
	mov.u32 	%r153, %r19;

BB25_11:
	mov.u32 	%r21, %r153;
	ld.global.u32 	%r23, [%rd26+-24];
	setp.le.s32	%p12, %r23, %r21;
	mov.u32 	%r152, %r21;
	@%p12 bra 	BB25_13;

	ld.global.u32 	%r156, [%rd25+-24];
	mov.u32 	%r152, %r23;

BB25_13:
	mov.u32 	%r25, %r152;
	ld.global.u32 	%r27, [%rd26+-20];
	setp.le.s32	%p13, %r27, %r25;
	mov.u32 	%r151, %r25;
	@%p13 bra 	BB25_15;

	ld.global.u32 	%r156, [%rd25+-20];
	mov.u32 	%r151, %r27;

BB25_15:
	mov.u32 	%r29, %r151;
	ld.global.u32 	%r31, [%rd26+-16];
	setp.le.s32	%p14, %r31, %r29;
	mov.u32 	%r150, %r29;
	@%p14 bra 	BB25_17;

	ld.global.u32 	%r156, [%rd25+-16];
	mov.u32 	%r150, %r31;

BB25_17:
	mov.u32 	%r33, %r150;
	ld.global.u32 	%r35, [%rd26+-12];
	setp.le.s32	%p15, %r35, %r33;
	mov.u32 	%r149, %r33;
	@%p15 bra 	BB25_19;

	ld.global.u32 	%r156, [%rd25+-12];
	mov.u32 	%r149, %r35;

BB25_19:
	mov.u32 	%r37, %r149;
	ld.global.u32 	%r39, [%rd26+-8];
	setp.le.s32	%p16, %r39, %r37;
	mov.u32 	%r148, %r37;
	@%p16 bra 	BB25_21;

	ld.global.u32 	%r156, [%rd25+-8];
	mov.u32 	%r148, %r39;

BB25_21:
	mov.u32 	%r41, %r148;
	ld.global.u32 	%r43, [%rd26+-4];
	setp.le.s32	%p17, %r43, %r41;
	mov.u32 	%r147, %r41;
	@%p17 bra 	BB25_23;

	ld.global.u32 	%r156, [%rd25+-4];
	mov.u32 	%r147, %r43;

BB25_23:
	mov.u32 	%r45, %r147;
	ld.global.u32 	%r47, [%rd26];
	setp.le.s32	%p18, %r47, %r45;
	mov.u32 	%r146, %r45;
	@%p18 bra 	BB25_25;

	ld.global.u32 	%r156, [%rd25];
	mov.u32 	%r146, %r47;

BB25_25:
	mov.u32 	%r49, %r146;
	ld.global.u32 	%r51, [%rd26+4];
	setp.le.s32	%p19, %r51, %r49;
	mov.u32 	%r145, %r49;
	@%p19 bra 	BB25_27;

	ld.global.u32 	%r156, [%rd25+4];
	mov.u32 	%r145, %r51;

BB25_27:
	mov.u32 	%r53, %r145;
	ld.global.u32 	%r55, [%rd26+8];
	setp.le.s32	%p20, %r55, %r53;
	mov.u32 	%r144, %r53;
	@%p20 bra 	BB25_29;

	ld.global.u32 	%r156, [%rd25+8];
	mov.u32 	%r144, %r55;

BB25_29:
	mov.u32 	%r57, %r144;
	ld.global.u32 	%r59, [%rd26+12];
	setp.le.s32	%p21, %r59, %r57;
	mov.u32 	%r143, %r57;
	@%p21 bra 	BB25_31;

	ld.global.u32 	%r156, [%rd25+12];
	mov.u32 	%r143, %r59;

BB25_31:
	mov.u32 	%r61, %r143;
	ld.global.u32 	%r63, [%rd26+16];
	setp.le.s32	%p22, %r63, %r61;
	mov.u32 	%r142, %r61;
	@%p22 bra 	BB25_33;

	ld.global.u32 	%r156, [%rd25+16];
	mov.u32 	%r142, %r63;

BB25_33:
	mov.u32 	%r65, %r142;
	ld.global.u32 	%r67, [%rd26+20];
	setp.le.s32	%p23, %r67, %r65;
	mov.u32 	%r141, %r65;
	@%p23 bra 	BB25_35;

	ld.global.u32 	%r156, [%rd25+20];
	mov.u32 	%r141, %r67;

BB25_35:
	mov.u32 	%r69, %r141;
	ld.global.u32 	%r71, [%rd26+24];
	setp.le.s32	%p24, %r71, %r69;
	mov.u32 	%r139, %r69;
	@%p24 bra 	BB25_37;

	ld.global.u32 	%r156, [%rd25+24];
	mov.u32 	%r139, %r71;

BB25_37:
	mov.u32 	%r73, %r139;
	ld.global.u32 	%r75, [%rd26+28];
	setp.le.s32	%p25, %r75, %r73;
	mov.u32 	%r140, %r73;
	@%p25 bra 	BB25_39;

	ld.global.u32 	%r156, [%rd25+28];
	mov.u32 	%r140, %r75;

BB25_39:
	mov.u32 	%r155, %r140;
	add.s64 	%rd26, %rd26, 64;
	add.s64 	%rd25, %rd25, 64;
	add.s32 	%r121, %r121, 16;
	setp.ne.s32	%p26, %r121, 40000;
	@%p26 bra 	BB25_7;

	mad.lo.s32 	%r80, %r107, 40000, 39999;
	bar.sync 	0;
	setp.gt.s32	%p27, %r80, 959999;
	@%p27 bra 	BB25_43;

	add.s32 	%r111, %r82, -100;
	cvt.rn.f64.s32	%fd1, %r111;
	add.f64 	%fd2, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd3, %fd2, 0d3FB47AE147AE147B;
	mov.f64 	%fd4, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f1, %fd3;
	mov.u32 	%r113, 100;
	sub.s32 	%r114, %r113, %r81;
	cvt.rn.f64.s32	%fd5, %r114;
	add.f64 	%fd6, %fd5, 0dBFE0000000000000;
	mul.f64 	%fd7, %fd6, 0d3FB47AE147AE147B;
	cvt.rn.f32.f64	%f2, %fd7;
	mul.wide.s32 	%rd24, %r80, 4;
	add.s64 	%rd7, %rd22, %rd24;
	ld.global.u32 	%r115, [%rd7];
	cvt.rn.f64.s32	%fd8, %r115;
	cvt.rn.f64.s32	%fd9, %r156;
	mul.f64 	%fd10, %fd9, 0d3FF0000000000000;
	setp.gt.f64	%p28, %fd8, %fd10;
	selp.u32	%r116, 1, 0, %p28;
	st.global.u32 	[%rd7], %r116;
	mul.f32 	%f3, %f1, %f1;
	mul.f32 	%f4, %f2, %f2;
	mul.f32 	%f5, %f4, 0f3F800000;
	fma.rn.f32 	%f6, %f3, 0f3F800000, %f5;
	max.f64 	%fd11, %fd4, %fd4;
	mul.f64 	%fd12, %fd11, 0d3FE0000000000000;
	mov.f64 	%fd13, 0d4020000000000000;
	sub.f64 	%fd14, %fd13, %fd12;
	mul.f64 	%fd15, %fd14, %fd14;
	mul.f64 	%fd16, %fd15, 0d3FF0000000000000;
	cvt.f64.f32	%fd17, %f6;
	setp.ltu.f64	%p29, %fd17, %fd16;
	@%p29 bra 	BB25_43;

	mov.u32 	%r117, 0;
	st.global.u32 	[%rd7], %r117;

BB25_43:
	ret;
}

.visible .entry _Z28create_hull_image_hybrid_GPURPbRPf(
	.param .u64 _Z28create_hull_image_hybrid_GPURPbRPf_param_0,
	.param .u64 _Z28create_hull_image_hybrid_GPURPbRPf_param_1
)
{
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<6>;
	.reg .f32 	%f<4>;
	.reg .s64 	%rd<13>;


	ld.param.u64 	%rd1, [_Z28create_hull_image_hybrid_GPURPbRPf_param_0];
	ld.param.u64 	%rd2, [_Z28create_hull_image_hybrid_GPURPbRPf_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, 200, %r2;
	mad.lo.s32 	%r5, %r3, 40000, %r4;
	ldu.global.u64 	%rd5, [%rd4];
	cvta.to.global.u64 	%rd6, %rd5;
	cvt.s64.s32	%rd7, %r5;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.s8 	%rs1, [%rd8];
	cvt.rn.f32.s16	%f1, %rs1;
	ldu.global.u64 	%rd9, [%rd3];
	cvta.to.global.u64 	%rd10, %rd9;
	mul.wide.s32 	%rd11, %r5, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f2, [%rd12];
	mul.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd12], %f3;
	ret;
}

.visible .entry _Z13test_func_GPUPi(
	.param .u64 _Z13test_func_GPUPi_param_0
)
{
	.local .align 8 .b8 	__local_depot27[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .s32 	%r<24>;
	.reg .f32 	%f<3>;
	.reg .s64 	%rd<25>;
	.reg .f64 	%fd<97>;


	mov.u64 	%SPL, __local_depot27;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd4, [_Z13test_func_GPUPi_param_0];
	add.u64 	%rd5, %SP, 36;
	cvta.to.local.u64 	%rd1, %rd5;
	add.u64 	%rd6, %SP, 40;
	cvta.to.local.u64 	%rd2, %rd6;
	cvta.global.u64 	%rd7, $str29;
	mov.u64 	%rd8, 0;
	// Callseq Start 58
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd7;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd8;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r7, [retval0+0];
	}
	// Callseq End 58
	cvta.global.u64 	%rd9, $str30;
	// Callseq Start 59
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd9;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd8;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r8, [retval0+0];
	}
	// Callseq End 59
	mov.f64 	%fd91, 0d3FF0000000000000;
	abs.f64 	%fd1, %fd91;
	setp.eq.f64	%p1, %fd1, 0d7FF0000000000000;
	@%p1 bra 	BB27_1;
	bra.uni 	BB27_2;

BB27_1:
	mov.f64 	%fd28, 0d0000000000000000;
	mov.f64 	%fd29, 0d3FF0000000000000;
	mul.rn.f64 	%fd91, %fd29, %fd28;

BB27_2:
	mul.f64 	%fd30, %fd91, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r22, %fd30;
	st.local.u32 	[%rd2], %r22;
	cvt.rn.f64.s32	%fd31, %r22;
	neg.f64 	%fd32, %fd31;
	mov.f64 	%fd33, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd34, %fd32, %fd33, %fd91;
	mov.f64 	%fd35, 0d3C91A62633145C00;
	fma.rn.f64 	%fd36, %fd32, %fd35, %fd34;
	mov.f64 	%fd37, 0d397B839A252049C0;
	fma.rn.f64 	%fd92, %fd32, %fd37, %fd36;
	abs.f64 	%fd38, %fd91;
	setp.leu.f64	%p2, %fd38, 0d41E0000000000000;
	@%p2 bra 	BB27_4;

	// Callseq Start 60
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd91;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd6;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd92, [retval0+0];
	}
	// Callseq End 60
	ld.local.u32 	%r22, [%rd2];

BB27_4:
	shl.b32 	%r9, %r22, 3;
	and.b32  	%r10, %r9, 8;
	and.b32  	%r11, %r22, 1;
	setp.eq.b32	%p3, %r11, 1;
	not.pred 	%p4, %p3;
	selp.f64	%fd39, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p4;
	mul.wide.u32 	%rd11, %r10, 8;
	mov.u64 	%rd12, __cudart_sin_cos_coeffs;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.f64 	%fd40, [%rd13+8];
	mul.rn.f64 	%fd7, %fd92, %fd92;
	fma.rn.f64 	%fd41, %fd39, %fd7, %fd40;
	ld.const.f64 	%fd42, [%rd13+16];
	fma.rn.f64 	%fd43, %fd41, %fd7, %fd42;
	ld.const.f64 	%fd44, [%rd13+24];
	fma.rn.f64 	%fd45, %fd43, %fd7, %fd44;
	ld.const.f64 	%fd46, [%rd13+32];
	fma.rn.f64 	%fd47, %fd45, %fd7, %fd46;
	ld.const.f64 	%fd48, [%rd13+40];
	fma.rn.f64 	%fd49, %fd47, %fd7, %fd48;
	ld.const.f64 	%fd50, [%rd13+48];
	fma.rn.f64 	%fd8, %fd49, %fd7, %fd50;
	fma.rn.f64 	%fd93, %fd8, %fd92, %fd92;
	@%p4 bra 	BB27_6;

	mov.f64 	%fd51, 0d3FF0000000000000;
	fma.rn.f64 	%fd93, %fd8, %fd7, %fd51;

BB27_6:
	and.b32  	%r12, %r22, 2;
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB27_8;

	mov.f64 	%fd52, 0d0000000000000000;
	mov.f64 	%fd53, 0dBFF0000000000000;
	fma.rn.f64 	%fd93, %fd93, %fd53, %fd52;

BB27_8:
	@%p1 bra 	BB27_10;

	mov.f64 	%fd94, 0d3FF0000000000000;
	bra.uni 	BB27_11;

BB27_10:
	mov.f64 	%fd55, 0d0000000000000000;
	mov.f64 	%fd56, 0d3FF0000000000000;
	mul.rn.f64 	%fd94, %fd56, %fd55;

BB27_11:
	mul.f64 	%fd57, %fd94, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r23, %fd57;
	st.local.u32 	[%rd1], %r23;
	cvt.rn.f64.s32	%fd58, %r23;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd61, %fd59, %fd33, %fd94;
	fma.rn.f64 	%fd63, %fd59, %fd35, %fd61;
	fma.rn.f64 	%fd95, %fd59, %fd37, %fd63;
	abs.f64 	%fd65, %fd94;
	setp.leu.f64	%p7, %fd65, 0d41E0000000000000;
	@%p7 bra 	BB27_13;

	// Callseq Start 61
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd94;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd5;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd95, [retval0+0];
	}
	// Callseq End 61
	ld.local.u32 	%r23, [%rd1];

BB27_13:
	shl.b32 	%r13, %r23, 3;
	and.b32  	%r14, %r13, 8;
	and.b32  	%r15, %r23, 1;
	setp.eq.b32	%p8, %r15, 1;
	not.pred 	%p9, %p8;
	selp.f64	%fd66, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p9;
	mul.wide.u32 	%rd15, %r14, 8;
	add.s64 	%rd17, %rd12, %rd15;
	ld.const.f64 	%fd67, [%rd17+8];
	mul.rn.f64 	%fd19, %fd95, %fd95;
	fma.rn.f64 	%fd68, %fd66, %fd19, %fd67;
	ld.const.f64 	%fd69, [%rd17+16];
	fma.rn.f64 	%fd70, %fd68, %fd19, %fd69;
	ld.const.f64 	%fd71, [%rd17+24];
	fma.rn.f64 	%fd72, %fd70, %fd19, %fd71;
	ld.const.f64 	%fd73, [%rd17+32];
	fma.rn.f64 	%fd74, %fd72, %fd19, %fd73;
	ld.const.f64 	%fd75, [%rd17+40];
	fma.rn.f64 	%fd76, %fd74, %fd19, %fd75;
	ld.const.f64 	%fd77, [%rd17+48];
	fma.rn.f64 	%fd20, %fd76, %fd19, %fd77;
	fma.rn.f64 	%fd96, %fd20, %fd95, %fd95;
	@%p9 bra 	BB27_15;

	mov.f64 	%fd78, 0d3FF0000000000000;
	fma.rn.f64 	%fd96, %fd20, %fd19, %fd78;

BB27_15:
	and.b32  	%r16, %r23, 2;
	setp.eq.s32	%p10, %r16, 0;
	@%p10 bra 	BB27_17;

	mov.f64 	%fd79, 0d0000000000000000;
	mov.f64 	%fd80, 0dBFF0000000000000;
	fma.rn.f64 	%fd96, %fd96, %fd80, %fd79;

BB27_17:
	add.u64 	%rd18, %SP, 0;
	cvta.to.local.u64 	%rd3, %rd18;
	setp.eq.f64	%p11, %fd93, 0d0000000000000000;
	selp.f64	%fd81, 0d3F989374BC6A7EFA, 0d0000000000000000, %p11;
	mov.f64 	%fd82, 0d3F8EB851EB851EB8;
	sub.f64 	%fd83, %fd82, %fd81;
	setp.eq.f64	%p12, %fd96, 0d0000000000000000;
	selp.f64	%fd84, 0d3F989374BC6A7EFA, 0d0000000000000000, %p12;
	sub.f64 	%fd85, %fd82, %fd84;
	mov.u64 	%rd19, 4607182418800017408;
	st.local.u64 	[%rd3], %rd19;
	mov.u64 	%rd20, 4582574750436065018;
	st.local.u64 	[%rd3+8], %rd20;
	st.local.f64 	[%rd3+16], %fd83;
	st.local.f64 	[%rd3+24], %fd85;
	cvta.global.u64 	%rd21, $str32;
	// Callseq Start 62
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd21;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd18;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r17, [retval0+0];
	}
	// Callseq End 62
	mov.f64 	%fd86, 0d4000000000000000;
	div.rn.f64 	%fd87, %fd86, 0d0000000000000000;
	cvt.rn.f32.f64	%f1, %fd87;
	setp.leu.f32	%p13, %f1, 0f3F800000;
	@%p13 bra 	BB27_19;

	cvta.to.global.u64 	%rd22, %rd4;
	mov.u32 	%r18, 1;
	st.global.u32 	[%rd22+4], %r18;
	st.global.u32 	[%rd22+8], %r18;

BB27_19:
	cvt.f64.f32	%fd90, %f1;
	st.local.f64 	[%rd3], %fd90;
	st.local.f64 	[%rd3+8], %fd90;
	st.local.f64 	[%rd3+16], %fd90;
	setp.gt.f32	%p14, %f1, 0f3F800000;
	selp.u32	%r19, 1, 0, %p14;
	mov.u32 	%r20, 0;
	st.local.v2.u32 	[%rd3+24], {%r20, %r19};
	st.local.u32 	[%rd3+32], %r19;
	cvta.global.u64 	%rd24, $str33;
	// Callseq Start 63
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64	[param0+0], %rd24;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd18;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32	%r21, [retval0+0];
	}
	// Callseq End 63
	ret;
}

.visible .entry _Z19initialize_hull_GPUIbEvPT_(
	.param .u64 _Z19initialize_hull_GPUIbEvPT__param_0
)
{
	.reg .pred 	%p<2>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<9>;
	.reg .s64 	%rd<5>;
	.reg .f64 	%fd<11>;


	ld.param.u64 	%rd1, [_Z19initialize_hull_GPUIbEvPT__param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, -100;
	cvt.rn.f64.s32	%fd1, %r3;
	add.f64 	%fd2, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd3, %fd2, 0d3FB47AE147AE147B;
	mov.u32 	%r4, 100;
	sub.s32 	%r5, %r4, %r1;
	cvt.rn.f64.s32	%fd4, %r5;
	add.f64 	%fd5, %fd4, 0dBFE0000000000000;
	mul.f64 	%fd6, %fd3, %fd3;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r1, 200, %r2;
	mad.lo.s32 	%r8, %r6, 40000, %r7;
	mul.f64 	%fd7, %fd5, 0d3FB47AE147AE147B;
	mul.f64 	%fd8, %fd7, %fd7;
	mul.f64 	%fd9, %fd8, 0d3FF0000000000000;
	fma.rn.f64 	%fd10, %fd6, 0d3FF0000000000000, %fd9;
	setp.lt.f64	%p1, %fd10, 0d4050000000000000;
	cvt.s64.s32	%rd3, %r8;
	add.s64 	%rd4, %rd2, %rd3;
	selp.u16	%rs1, 1, 0, %p1;
	st.global.u8 	[%rd4], %rs1;
	ret;
}

.visible .entry _Z19initialize_hull_GPUIiEvPT_(
	.param .u64 _Z19initialize_hull_GPUIiEvPT__param_0
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<10>;
	.reg .s64 	%rd<5>;
	.reg .f64 	%fd<11>;


	ld.param.u64 	%rd1, [_Z19initialize_hull_GPUIiEvPT__param_0];
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, -100;
	cvt.rn.f64.s32	%fd1, %r3;
	add.f64 	%fd2, %fd1, 0d3FE0000000000000;
	mul.f64 	%fd3, %fd2, 0d3FB47AE147AE147B;
	mov.u32 	%r4, 100;
	sub.s32 	%r5, %r4, %r1;
	cvt.rn.f64.s32	%fd4, %r5;
	add.f64 	%fd5, %fd4, 0dBFE0000000000000;
	mul.f64 	%fd6, %fd3, %fd3;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r1, 200, %r2;
	mad.lo.s32 	%r8, %r6, 40000, %r7;
	cvta.to.global.u64 	%rd2, %rd1;
	mul.f64 	%fd7, %fd5, 0d3FB47AE147AE147B;
	mul.f64 	%fd8, %fd7, %fd7;
	mul.f64 	%fd9, %fd8, 0d3FF0000000000000;
	fma.rn.f64 	%fd10, %fd6, 0d3FF0000000000000, %fd9;
	setp.lt.f64	%p1, %fd10, 0d4050000000000000;
	mul.wide.s32 	%rd3, %r8, 4;
	add.s64 	%rd4, %rd2, %rd3;
	selp.u32	%r9, 1, 0, %p1;
	st.global.u32 	[%rd4], %r9;
	ret;
}

.visible .entry _Z20averaging_filter_GPUIbEvPT_S1_b(
	.param .u64 _Z20averaging_filter_GPUIbEvPT_S1_b_param_0,
	.param .u64 _Z20averaging_filter_GPUIbEvPT_S1_b_param_1,
	.param .u8 _Z20averaging_filter_GPUIbEvPT_S1_b_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<33>;
	.reg .s64 	%rd<9>;
	.reg .f64 	%fd<13>;


	ld.param.u64 	%rd3, [_Z20averaging_filter_GPUIbEvPT_S1_b_param_0];
	ld.param.u64 	%rd4, [_Z20averaging_filter_GPUIbEvPT_S1_b_param_1];
	ld.param.s8 	%rs1, [_Z20averaging_filter_GPUIbEvPT_S1_b_param_2];
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ctaid.x;
	add.s32 	%r12, %r11, -2;
	mov.u32 	%r13, 0;
	max.s32 	%r31, %r12, %r13;
	add.s32 	%r14, %r11, 2;
	mov.u32 	%r15, 199;
	min.s32 	%r2, %r14, %r15;
	add.s32 	%r16, %r10, -2;
	max.s32 	%r3, %r16, %r13;
	add.s32 	%r17, %r10, 2;
	min.s32 	%r4, %r17, %r15;
	mov.u32 	%r18, 1;
	sub.s32 	%r19, %r18, %r31;
	add.s32 	%r20, %r19, %r2;
	sub.s32 	%r21, %r18, %r3;
	add.s32 	%r22, %r21, %r4;
	mul.lo.s32 	%r23, %r22, %r20;
	cvt.rn.f64.s32	%fd1, %r23;
	mov.f64 	%fd12, 0d0000000000000000;
	setp.le.s32	%p4, %r31, %r2;
	@%p4 bra 	BB30_1;
	bra.uni 	BB30_6;

BB30_1:
	cvta.to.global.u64 	%rd1, %rd3;

BB30_2:
	setp.gt.s32	%p5, %r3, %r4;
	@%p5 bra 	BB30_5;

	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r6, %r24, 40000, %r31;
	mov.u32 	%r32, %r3;

BB30_4:
	mov.u32 	%r7, %r32;
	mad.lo.s32 	%r25, %r7, 200, %r6;
	cvt.s64.s32	%rd5, %r25;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.s8 	%rs2, [%rd6];
	cvt.rn.f64.s16	%fd8, %rs2;
	add.f64 	%fd12, %fd12, %fd8;
	add.s32 	%r8, %r7, 1;
	setp.le.s32	%p6, %r8, %r4;
	mov.u32 	%r32, %r8;
	@%p6 bra 	BB30_4;

BB30_5:
	add.s32 	%r31, %r31, 1;
	setp.le.s32	%p7, %r31, %r2;
	@%p7 bra 	BB30_2;

BB30_6:
	cvta.to.global.u64 	%rd7, %rd4;
	mad.lo.s32 	%r28, %r10, 200, %r11;
	mov.u32 	%r29, %tid.x;
	mad.lo.s32 	%r30, %r29, 40000, %r28;
	cvt.s64.s32	%rd8, %r30;
	add.s64 	%rd2, %rd7, %rd8;
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p8, %rs3, 0;
	@%p8 bra 	BB30_8;

	mul.f64 	%fd10, %fd1, 0d3FB999999999999A;
	setp.gt.f64	%p9, %fd12, %fd10;
	bra.uni 	BB30_9;

BB30_8:
	div.rn.f64 	%fd11, %fd12, %fd1;
	setp.neu.f64	%p9, %fd11, 0d0000000000000000;

BB30_9:
	selp.u16	%rs4, 1, 0, %p9;
	st.global.u8 	[%rd2], %rs4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot31[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .s32 	%r<43>;
	.reg .s64 	%rd<98>;
	.reg .f64 	%fd<3>;


	mov.u64 	%SPL, __local_depot31;
	ld.param.f64 	%fd1, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd1;
	}
	and.b32  	%r41, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	and.b32  	%r14, %r3, 2047;
	add.s32 	%r15, %r14, -1024;
	shr.u32 	%r16, %r15, 6;
	mov.u32 	%r17, 15;
	sub.s32 	%r4, %r17, %r16;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r16;
	mov.u32 	%r20, 18;
	min.s32 	%r5, %r20, %r19;
	setp.lt.s32	%p1, %r4, %r5;
	mov.u64 	%rd88, %rd38;
	@%p1 bra 	BB31_2;

	mov.u64 	%rd92, 0;
	mov.u64 	%rd91, %rd38;
	bra.uni 	BB31_4;

BB31_2:
	mov.b64 	 %rd41, %fd1;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	sub.s32 	%r26, %r17, %r16;
	mul.wide.s32 	%rd43, %r26, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd89, %rd44, %rd43;
	mov.u64 	%rd92, 0;
	mov.u64 	%rd90, %rd38;
	mov.u32 	%r40, %r4;

BB31_3:
	.pragma "nounroll";
	mov.u32 	%r6, %r40;
	ld.const.u64 	%rd47, [%rd89];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd92;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd46, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd90], %rd45;
	add.s32 	%r7, %r6, 1;
	sub.s32 	%r27, %r7, %r4;
	mul.wide.s32 	%rd50, %r27, 8;
	add.s64 	%rd90, %rd38, %rd50;
	add.s64 	%rd89, %rd89, 8;
	add.s64 	%rd14, %rd88, 8;
	setp.lt.s32	%p2, %r7, %r5;
	mov.u64 	%rd88, %rd14;
	mov.u64 	%rd92, %rd46;
	mov.u32 	%r40, %r7;
	mov.u64 	%rd91, %rd14;
	@%p2 bra 	BB31_3;

BB31_4:
	mov.u64 	%rd15, %rd91;
	st.local.u64 	[%rd15], %rd92;
	ld.local.u64 	%rd93, [%rd38+16];
	ld.local.u64 	%rd94, [%rd38+24];
	and.b32  	%r8, %r3, 63;
	setp.eq.s32	%p3, %r8, 0;
	@%p3 bra 	BB31_6;

	mov.u32 	%r28, 64;
	sub.s32 	%r29, %r28, %r8;
	shl.b64 	%rd51, %rd94, %r8;
	shr.u64 	%rd52, %rd93, %r29;
	or.b64  	%rd94, %rd51, %rd52;
	shl.b64 	%rd53, %rd93, %r8;
	ld.local.u64 	%rd54, [%rd38+8];
	shr.u64 	%rd55, %rd54, %r29;
	or.b64  	%rd93, %rd55, %rd53;

BB31_6:
	shr.u64 	%rd56, %rd94, 62;
	cvt.u32.u64	%r30, %rd56;
	shr.u64 	%rd57, %rd93, 62;
	shl.b64 	%rd58, %rd94, 2;
	or.b64  	%rd96, %rd58, %rd57;
	shl.b64 	%rd95, %rd93, 2;
	shr.u64 	%rd59, %rd94, 61;
	cvt.u32.u64	%r31, %rd59;
	and.b32  	%r32, %r31, 1;
	add.s32 	%r33, %r32, %r30;
	neg.s32 	%r34, %r33;
	setp.eq.s32	%p4, %r41, 0;
	selp.b32	%r35, %r33, %r34, %p4;
	st.u32 	[%rd37], %r35;
	setp.eq.s32	%p5, %r32, 0;
	@%p5 bra 	BB31_8;

	mov.u64 	%rd63, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd63;
	mov.b64         {a2,a3}, %rd63;
	mov.b64         {b0,b1}, %rd95;
	mov.b64         {b2,b3}, %rd96;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd60, {r0,r1};
	mov.b64         %rd61, {r2,r3};
	}
	// inline asm
	xor.b32  	%r41, %r41, -2147483648;
	mov.u64 	%rd96, %rd61;
	mov.u64 	%rd95, %rd60;

BB31_8:
	clz.b64 	%r42, %rd96;
	setp.eq.s32	%p6, %r42, 0;
	@%p6 bra 	BB31_10;

	shl.b64 	%rd66, %rd96, %r42;
	mov.u32 	%r36, 64;
	sub.s32 	%r37, %r36, %r42;
	shr.u64 	%rd67, %rd95, %r37;
	or.b64  	%rd96, %rd67, %rd66;

BB31_10:
	mov.u64 	%rd71, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd96;   
	mov.b64         {blo,bhi}, %rd71;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd68, {r0,r1};     
	mov.b64         %rd69, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p7, %rd69, 1;
	mov.u64 	%rd97, %rd69;
	@%p7 bra 	BB31_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd68;
	mov.b64         {a2,a3}, %rd69;
	mov.b64         {b0,b1}, %rd68;
	mov.b64         {b2,b3}, %rd69;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd72, {r0,r1};
	mov.b64         %rd73, {r2,r3};
	}
	// inline asm
	add.s32 	%r42, %r42, 1;
	mov.u64 	%rd97, %rd73;

BB31_12:
	cvt.u64.u32	%rd78, %r41;
	shl.b64 	%rd79, %rd78, 32;
	mov.u32 	%r38, 1022;
	sub.s32 	%r39, %r38, %r42;
	cvt.u64.u32	%rd80, %r39;
	shl.b64 	%rd81, %rd80, 52;
	add.s64 	%rd82, %rd97, 1;
	shr.u64 	%rd83, %rd82, 10;
	add.s64 	%rd84, %rd83, 1;
	shr.u64 	%rd85, %rd84, 1;
	add.s64 	%rd86, %rd81, %rd85;
	or.b64  	%rd87, %rd86, %rd79;
	mov.b64 	 %fd2, %rd87;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<11>;
	.reg .s32 	%r<39>;
	.reg .f32 	%f<5>;
	.reg .f64 	%fd<144>;


	ld.param.f64 	%fd14, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd15, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd14;
	}
	shr.u32 	%r18, %r35, 20;
	and.b32  	%r36, %r18, 2047;
	setp.ne.s32	%p1, %r36, 0;
	@%p1 bra 	BB32_2;

	mul.f64 	%fd16, %fd14, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd16;
	}
	shr.u32 	%r19, %r35, 20;
	and.b32  	%r20, %r19, 2047;
	add.s32 	%r36, %r20, -54;

BB32_2:
	add.s32 	%r37, %r36, -1023;
	and.b32  	%r21, %r35, -2146435073;
	or.b32  	%r22, %r21, 1072693248;
	mov.b64 	%fd141, {%r34, %r22};
	setp.lt.u32	%p2, %r22, 1073127583;
	@%p2 bra 	BB32_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd141;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd141;
	}
	add.s32 	%r25, %r24, -1048576;
	mov.b64 	%fd141, {%r23, %r25};
	add.s32 	%r37, %r36, -1022;

BB32_4:
	add.f64 	%fd17, %fd141, 0d3FF0000000000000;
	mov.f64 	%fd19, 0d3FF0000000000000;
	// inline asm
	cvt.rn.f32.f64     %f1,%fd17;
	// inline asm
	// inline asm
	rcp.approx.ftz.f32 %f2,%f1;
	// inline asm
	// inline asm
	cvt.f64.f32        %fd18,%f2;
	// inline asm
	neg.f64 	%fd20, %fd17;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd18, %fd18;
	add.f64 	%fd24, %fd141, 0dBFF0000000000000;
	mul.f64 	%fd25, %fd24, %fd23;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	mul.f64 	%fd27, %fd26, %fd26;
	mov.f64 	%fd28, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd29, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F6249249242B910;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F89999999999DFB;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	sub.f64 	%fd41, %fd24, %fd26;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd26;
	fma.rn.f64 	%fd44, %fd43, %fd24, %fd42;
	mul.f64 	%fd45, %fd23, %fd44;
	fma.rn.f64 	%fd46, %fd40, %fd27, 0d3FB5555555555555;
	mov.f64 	%fd47, 0d3FB5555555555555;
	sub.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd40, %fd27, %fd48;
	add.f64 	%fd50, %fd49, 0d0000000000000000;
	add.f64 	%fd51, %fd50, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd52, %fd46, %fd51;
	sub.f64 	%fd53, %fd46, %fd52;
	add.f64 	%fd54, %fd53, %fd51;
	mul.rn.f64 	%fd55, %fd52, %fd26;
	neg.f64 	%fd56, %fd55;
	fma.rn.f64 	%fd57, %fd52, %fd26, %fd56;
	fma.rn.f64 	%fd58, %fd52, %fd45, %fd57;
	fma.rn.f64 	%fd59, %fd54, %fd26, %fd58;
	add.f64 	%fd60, %fd55, %fd59;
	sub.f64 	%fd61, %fd55, %fd60;
	add.f64 	%fd62, %fd61, %fd59;
	mul.rn.f64 	%fd63, %fd60, %fd26;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd60, %fd26, %fd64;
	fma.rn.f64 	%fd66, %fd60, %fd45, %fd65;
	fma.rn.f64 	%fd67, %fd62, %fd26, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd69, %fd67;
	mul.rn.f64 	%fd71, %fd68, %fd26;
	neg.f64 	%fd72, %fd71;
	fma.rn.f64 	%fd73, %fd68, %fd26, %fd72;
	fma.rn.f64 	%fd74, %fd68, %fd45, %fd73;
	fma.rn.f64 	%fd75, %fd70, %fd26, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd77, %fd75;
	add.f64 	%fd79, %fd26, %fd76;
	sub.f64 	%fd80, %fd26, %fd79;
	add.f64 	%fd81, %fd80, %fd76;
	add.f64 	%fd82, %fd81, %fd78;
	add.f64 	%fd83, %fd82, %fd45;
	add.f64 	%fd84, %fd79, %fd83;
	sub.f64 	%fd85, %fd79, %fd84;
	add.f64 	%fd86, %fd85, %fd83;
	cvt.rn.f64.s32	%fd87, %r37;
	mov.f64 	%fd88, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd89, %fd87, %fd88;
	mov.f64 	%fd90, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd91, %fd87, %fd90;
	add.f64 	%fd92, %fd89, %fd84;
	sub.f64 	%fd93, %fd89, %fd92;
	add.f64 	%fd94, %fd93, %fd84;
	add.f64 	%fd95, %fd94, %fd86;
	add.f64 	%fd96, %fd95, %fd91;
	add.f64 	%fd97, %fd92, %fd96;
	sub.f64 	%fd98, %fd92, %fd97;
	add.f64 	%fd99, %fd98, %fd96;
	abs.f64 	%fd100, %fd15;
	setp.gt.f64	%p3, %fd100, 0d7F0D2A1BE4048F90;
	mul.f64 	%fd101, %fd15, 0d3F20000000000000;
	selp.f64	%fd102, %fd101, %fd15, %p3;
	mul.rn.f64 	%fd103, %fd97, %fd102;
	neg.f64 	%fd104, %fd103;
	fma.rn.f64 	%fd105, %fd97, %fd102, %fd104;
	fma.rn.f64 	%fd106, %fd99, %fd102, %fd105;
	add.f64 	%fd4, %fd103, %fd106;
	sub.f64 	%fd107, %fd103, %fd4;
	add.f64 	%fd5, %fd107, %fd106;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd4;
	}
	setp.lt.u32	%p4, %r13, 1082535491;
	setp.lt.s32	%p5, %r13, -1064875759;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB32_6;

	setp.lt.s32	%p7, %r13, 0;
	selp.f64	%fd108, 0d0000000000000000, 0d7FF0000000000000, %p7;
	abs.f64 	%fd109, %fd4;
	setp.gtu.f64	%p8, %fd109, 0d7FF0000000000000;
	add.f64 	%fd110, %fd4, %fd4;
	selp.f64	%fd143, %fd110, %fd108, %p8;
	bra.uni 	BB32_10;

BB32_6:
	mul.f64 	%fd111, %fd4, 0d3FF71547652B82FE;
	cvt.rni.f64.f64	%fd112, %fd111;
	cvt.rzi.s32.f64	%r14, %fd112;
	mov.f64 	%fd113, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd114, %fd112, %fd113, %fd4;
	mov.f64 	%fd115, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd116, %fd112, %fd115, %fd114;
	mov.f64 	%fd117, 0d3E928A27E30F5561;
	mov.f64 	%fd118, 0d3E5AE6449C0686C0;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	mov.f64 	%fd120, 0d3EC71DE8E6486D6B;
	fma.rn.f64 	%fd121, %fd119, %fd116, %fd120;
	mov.f64 	%fd122, 0d3EFA019A6B2464C5;
	fma.rn.f64 	%fd123, %fd121, %fd116, %fd122;
	mov.f64 	%fd124, 0d3F2A01A0171064A5;
	fma.rn.f64 	%fd125, %fd123, %fd116, %fd124;
	mov.f64 	%fd126, 0d3F56C16C17F29C8D;
	fma.rn.f64 	%fd127, %fd125, %fd116, %fd126;
	mov.f64 	%fd128, 0d3F8111111111A24E;
	fma.rn.f64 	%fd129, %fd127, %fd116, %fd128;
	mov.f64 	%fd130, 0d3FA555555555211D;
	fma.rn.f64 	%fd131, %fd129, %fd116, %fd130;
	mov.f64 	%fd132, 0d3FC5555555555530;
	fma.rn.f64 	%fd133, %fd131, %fd116, %fd132;
	mov.f64 	%fd134, 0d3FE0000000000005;
	fma.rn.f64 	%fd135, %fd133, %fd116, %fd134;
	fma.rn.f64 	%fd137, %fd135, %fd116, %fd19;
	fma.rn.f64 	%fd142, %fd137, %fd116, %fd19;
	abs.s32 	%r26, %r14;
	setp.lt.s32	%p9, %r26, 1023;
	@%p9 bra 	BB32_8;

	add.s32 	%r27, %r14, 2046;
	shl.b32 	%r28, %r27, 19;
	and.b32  	%r29, %r28, -1048576;
	shl.b32 	%r30, %r27, 20;
	sub.s32 	%r38, %r30, %r29;
	mov.u32 	%r31, 0;
	mov.b64 	%fd138, {%r31, %r29};
	mul.f64 	%fd142, %fd142, %fd138;
	bra.uni 	BB32_9;

BB32_8:
	shl.b32 	%r32, %r14, 20;
	add.s32 	%r38, %r32, 1072693248;

BB32_9:
	mov.u32 	%r33, 0;
	mov.b64 	%fd139, {%r33, %r38};
	mul.f64 	%fd143, %fd142, %fd139;

BB32_10:
	abs.f64 	%fd140, %fd143;
	setp.eq.f64	%p10, %fd140, 0d7FF0000000000000;
	@%p10 bra 	BB32_12;

	fma.rn.f64 	%fd143, %fd143, %fd5, %fd143;

BB32_12:
	st.param.f64	[func_retval0+0], %fd143;
	ret;
}


