=========================================================================
Proton Computed Tomography (pCT) Preprocessing/Image Reconstruction Program
=========================================================================

####This program expects proton tracker coordinate and Water Equivalent Path Length (WEPL) measurements acquired from various gantry angles and:

-------------------------------------------------------------------------
**Phase 1:**
-------------------------------------------------------------------------
**(1)** Parse the command line arguments, if any, read the *configuration* file from the input data directory or the path passed as commamd line argument, and write the output *configuration* file with the modified `key/value` pairs overwriting their default values. 						              
**(2)** Parse default, modified, or specified *configuration* file (depending on results **(1)**) and set the corresponding internal variables/options, I/O directories, file names, and file paths.  
**(3)** Transfer instance of `configurations` **struct** (`parameters`) whose members correspond to program parameters/options to GPU's global memory to establish persistent access to these throughout program execution by passing its global memory address instead of individually by value to each kernel.  
**(4)** Determine if **BOTH** the entry **AND** the exit paths of each proton intersect reconstruction volume and, thus, pass through it.  
**(5)** For protons that pass through reconstruction volume, accumulate entry/exit coordinates/angles of this intersection and their WEPL, gantry angle, and bin number.  
**(6)** Perform statistical analysis of each bin (mean/std dev) and remove histories with WEPL or xy/xz angle greater than 3 std devs from mean.  
**(7)** Perform hull-detection  (SC, MSC, SM, and/or FBP).  
**(8)** Perform any image filtering (2D/3D median/average) or data processing (WEPL distributions, radiographs, etc.) specified in *config* file.  
**(9)** Write the data/images specified in *config* file but not required for reconstruction to disk.  
**(10)** Choose the hull and perform the method for constructing the initial iterate specified in *config* file and write these to `hull.txt` and `x_0.txt`, respectively.  
**(11)** Using specified hull, determine if **BOTH** the entry **AND** the exit paths of each proton intersect hull and, thus, pass through it.  
**(12)** For protons that pass through the hull, determine their entry/exit coordinates/angles and update the values found in `(5)` to these, record their entry *x*/*y*/*z* voxel, and remove all data found in `(5)` for protons that do not pass through hull.  
**(13)** For the data accumulated in `(12)`, write the WEPL data to `WEPL.bin` and the entry/exit coordinate/angles, entry *x*/*y*/*z* voxel, bin number, and gantry angle to to `histories.bin`.   
**(14)** Using entry/exit data from `(12)`, calculate the MLP and write intersected voxels along paths to `MLP.bin`.  

-------------------------------------------------------------------------
**Phase 2:**
------------------------------------------------------------------------- 
**(1)** The accuracy of the reconstructed image and the rate of convergence vary depending on the order in which proton histories are used in image reconstruction.  Ideally, we would like to maximize the amount of new information added by each successive proton history used in image reconstruction, which occurs when they are orthogonal (i.e., there are no intersected voxels in common for the 2 protons ).  There are multiple ways to attempt to impose this for a three-dimensional image, for example, using protons which pass through different vertical portions of the image or whose relative angle is 90 degrees.  Unfortunately, there is no immediately obvious implementation of either of these approaches that would be computationally efficient.  However, assuming that there are the same number of histories for each projection angle, since the proton histories are initially ordered by projection angle, we can assume that the proton histories whose indices are separated by ~*N*/4 are approximately orthogonal.  Thus, successive protons are chosen as those whose index is ~*N*/4 away from the previous proton, which wraps back around from 0 when the index exceeds *N* (i.e. addition of ~*N*/4 modulo *N*).  

To generate a sequence of *N* numbers from the interval [0, N - 1] without repetition, the separation between indices must be a prime number *P*.  Thus, we choose the prime number *P* closest to *N*/4 as this generates a sequence of the *N* proton histories with each separated by approximately 90 degrees with no repeated indices (i.e., the sequence is equivalent to a 1:1 mapping of the indices [0, N-1]).  Mathematically speaking, the resulting sequence is a 1:1 mapping where element *i* of the ordered sequence corresponds to the proton history *n(i)* given by:

*n(i+1) - n(i) = P mod(N)* or  *n(i+1) = n(i) + P mod(N)*

Note that this sequence is not used to reorder the proton histories themselves, which would be prohibitively computationally expensive, it is simply used to define the order in which the proton histories are *accessed*.   			
**(2)** Perform image reconstruction using the iterative projection method specified in the *configurations* file (*settings.cfg*).			        
**(3)** After each iteration, write the image (*x*) to disk as `x_k.txt`, where *k* denotes the iteration number.  
**(4)** After each iteration, apply a 2D/3D median/average filter with radius *r* if specified in the *configurations* file, writing the resulting image to disk as `x_k_xxx_xx_rx`, where `xxx` is *med* or *avg*, `xx` is *2D* or *3D*, and the last `x` specifies the filter width *w = 2r+1* corresponding to the filter radius *r* specified in the *configurations* file.    
**(5)** After the last iteration, repeat the procedure outlined in (4) and write the final image (after performing any desired filtering) to disk as `x.txt`.

The iterative projection algorithms that have been implemented and are currently available for use are ART, DROP, and 2 variations of a Robust approach to DROP (still in development and being improved).  There are 4 variations of Total Variation Superiorization (TVS) that were recently added and are now available for testing, providing the ability to (1) change the order of the superiorization and feasibility seeking portions of TVS+DROP and/or (2) choose the parallel implementation of TVS. 

Data/task parallelism is inherent in nearly every aspect of preprocessing and reconstruction and has been exploited (except for MLP and image reconstruction) using GPGPU programming (CUDA).  The host is primarily used for disk/user IO, manage host/GPU data allocations/transfers, and to configure/launch GPU kernels. The desired program behaviour and associated options/parameters are specified by setting the corresponding key/value pairs in the configuration file `settings.cfg`, preventing the source code from being modified each time program settings are modified, thereby eliminating the need to recompile the program each time.  The program expects this file to be with the input data but its path can be specified via command line argument if it's located elsewhere.

The program also expects the file/folder naming and organizational scheme to adhere to the pCT data format defined in the pCT documentation.  Although the program provides some flexibility in the location of the input data directory, allowing this to be specified in the configuration file, the program requires a strict adherence to the naming and organizational scheme of its subdirectories and data files.  In particular, the organizational scheme encodes the properties of the data acquisition process (dates and scanned object) and the various data dependencies, information that is extracted and used by the program.  Each time the program is executed, this information is used to locate the desired `projection_xxx.bin' input data, automatically create/name the output data directories where the output data/images are to be written, and an entry in an execution log is added which  records the input/output data/image information and program options/settings associated with that reconstruction.  

If the input data is maintained in the proper pCT Data format, the user need only modify the configuration file to specify the desired program behaviour and execute the program, the reading of the configuration file and input data and all output data/images and execution information (execution times, execution log, etc.) are performed automatically.  In some cases, it may not be possible to completely comply with the organizational scheme, which is why the corresponding object/scan information can be specified in the configuration file.  However, this can only be used to specify (1) the path to the directory where the pCT data hierarchy is located and/or (2) the path or subdirectory names leading to the desired input data, it cannot be used to avoid adherence to the pCT data format.  

The pCT data directory's hierarchy of subdirectories not only provides a consistent organizational scheme for the collaboration and encodes the details of the data acquisition, it also establishes data dependencies for at each stage of reconstruction.  The results at each stage are in a subdirectory of a folder named by date, providing the ability to recreate results by acquiring the associated version of each program from GitHub.  This also makes it possible to isolate and assess the effects of a particular development and compare to previous results by ensuring that the other programs used to generate the previous data are the same.  With multiple programs in the reconstruction process, it is important to be able to perform such isolated comparisons, particularly when assessing the way changes in early stages of reconstruction propagate and ultimately effect the reconstructed images.  Such propagation effects are extremely important and have been largely absent in the past and although it is infeasible to perform such analyses of previous results, imposing the pCT data format and using GitHub in program development will now make this possible.    

The release date of a new version of a program is always available and one can determine if it has been used on a particular data set by navigating to this subdirectory and comparing it to the date subdirectories present.  Navigating to the lowest level of each branch also indicates which steps of reconstruction remain to be performed in each subdirectory.  In the past it has been nearly impossible to determine such information, particularly for those that are not directly involved with the execution of reconstruction programs, but now anyone can determine the reconstruction progress for each data set as well as which versions of each program have been used thus far.  

For typical (single execution) reconstructions, the `settings.cfg` file is used to specify the desired parameter values and control the program's behavior/options.  By default, this file is in the same directory as the `projection_xxx` data, but if it is not in this default location or another *config* file is desired instead, the path to this *config* file can be passed as an argument to the program when executed from the command line.  To make it possible to run multiple reconstructions consecutively (i.e. batch execution) with different values for 1 or more parameters/options, the new `key/value` pairs desired can be passed to the program as a series of arguments when it is executed from the command line.  This will save a copy of the *config* file in the directory where the preprocessing data will be written but with the value of these keys overwritten with their new values.  This will not affect the existing *config* file since it will be needed for subsequent reconstructions in the batch, but the new *config* file will be written to the directory containing the resulting preprocessed data and/or reconstructed images, thereby providing a record of the parameters/options used in generating it.  

Similarly, if the path to a *config* file is passed to the program as a command line argument, a copy of it will be written to the directory containing the resulting preprocessed data regardless of whether its values were changed via `key/value` pair command line arguments or not.  In other words, a *config* file is always written to the directory containing the resulting preprocessed data unless there is a *config* file in the directory containing the projection data and none of its values were changed.  Thus, if the directory containing the preprocessed data does not contain a *config* file, then the *config* file in the projection data directory was used.    

Since modifications to the *config* file are always specified as `key/value` pairs, the optional path to a *config* file in a non-default location is always passed as the last command line argument so it can easily be identified.  Regardless of how many `key/value` pair changes are made, there will always be an odd number of command line arguments passed if the path to the *config* file is passed and even number otherwise.  Thus, but counting the number of command line arguments passed to the program, it is easy to determine **(1)** how many `key/value` pairs are to be modified and **(2)** if the path to a specific *config* file has been specified.  The series of commands to execute the program and pass these optional parameters is as follows:

#####`nvcc -std=c++11 -gencode arch=compute_35,code=sm_35 -O3 pCT_Reconstruction_Data_Segments_Blake.cu -o jpertwee.out`
#####`./pCT_Reconstruction[key 1][val 1][key 2][val 2]...[key n][val n][cfg path]`  

Program execution begins by parsing the program's command line arguments and determines how many `key/value` pairs are to be modified and if the *config* file containing the program's parameters/options is to be read from the default location or from a user defined location.  All command line arguments are optional, none are required, in which case the program reads the *config* file from its default location in the projection data directory and no changes are made to it.  If the *config* file is to be changed, the new *config* file is generated and written to disk first.  If a *config* file from a non-default location is to be used, it is first copied from its existing location into the directory where the preprocessed data will be written.  Once this has been been finished, the appropriate *config* file is read and parsed and its `key/value` pairs used to set the corresponding members of the global instance of the **struct** `configurations` named `parameters`.  

After reading the entire *config* file and setting the appropriate parameters/options, internal variables dependent on these configurations are then set.  Once all options, parameters, and variables are set, then the input/output directories, file names, and file paths are established.  The *config* file has `key/value` pairs for specifying whether existing directories should or should not be overwritten, so if overwriting is not permitted, then existing directories are renamed by appending `_i` beginning with *i = 1* until a unique directory name has been achieved.  If overwriting is permitted, then directory names are not changed and any existing data in these directories may be overwritten since file names do not change.  Once directories and file name are all defined, each file is combined with the directory where it should be written so this complete path can be passed to I/O file operation functions.  

However, there are a few images whose file names can change, namely those that result from filtering as the size of the filter neighbourhood is reflected in their file name.  For example, a 2D median filter with filter width 7 (radius 3) applied to the FBP image is named `FBP_med_2D_r7.txt`.  For `FBP`, `hull`, `x_0`, `x_k`, and `x`, the *config* file has `key/value` pairs specifying the radii to use when applying median filter and average filters and it is permissible to generate 1 or more of these during preprocessing/reconstruction.  However, although there is the option to generate and save each of these during preprocessing, the *config* file is used to specify which is to be used as the hull (`hull_h`) and the initial iterate (`x_h`), with `x_h` being updated after each iteration of reconstruction; i.e., each iteration of reconstruction is stored in the same `x_h` global variable and the only way each iteration is known after reconstruction is because the initial iterate is written to disk as `x_0.txt` and after the *k*th iteration it is saved as `x_k.txt`.